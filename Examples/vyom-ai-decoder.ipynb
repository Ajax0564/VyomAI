{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:12.782256Z",
     "iopub.status.busy": "2025-02-09T05:44:12.781985Z",
     "iopub.status.idle": "2025-02-09T05:44:20.483415Z",
     "shell.execute_reply": "2025-02-09T05:44:20.482744Z",
     "shell.execute_reply.started": "2025-02-09T05:44:12.782235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:20.485059Z",
     "iopub.status.busy": "2025-02-09T05:44:20.484603Z",
     "iopub.status.idle": "2025-02-09T05:44:24.478291Z",
     "shell.execute_reply": "2025-02-09T05:44:24.477336Z",
     "shell.execute_reply.started": "2025-02-09T05:44:20.485030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:24.480880Z",
     "iopub.status.busy": "2025-02-09T05:44:24.480615Z",
     "iopub.status.idle": "2025-02-09T05:44:24.514981Z",
     "shell.execute_reply": "2025-02-09T05:44:24.514291Z",
     "shell.execute_reply.started": "2025-02-09T05:44:24.480857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class AbsoluteEncoding(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.pos_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(config.max_position_embeddings).expand((1, -1)),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.max_size = config.max_position_embeddings\n",
    "\n",
    "    def forward(self, size: int) -> torch.Tensor:\n",
    "        if self.max_size < size:\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({size }) is more than the config max_position_embeddings {self.max_size}\"\n",
    "            )\n",
    "        return self.pos_embeddings(self.position_ids[:, :size])\n",
    "\n",
    "\n",
    "class SinusoidalEncoding(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        if config.hidden_size % 2 != 0:\n",
    "            raise ValueError(\n",
    "                f\"Cannot use SinusoidalEncoding with \"\n",
    "                \"odd hidden dim got dim {config.hidden_size}\"\n",
    "            )\n",
    "        self.positional_encoding = torch.zeros(\n",
    "            1, config.max_position_embeddings, config.hidden_size\n",
    "        )\n",
    "        self.position = torch.arange(0, config.max_position_embeddings).unsqueeze(1)\n",
    "        self.div_term = torch.exp(\n",
    "            (\n",
    "                torch.arange(0, config.hidden_size, 2, dtype=torch.float)\n",
    "                * -(torch.log(torch.tensor(10000.0)) / config.hidden_size)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.positional_encoding[:, :, 0::2] = torch.sin(\n",
    "            self.position.float() * self.div_term\n",
    "        )\n",
    "        self.positional_encoding[:, :, 1::2] = torch.cos(\n",
    "            self.position.float() * self.div_term\n",
    "        )\n",
    "\n",
    "    def forward(self, seq_len: int) -> torch.Tensor:\n",
    "\n",
    "        return self.positional_encoding[:, :seq_len]\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    RotaryEmbedding is a PyTorch module that implements rotary positional embeddings for attention mechanisms.\n",
    "    Args:\n",
    "        config (object): Configuration object containing the following attributes:\n",
    "            hidden_size (int): The hidden size of the model.\n",
    "            num_attention_heads (int): The number of attention heads.\n",
    "    Attributes:\n",
    "        inv_freq (torch.Tensor): A tensor containing the inverse frequencies for the rotary embeddings.\n",
    "    Methods:\n",
    "        forward(seq_len):\n",
    "            Computes the rotary positional embeddings for a given sequence length.\n",
    "            Args:\n",
    "                seq_len (int): The length of the input sequence.\n",
    "            Returns:\n",
    "                torch.Tensor: A tensor containing the rotary positional embeddings with shape (1, seq_len, dim).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        dim = int(config.hidden_size // config.num_attention_heads)\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i, j -> i j\", t, self.inv_freq)\n",
    "\n",
    "        return freqs[None, :, :]\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"\n",
    "    Rotates half the hidden dimensions of the input tensor.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor to be rotated.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The tensor with half of its hidden dimensions rotated.\n",
    "    \"\"\"\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n",
    "def apply_rotary_pos_emb(\n",
    "    q, k, freqs, only_q: bool = False, unsqueeze_dim=1\n",
    ") -> Tuple[torch.Tensor]:\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        freqs: precalculated frqs for sin cos\n",
    "        only_q: bool = False for encoder decoder\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)\n",
    "    cos = emb.cos()\n",
    "    sin = emb.sin()\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    #     print(cos.size(),sin.size(),q.size(),k.size())\n",
    "    if only_q:\n",
    "        q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    else:\n",
    "\n",
    "        q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "        return q_embed, k_embed\n",
    "\n",
    "\n",
    "# To do :  Alibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:24.516337Z",
     "iopub.status.busy": "2025-02-09T05:44:24.516112Z",
     "iopub.status.idle": "2025-02-09T05:44:24.539883Z",
     "shell.execute_reply": "2025-02-09T05:44:24.539092Z",
     "shell.execute_reply.started": "2025-02-09T05:44:24.516318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttentionSelfOutput(nn.Module):\n",
    "    def __init__(\n",
    "        self, config, bias: Optional[bool] = True, out_features: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.hidden_size if out_features is None else out_features,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.layernorm = nn.LayerNorm(\n",
    "            config.hidden_size, eps=getattr(config, \"layer_norm_eps\", 1e-6)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.Tensor, input_tensor: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: torch.FloatTensor of shape (batch, seq_len, embed_dim)`\n",
    "            input_tensor: torch.FloatTensor of shape (batch, seq_len, embed_dim)`\n",
    "\n",
    "        return:\n",
    "               hidden_states: torch.FloatTensor of shape (batch, seq_len, embed_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.layernorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class DecoderAttention(nn.Module):\n",
    "    def __init__(self, config, layer_idx: int) -> None:\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "        self.head_size = int(config.hidden_size // config.num_attention_heads)\n",
    "        self.attention_bias = getattr(config, \"attention_bias\", True)\n",
    "        self.layer_idx = layer_idx\n",
    "        # self.qkv = nn.Linear(config.hidden_size,3*config.hidden_size)\n",
    "        self.query = nn.Linear(\n",
    "            config.hidden_size, config.hidden_size, bias=self.attention_bias\n",
    "        )\n",
    "        self.key = nn.Linear(\n",
    "            config.hidden_size, config.hidden_size, bias=self.attention_bias\n",
    "        )\n",
    "        self.value = nn.Linear(\n",
    "            config.hidden_size, config.hidden_size, bias=self.attention_bias\n",
    "        )\n",
    "        self.out = AttentionSelfOutput(config=config, bias=self.attention_bias)\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n",
    "        if not self.flash and self.layer_idx == 0:  # avoid to print m times:\n",
    "            print(\"WARNING: Flash Attention requires PyTorch >= 2.0\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_state: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        freqs: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        kv_cache: List[torch.FloatTensor] = None,\n",
    "        start_pos: Optional[int] = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: torch.Tensor of shape (batch, seq_len, embed_dim)`\n",
    "            Attention_mask: torch.Tensor of shape (batch,1, seq_len, seqlen)`\n",
    "            freqs: Positional freqs in case of RoPE embedding\n",
    "            use_cace: Optional to use kvCache\n",
    "            start_pos: in case of kvCache to get store kv-cache at start_pos\n",
    "        return:\n",
    "               hidden_states: torch.Tensor of shape (batch, seq_len, embed_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        q = self.query(hidden_state)\n",
    "        k = self.key(hidden_state)\n",
    "        v = self.value(hidden_state)\n",
    "        # transform it into batch_size x no_of_heads x seqlen x head_dim for Multihead Attention\n",
    "        q = rearrange(q, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n",
    "        k = rearrange(k, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n",
    "        v = rearrange(v, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n",
    "\n",
    "        if freqs is not None:\n",
    "            q, k = apply_rotary_pos_emb(q, k, freqs)  # apply RoPE if freqs is available\n",
    "\n",
    "        if use_cache:\n",
    "            if kv_cache is None:\n",
    "                raise ValueError(\"you need to pass kv_cache\")\n",
    "            k, v = kv_cache.update(self.layer_idx, k, v, start_pos)\n",
    "\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query=q, key=k, value=v, attn_mask=attention_mask\n",
    "        )\n",
    "        # transform it back into batch_size x seqlen x hidden_dim\n",
    "        out = rearrange(out, \"b h l d -> b l (h d)\")\n",
    "\n",
    "        return self.out(out, hidden_state), kv_cache\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(\n",
    "        batch, num_key_value_heads, n_rep, slen, head_dim\n",
    "    )\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "class DecoderAttentionGqa(nn.Module):\n",
    "    def __init__(self, config, layer_idx: int) -> None:\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n",
    "        if not self.flash and self.layer_idx == 0:  # avoid to print m times\n",
    "            print(\"WARNING: Flash Attention requires PyTorch >= 2.0\")\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = int(config.hidden_size // config.num_attention_heads)\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.num_key_value_heads = getattr(config, \"num_key_value_heads\", 4)\n",
    "        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads\n",
    "        if (\n",
    "            self.num_attention_heads % self.num_key_value_heads != 0\n",
    "            or self.num_attention_heads < self.num_key_value_heads\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"num_key_value_heads {self.num_key_value_heads }  should be less than equal num_attention_heads {config.num_attention_heads} and  multiple of num_attention_heads {config.num_attention_heads} \"\n",
    "            )\n",
    "        self.attention_bias = getattr(config, \"attention_bias\", True)\n",
    "        self.out = AttentionSelfOutput(config=config, bias=self.attention_bias)\n",
    "        self.query = nn.Linear(\n",
    "            config.hidden_size, config.hidden_size, bias=self.attention_bias\n",
    "        )\n",
    "        self.key = nn.Linear(\n",
    "            config.hidden_size,\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            bias=self.attention_bias,\n",
    "        )\n",
    "        self.value = nn.Linear(\n",
    "            config.hidden_size,\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            bias=self.attention_bias,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_state: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        freqs: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        kv_cache: List[torch.FloatTensor] = None,\n",
    "        start_pos: Optional[int] = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: torch.Tensor of shape (batch, seq_len, embed_dim)`\n",
    "            Attention_mask: torch.Tensor of shape (batch,1, seq_len, seqlen)`\n",
    "            freqs: Positional freqs in case of RoPE embedding\n",
    "            use_cace: Optional to use kvCache\n",
    "            start_pos: in case of kvCache to get store kv-cache at start_pos\n",
    "        return:\n",
    "               hidden_states: torch.Tensor of shape (batch, seq_len, embed_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        q = self.query(hidden_state)\n",
    "        k = self.key(hidden_state)\n",
    "        v = self.value(hidden_state)\n",
    "        # transform it into batch_size x no_of_heads x seqlen x head_dim for Multihead Attention\n",
    "        q = rearrange(q, \"b l (h d) -> b h l d\", d=self.head_dim)\n",
    "        k = rearrange(k, \"b l (h d) -> b h l d\", d=self.head_dim)\n",
    "        v = rearrange(v, \"b l (h d) -> b h l d\", d=self.head_dim)\n",
    "        if freqs is not None:\n",
    "            q, k = apply_rotary_pos_emb(q, k, freqs)  # apply RoPE if freqs is available\n",
    "\n",
    "        if use_cache:\n",
    "            if kv_cache is None:\n",
    "                raise ValueError(\"you need to pass kv_cache\")\n",
    "            k, v = kv_cache.update(self.layer_idx, k, v, start_pos)\n",
    "\n",
    "        k = repeat_kv(\n",
    "            k, n_rep=self.num_key_value_groups\n",
    "        )  # in case of GQA repeat k,v to make it same as q\n",
    "        v = repeat_kv(v, n_rep=self.num_key_value_groups)\n",
    "\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query=q, key=k, value=v, attn_mask=attention_mask\n",
    "        )\n",
    "        # transform it back into batch_size x seqlen x hidden_dim\n",
    "        out = rearrange(out, \"b h l d -> b l (h d)\")\n",
    "\n",
    "        return self.out(out, hidden_state), kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:24.540931Z",
     "iopub.status.busy": "2025-02-09T05:44:24.540719Z",
     "iopub.status.idle": "2025-02-09T05:44:24.559020Z",
     "shell.execute_reply": "2025-02-09T05:44:24.558118Z",
     "shell.execute_reply.started": "2025-02-09T05:44:24.540912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_ACT_ = {\n",
    "    \"gelu\": nn.GELU(),\n",
    "    \"leaky_relu\": nn.LeakyReLU(),\n",
    "    \"relu6\": nn.ReLU6(),\n",
    "    \"sigmoid\": nn.Sigmoid(),\n",
    "    \"silu\": nn.SiLU(),\n",
    "    \"swish\": nn.SiLU(),\n",
    "    \"tanh\": nn.Tanh(),\n",
    "}\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config, multiplier: Union[int, float] = 4) -> None:\n",
    "        super().__init__()\n",
    "        self.intermediate = nn.Linear(\n",
    "            config.hidden_size, int(multiplier) * config.hidden_size\n",
    "        )\n",
    "        self.dropout = nn.Dropout(getattr(config, \"attention_dropout\", 0.0))\n",
    "        self.layerNorm = nn.LayerNorm(\n",
    "            config.hidden_size, eps=getattr(config, \"layer_norm_eps\", 1e-6)\n",
    "        )\n",
    "        if _ACT_.get(getattr(config, \"hidden_act\", None), None):\n",
    "            self.act_fn = _ACT_[config.hidden_act]\n",
    "        else:\n",
    "            self.act_fn = nn.GELU()\n",
    "        self.out = nn.Linear(int(multiplier) * config.hidden_size, config.hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_state: torch.Tensor, input_tensor: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        output = self.intermediate(hidden_state)\n",
    "        output = self.act_fn(output)\n",
    "        output = self.out(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layerNorm(output + input_tensor)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:24.560165Z",
     "iopub.status.busy": "2025-02-09T05:44:24.559956Z",
     "iopub.status.idle": "2025-02-09T05:44:24.576733Z",
     "shell.execute_reply": "2025-02-09T05:44:24.575946Z",
     "shell.execute_reply.started": "2025-02-09T05:44:24.560146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# mainly 2way to do one keep it into the model init like llama https://github.com/meta-llama/llama/blob/main/llama/model.py\n",
    "# every attention layer have its own kv-cache storage\n",
    "# or keep all attention layer kv-cache into single storage like Huggingface Transformer\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicCache:\n",
    "    \"\"\"\n",
    "    A cache that grows dynamically as more tokens are generated.\n",
    "\n",
    "    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n",
    "    `[batch_size, num_heads, seq_len, head_dim]`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, is_gqa: bool = False) -> None:\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "        self._seen_tokens = False\n",
    "\n",
    "        self.layers = config.num_hidden_layers\n",
    "        for _ in range(self.layers):\n",
    "            self.key_cache.append([])\n",
    "            self.value_cache.append([])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n",
    "        to the number of layers in the model.\n",
    "        \"\"\"\n",
    "        if len(self.key_cache) == 0:\n",
    "            return 0\n",
    "        return self.key_cache[0].shape[-2]\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        index: int,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        start_pos: int = 0,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
    "\n",
    "        Parameters:\n",
    "            key_states (`torch.Tensor`):\n",
    "                The new key states to cache.\n",
    "            value_states (`torch.Tensor`):\n",
    "                The new value states to cache.\n",
    "            layer_idx (`int`):\n",
    "                The index of the layer to cache the states for.\n",
    "            cache_kwargs (`Dict[str, Any]`, `optional`):\n",
    "                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n",
    "\n",
    "        Return:\n",
    "            A tuple containing the updated key and value states.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update the cache first iteration'\n",
    "\n",
    "        if len(self.key_cache[index]) == 0:\n",
    "            self._seen_tokens = True\n",
    "            self.key_cache[index] = key_states.clone()\n",
    "            self.value_cache[index] = value_states.clone()\n",
    "        else:\n",
    "            self.key_cache[index] = torch.cat(\n",
    "                [self.key_cache[index], key_states], dim=-2\n",
    "            )\n",
    "            self.value_cache[index] = torch.cat(\n",
    "                [self.value_cache[index], value_states], dim=-2\n",
    "            )\n",
    "\n",
    "        return self.key_cache[index], self.value_cache[index]\n",
    "\n",
    "    def get(self, index: int) -> Tuple[torch.Tensor]:\n",
    "        if self._seen_tokens:\n",
    "            return self.key_cache[index], self.value_cache[index]\n",
    "        else:\n",
    "            raise ValueError(\"there is no token available in kv-cache\")\n",
    "\n",
    "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
    "        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n",
    "        if self.key_cache is None:\n",
    "            return 0\n",
    "        return self.key_cache[layer_idx].shape[-2]\n",
    "\n",
    "    def get_max_length(self) -> Optional[int]:\n",
    "        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n",
    "        return self.max_cache_len\n",
    "\n",
    "\n",
    "class StaticCache:\n",
    "    \"\"\"\n",
    "    A cache that grows dynamically as more tokens are generated.\n",
    "\n",
    "    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n",
    "    `[batch_size, num_heads, seq_len, head_dim]`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        max_cache_len: int = None,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        batch_size: int = 1,\n",
    "        is_gqa: bool = False,\n",
    "    ) -> None:\n",
    "        self.head_size = int(config.hidden_size // config.num_attention_heads)\n",
    "        self.heads = None\n",
    "        self.batch_size = batch_size\n",
    "        # if is_gqa:\n",
    "        self.heads = getattr(config, \"num_key_value_heads\", None)\n",
    "        # if self.heads is None:\n",
    "        #     raise ValueError(\n",
    "        #         \"you are using is_gqa=True and config.num_key_value_heads is not available\"\n",
    "        #     )\n",
    "        if self.heads is None:\n",
    "\n",
    "            self.heads = config.num_attention_heads\n",
    "\n",
    "        self.max_cache_len = (\n",
    "            config.max_position_embeddings if max_cache_len is None else max_cache_len\n",
    "        )\n",
    "\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "\n",
    "        self.cache_shape = (\n",
    "            self.batch_size,\n",
    "            self.heads,\n",
    "            self.max_cache_len,\n",
    "            self.head_size,\n",
    "        )\n",
    "\n",
    "        self._seen_tokens = False\n",
    "        self.layers = config.num_hidden_layers\n",
    "        for _ in range(self.layers):\n",
    "            blank_key_cache = torch.zeros(\n",
    "                self.cache_shape, dtype=self.dtype, device=self.device\n",
    "            )\n",
    "            blank_value_cache = torch.zeros(\n",
    "                self.cache_shape, dtype=self.dtype, device=self.device\n",
    "            )\n",
    "            self.key_cache.append(blank_key_cache)\n",
    "            self.value_cache.append(blank_value_cache)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.key_cache is None:\n",
    "            return 0\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n",
    "        to the number of layers in the model.\n",
    "        \"\"\"\n",
    "        return self.key_cache.shape[-2]\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        index: int,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        start_pos: int = 0,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
    "\n",
    "        Parameters:\n",
    "            key_states (`torch.Tensor`):\n",
    "                The new key states to cache.\n",
    "            value_states (`torch.Tensor`):\n",
    "                The new value states to cache.\n",
    "            layer_idx (`int`):\n",
    "                The index of the layer to cache the states for.\n",
    "            cache_kwargs (`Dict[str, Any]`, `optional`):\n",
    "                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n",
    "\n",
    "        Return:\n",
    "            A tuple containing the updated key and value states.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update the cache first iteration'\n",
    "\n",
    "        bsz, head, seqlen, _ = key_states.shape\n",
    "        if seqlen > self.key_cache[index].size()[2]:\n",
    "            raise ValueError(\n",
    "                f\"{k.shape} is more than init k_cache size {self.key_cache}\"\n",
    "            )\n",
    "\n",
    "        self.key_cache[index][:bsz, :, start_pos : start_pos + seqlen] = key_states\n",
    "        self.value_cache[index][:bsz, :, start_pos : start_pos + seqlen] = value_states\n",
    "\n",
    "        k = self.key_cache[index][:bsz, :, : start_pos + seqlen]\n",
    "        v = self.value_cache[index][:bsz, :, : start_pos + seqlen]\n",
    "\n",
    "        return k, v\n",
    "\n",
    "    def get(self, index: int) -> Tuple[torch.Tensor]:\n",
    "        if self._seen_tokens:\n",
    "            return self.key_cache[index], self.value_cache[index]\n",
    "        else:\n",
    "            raise ValueError(\"there is no token available in kv-cache\")\n",
    "\n",
    "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
    "        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n",
    "        if self.key_cache is None:\n",
    "            return 0\n",
    "        return self.key_cache[layer_idx].shape[-2]\n",
    "\n",
    "    def get_max_length(self) -> Optional[int]:\n",
    "        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:24.577845Z",
     "iopub.status.busy": "2025-02-09T05:44:24.577578Z",
     "iopub.status.idle": "2025-02-09T05:44:24.600983Z",
     "shell.execute_reply": "2025-02-09T05:44:24.600180Z",
     "shell.execute_reply.started": "2025-02-09T05:44:24.577818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPT style model Casual language modeling\n",
    "\n",
    "_position_embeddings = {\n",
    "    \"absolute\": AbsoluteEncoding,\n",
    "    \"sinusoidal\": SinusoidalEncoding,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecoderOutput(object):\n",
    "    logits: torch.Tensor\n",
    "    past_key_value: Optional[object]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CLMOutput(object):\n",
    "    hidden_state: torch.Tensor\n",
    "    logits: torch.Tensor\n",
    "    kv_cache: List[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config, layer_idx: int, attention_type: str = None) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = (\n",
    "            DecoderAttentionGqa(config, layer_idx=layer_idx)\n",
    "            if attention_type == \"gqa\"\n",
    "            else DecoderAttention(config, layer_idx=layer_idx)\n",
    "        )\n",
    "        if attention_type == \"gqa\" and layer_idx == 0:  # avoid to print m times\n",
    "            print(\"Decoder Using GQA Attention\")\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_state: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        freqs: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        kv_cache: List[torch.FloatTensor] = None,\n",
    "        start_pos: Optional[int] = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        out, kv_cache = self.attention(\n",
    "            hidden_state=hidden_state,\n",
    "            attention_mask=attention_mask,\n",
    "            freqs=freqs,\n",
    "            use_cache=use_cache,\n",
    "            kv_cache=kv_cache,\n",
    "            start_pos=start_pos,\n",
    "        )\n",
    "        out = self.feed_forward(out, hidden_state)\n",
    "        return out, kv_cache\n",
    "\n",
    "\n",
    "class LMHead(nn.Module):\n",
    "    \"\"\"Head for masked language modelling\"\"\"\n",
    "\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layerNorm = nn.LayerNorm(\n",
    "            config.hidden_size, eps=getattr(config, \"layer_norm_eps\", 1e-6)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dense(hidden_state)\n",
    "        x = nn.GELU()(x)\n",
    "        x = self.layerNorm(x)\n",
    "\n",
    "        # project back to size of vocabulary with bias\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        pos_embedding_type: Optional[str] = \"absolute\",\n",
    "        attention_type: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            padding_idx=getattr(config, \"pad_token_id\", None),\n",
    "        )\n",
    "        if _position_embeddings.get(pos_embedding_type, None) is not None:\n",
    "            self.position_embeddings = _position_embeddings.get(pos_embedding_type)(\n",
    "                config\n",
    "            )\n",
    "        else:\n",
    "            self.position_embeddings = None\n",
    "        if pos_embedding_type == \"rope\":\n",
    "            self.emb_freq = RotaryEmbedding(config)(config.max_position_embeddings)\n",
    "            print(\n",
    "                \"Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\"\n",
    "            )\n",
    "        self.all_layer = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(config, layer_idx, attention_type)\n",
    "                for layer_idx in range(config.num_hidden_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.lm_head = LMHead(config=config)\n",
    "        self.config = config\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(\n",
    "                module.weight, mean=0.0, std=0.02 / torch.sqrt(2 * len(self.all_layer))\n",
    "            )\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(\n",
    "                module.weight, mean=0.0, std=0.02 / torch.sqrt(2 * len(self.all_layer))\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        kv_cache: List[torch.FloatTensor] = None,\n",
    "        start_pos: Optional[int] = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        _bsz, seqlen = input_ids.shape\n",
    "        hidden_state = self.word_embeddings(input_ids)\n",
    "        freqs = None\n",
    "        if self.position_embeddings is not None:\n",
    "            pos_info = self.position_embeddings(start_pos + seqlen)[\n",
    "                :, start_pos : start_pos + seqlen, :\n",
    "            ].to(input_ids.device)\n",
    "            hidden_state = hidden_state + pos_info\n",
    "        else:\n",
    "            freqs = self.emb_freq[:, start_pos : start_pos + seqlen].to(\n",
    "                input_ids.device\n",
    "            )\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = self.create_mask_for_decoder(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, start_pos=start_pos\n",
    "            )\n",
    "            mask = (1.0 - mask) * torch.finfo(\n",
    "                hidden_state.dtype\n",
    "            ).min  # invert it to to add directly to attention score\n",
    "\n",
    "        for layer in self.all_layer:\n",
    "            hidden_state, kv_cache = layer(\n",
    "                hidden_state,\n",
    "                mask,\n",
    "                freqs=freqs,\n",
    "                use_cache=use_cache,\n",
    "                kv_cache=kv_cache,\n",
    "                start_pos=start_pos,\n",
    "            )\n",
    "        logits = self.lm_head(hidden_state)\n",
    "        return CLMOutput(hidden_state=hidden_state, logits=logits, kv_cache=kv_cache)\n",
    "\n",
    "    def create_mask_for_decoder(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        start_pos: Optional[int] = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        device = input_ids.device\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (\n",
    "                torch.ones(seq_length + start_pos).repeat(batch_size, 1).to(device)\n",
    "            )\n",
    "        seq_ids = torch.arange(seq_length).to(device)\n",
    "        causal_mask = (\n",
    "            seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n",
    "            <= seq_ids[None, :, None]\n",
    "        )  # 1x1xl repeat bxlxl compare to 1xlx1\n",
    "\n",
    "        causal_mask = causal_mask.to(attention_mask.dtype)\n",
    "\n",
    "        if start_pos > 0:  # correct the attention mask  for kv-cache operation\n",
    "            causal_mask = torch.cat(\n",
    "                [\n",
    "                    torch.ones(\n",
    "                        (batch_size, seq_length, start_pos),\n",
    "                        device=device,\n",
    "                        dtype=causal_mask.dtype,\n",
    "                    ),\n",
    "                    causal_mask,\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "\n",
    "        extended_attention_mask = (\n",
    "            causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "        )  # # this is mainly if batch contains <PAD> tokens. stop casual procees before <PAD>\n",
    "        return extended_attention_mask\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(\n",
    "        cls,\n",
    "        config,\n",
    "        pos_embedding_type: Optional[str] = \"absolute\",\n",
    "        attention_type: Optional[str] = None,\n",
    "    ) -> nn.Module:\n",
    "        return cls(config, pos_embedding_type, attention_type)\n",
    "\n",
    "    def _setup_cache(self, config, cls: Optional[object] = StaticCache) -> None:\n",
    "        for layer in self.all_layer:\n",
    "            layer.attention.cache = cls(config)\n",
    "\n",
    "    def _clean_cache(self) -> None:\n",
    "        for layer in self.all_layer:\n",
    "            layer.attention.cache = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:24.603833Z",
     "iopub.status.busy": "2025-02-09T05:44:24.603558Z",
     "iopub.status.idle": "2025-02-09T05:44:24.619746Z",
     "shell.execute_reply": "2025-02-09T05:44:24.618783Z",
     "shell.execute_reply.started": "2025-02-09T05:44:24.603799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:24.621635Z",
     "iopub.status.busy": "2025-02-09T05:44:24.621370Z",
     "iopub.status.idle": "2025-02-09T05:44:37.737824Z",
     "shell.execute_reply": "2025-02-09T05:44:37.737109Z",
     "shell.execute_reply.started": "2025-02-09T05:44:24.621614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../input/transformer-distilation-gpt-2/gpt2_6L\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:37.739198Z",
     "iopub.status.busy": "2025-02-09T05:44:37.738713Z",
     "iopub.status.idle": "2025-02-09T05:44:39.646024Z",
     "shell.execute_reply": "2025-02-09T05:44:39.645317Z",
     "shell.execute_reply.started": "2025-02-09T05:44:37.739176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01494bc916d4968b74e8eac01205336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a109cd2b544876aad80b1ff2f48c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c6dbdec0df4954a35acd429879d51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ea4c36344b4233a1398366791ffbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f94fe19ad8465c8220b94da147f1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:39.646991Z",
     "iopub.status.busy": "2025-02-09T05:44:39.646783Z",
     "iopub.status.idle": "2025-02-09T05:44:39.651823Z",
     "shell.execute_reply": "2025-02-09T05:44:39.651183Z",
     "shell.execute_reply.started": "2025-02-09T05:44:39.646973Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:39.652784Z",
     "iopub.status.busy": "2025-02-09T05:44:39.652448Z",
     "iopub.status.idle": "2025-02-09T05:44:39.701788Z",
     "shell.execute_reply": "2025-02-09T05:44:39.700986Z",
     "shell.execute_reply.started": "2025-02-09T05:44:39.652756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 512:\n",
    "    tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:39.702842Z",
     "iopub.status.busy": "2025-02-09T05:44:39.702577Z",
     "iopub.status.idle": "2025-02-09T05:44:39.715948Z",
     "shell.execute_reply": "2025-02-09T05:44:39.715366Z",
     "shell.execute_reply.started": "2025-02-09T05:44:39.702823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "state_dict = m.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:39.716857Z",
     "iopub.status.busy": "2025-02-09T05:44:39.716637Z",
     "iopub.status.idle": "2025-02-09T05:44:39.785280Z",
     "shell.execute_reply": "2025-02-09T05:44:39.784596Z",
     "shell.execute_reply.started": "2025-02-09T05:44:39.716840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6588596"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = open(\n",
    "    \"/kaggle/input/mark-twain-books/Combine.txt\", encoding=\"utf8\", errors=\"ignore\"\n",
    ").read()\n",
    "\n",
    "new_str = re.sub(\"�\", \"\", string)\n",
    "\n",
    "open(\"Train.txt\", \"w\").write(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:39.786210Z",
     "iopub.status.busy": "2025-02-09T05:44:39.786015Z",
     "iopub.status.idle": "2025-02-09T05:44:39.980624Z",
     "shell.execute_reply": "2025-02-09T05:44:39.979724Z",
     "shell.execute_reply.started": "2025-02-09T05:44:39.786192Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c25a23957047c2900857818db1db17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ckpt = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:39.981807Z",
     "iopub.status.busy": "2025-02-09T05:44:39.981483Z",
     "iopub.status.idle": "2025-02-09T05:44:39.987191Z",
     "shell.execute_reply": "2025-02-09T05:44:39.986366Z",
     "shell.execute_reply.started": "2025-02-09T05:44:39.981786Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.47.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:39.988281Z",
     "iopub.status.busy": "2025-02-09T05:44:39.988000Z",
     "iopub.status.idle": "2025-02-09T05:44:40.000346Z",
     "shell.execute_reply": "2025-02-09T05:44:39.999561Z",
     "shell.execute_reply.started": "2025-02-09T05:44:39.988261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:40.001613Z",
     "iopub.status.busy": "2025-02-09T05:44:40.001292Z",
     "iopub.status.idle": "2025-02-09T05:44:40.023298Z",
     "shell.execute_reply": "2025-02-09T05:44:40.022429Z",
     "shell.execute_reply.started": "2025-02-09T05:44:40.001585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = SimpleNamespace(**config.__dict__)\n",
    "config.vocab_size = len(tokenizer)\n",
    "config.num_hidden_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:40.024340Z",
     "iopub.status.busy": "2025-02-09T05:44:40.024068Z",
     "iopub.status.idle": "2025-02-09T05:44:41.053670Z",
     "shell.execute_reply": "2025-02-09T05:44:41.052739Z",
     "shell.execute_reply.started": "2025-02-09T05:44:40.024313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\n"
     ]
    }
   ],
   "source": [
    "model = DecoderModel.from_config(config, pos_embedding_type=\"rope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:41.054742Z",
     "iopub.status.busy": "2025-02-09T05:44:41.054463Z",
     "iopub.status.idle": "2025-02-09T05:44:41.072023Z",
     "shell.execute_reply": "2025-02-09T05:44:41.071252Z",
     "shell.execute_reply.started": "2025-02-09T05:44:41.054721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.word_embeddings = nn.Embedding.from_pretrained(\n",
    "    state_dict[\"transformer.wte.weight\"], freeze=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Source**\n",
    "\n",
    "https://www.kaggle.com/datasets/msinger007/mark-twain-books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:41.073023Z",
     "iopub.status.busy": "2025-02-09T05:44:41.072835Z",
     "iopub.status.idle": "2025-02-09T05:44:41.076192Z",
     "shell.execute_reply": "2025-02-09T05:44:41.075521Z",
     "shell.execute_reply.started": "2025-02-09T05:44:41.073006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_path = \"Train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:41.077226Z",
     "iopub.status.busy": "2025-02-09T05:44:41.076975Z",
     "iopub.status.idle": "2025-02-09T05:44:41.093244Z",
     "shell.execute_reply": "2025-02-09T05:44:41.092667Z",
     "shell.execute_reply.started": "2025-02-09T05:44:41.077194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, file_path: str, block_size: int):\n",
    "        if os.path.isfile(file_path) is False:\n",
    "            raise ValueError(f\"Input file path {file_path} not found\")\n",
    "\n",
    "        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "        saved = False\n",
    "        cache_dir = None\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else directory,\n",
    "            f\"cached_lm_{tokenizer.__class__.__name__}_{block_size}_{filename}\",\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and saved:\n",
    "            start = time.time()\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        #                 logger.info(\n",
    "        #                     f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "        #                 )\n",
    "\n",
    "        else:\n",
    "            #                 logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "\n",
    "            self.examples = []\n",
    "            with open(file_path, encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "            for i in range(\n",
    "                0, len(tokenized_text) - block_size + 1, block_size\n",
    "            ):  # Truncate in block of block_size\n",
    "                self.examples.append(\n",
    "                    tokenizer.build_inputs_with_special_tokens(\n",
    "                        tokenized_text[i : i + block_size]\n",
    "                    )\n",
    "                )\n",
    "                # )\n",
    "            # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "            # If your dataset is small, first you should look for a bigger one :-) and second you\n",
    "            # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "            start = time.time()\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                saved = True\n",
    "\n",
    "    #                 logger.info(\n",
    "    #                     f\"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]\"\n",
    "    #                 )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return {\"input_ids\": torch.tensor(self.examples[i], dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:41.096809Z",
     "iopub.status.busy": "2025-02-09T05:44:41.096601Z",
     "iopub.status.idle": "2025-02-09T05:44:41.109897Z",
     "shell.execute_reply": "2025-02-09T05:44:41.109151Z",
     "shell.execute_reply.started": "2025-02-09T05:44:41.096780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    if tokenizer.pad_token_id is not None:\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:41.111371Z",
     "iopub.status.busy": "2025-02-09T05:44:41.111133Z",
     "iopub.status.idle": "2025-02-09T05:44:48.813230Z",
     "shell.execute_reply": "2025-02-09T05:44:48.812282Z",
     "shell.execute_reply.started": "2025-02-09T05:44:41.111341Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1580900 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TextDataset(tokenizer, train_path, 128), batch_size=16, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:48.814504Z",
     "iopub.status.busy": "2025-02-09T05:44:48.814237Z",
     "iopub.status.idle": "2025-02-09T05:44:48.818060Z",
     "shell.execute_reply": "2025-02-09T05:44:48.817190Z",
     "shell.execute_reply.started": "2025-02-09T05:44:48.814465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for data in train_loader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:48.819270Z",
     "iopub.status.busy": "2025-02-09T05:44:48.819014Z",
     "iopub.status.idle": "2025-02-09T05:44:48.834156Z",
     "shell.execute_reply": "2025-02-09T05:44:48.833385Z",
     "shell.execute_reply.started": "2025-02-09T05:44:48.819240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# out = model(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:48.835174Z",
     "iopub.status.busy": "2025-02-09T05:44:48.834923Z",
     "iopub.status.idle": "2025-02-09T05:44:48.851975Z",
     "shell.execute_reply": "2025-02-09T05:44:48.851305Z",
     "shell.execute_reply.started": "2025-02-09T05:44:48.835153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lr = 5e-5\n",
    "# train it fully\n",
    "no_decay = [\"bias\", \"layerNorm.weight\", \"layerNorm.bias\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:48.853052Z",
     "iopub.status.busy": "2025-02-09T05:44:48.852789Z",
     "iopub.status.idle": "2025-02-09T05:44:48.864468Z",
     "shell.execute_reply": "2025-02-09T05:44:48.863768Z",
     "shell.execute_reply.started": "2025-02-09T05:44:48.853028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "accumulation_steps = 2\n",
    "num_train_optimization_steps = int(EPOCHS * len(train_loader) / accumulation_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0.05 * num_train_optimization_steps,\n",
    "    num_training_steps=num_train_optimization_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:48.865440Z",
     "iopub.status.busy": "2025-02-09T05:44:48.865188Z",
     "iopub.status.idle": "2025-02-09T05:44:48.877378Z",
     "shell.execute_reply": "2025-02-09T05:44:48.876706Z",
     "shell.execute_reply.started": "2025-02-09T05:44:48.865408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def loss_fn(labels, prediction_scores):\n",
    "    shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
    "    labels = labels[:, 1:].contiguous()\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    lm_loss = loss_fct(\n",
    "        shifted_prediction_scores.view(-1, config.vocab_size), labels.view(-1)\n",
    "    )\n",
    "    return lm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:48.878447Z",
     "iopub.status.busy": "2025-02-09T05:44:48.878188Z",
     "iopub.status.idle": "2025-02-09T05:44:48.952685Z",
     "shell.execute_reply": "2025-02-09T05:44:48.951581Z",
     "shell.execute_reply.started": "2025-02-09T05:44:48.878416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:48.953887Z",
     "iopub.status.busy": "2025-02-09T05:44:48.953586Z",
     "iopub.status.idle": "2025-02-09T05:44:48.969344Z",
     "shell.execute_reply": "2025-02-09T05:44:48.968582Z",
     "shell.execute_reply.started": "2025-02-09T05:44:48.953838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader=train_loader, model=model):\n",
    "    best_epoch_loss = np.inf\n",
    "    model.to(device)\n",
    "    accelerator = Accelerator()\n",
    "    Config = {\n",
    "        \"num_epoch\": EPOCHS,\n",
    "        \"learning_rate\": lr,\n",
    "        \"loss_function\": str(torch.nn.CrossEntropyLoss),\n",
    "    }\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        avg_loss = 0.0\n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, file=sys.stdout)\n",
    "        loss_list = []\n",
    "        tbar.set_description(f\"Epoch {epoch+1}\")\n",
    "        for step, data in enumerate(tbar):\n",
    "            data = collate(data)\n",
    "            x = data[\"input_ids\"].to(device)\n",
    "            y = data[\"labels\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(input_ids=x).logits\n",
    "            loss = loss_fn(y, pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            tbar.set_postfix(loss=loss.item())\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "\n",
    "        avg_loss = np.round(np.mean(loss_list), 4)\n",
    "\n",
    "        print(f\"Epoch--{epoch+1} ### Train loss---{avg_loss}\")\n",
    "\n",
    "    PATH = f\"decoder__{epoch}.pth\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    del train_loader\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:44:48.970538Z",
     "iopub.status.busy": "2025-02-09T05:44:48.970242Z",
     "iopub.status.idle": "2025-02-09T05:57:17.241402Z",
     "shell.execute_reply": "2025-02-09T05:57:17.240448Z",
     "shell.execute_reply.started": "2025-02-09T05:44:48.970484Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09870fc26c7f4cb2876755722ba50c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--1 ### Train loss---6.3507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6590c4c98cc145ebbc7cb23b75262173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--2 ### Train loss---5.2408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf65d21e61d43f89d6fddf6466f964e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--3 ### Train loss---5.0446\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c0c56e8b66492789d4fa0f550d1e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--4 ### Train loss---5.0258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf9cdeb94ca44338502ce96bc033b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--5 ### Train loss---5.0258\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:57:17.242895Z",
     "iopub.status.busy": "2025-02-09T05:57:17.242636Z",
     "iopub.status.idle": "2025-02-09T05:57:17.248920Z",
     "shell.execute_reply": "2025-02-09T05:57:17.248024Z",
     "shell.execute_reply.started": "2025-02-09T05:57:17.242871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderModel(\n",
       "  (word_embeddings): Embedding(50257, 768)\n",
       "  (all_layer): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (attention): DecoderAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out): AttentionSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (intermediate): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (act_fn): GELU(approximate='none')\n",
       "        (out): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): LMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50257, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T05:57:17.250146Z",
     "iopub.status.busy": "2025-02-09T05:57:17.249918Z",
     "iopub.status.idle": "2025-02-09T05:57:17.265829Z",
     "shell.execute_reply": "2025-02-09T05:57:17.264921Z",
     "shell.execute_reply.started": "2025-02-09T05:57:17.250127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    max_len: int = 20,\n",
    "    temperature: float = 1.0,\n",
    "    use_cache: bool = True,\n",
    "    do_sample: bool = False,\n",
    "    use_static_cache: bool = False,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    device = input_ids.device\n",
    "\n",
    "    all_prompt_size = [t.size()[0] for t in input_ids]\n",
    "\n",
    "    min_prompt_len = min(all_prompt_size)\n",
    "    max_prompt_len = max(all_prompt_size)\n",
    "\n",
    "    max_len = (\n",
    "        max_len + max_prompt_len\n",
    "    )  # get  max len (prompt + to be generated token combined)\n",
    "\n",
    "    pad_id = getattr(model.config, \"pad_token_id\", 50256)\n",
    "    bsz, _ = input_ids.size()\n",
    "    tokens = torch.full((bsz, max_len), pad_id, dtype=torch.long, device=device)\n",
    "\n",
    "    kv_cache = None\n",
    "    if use_cache:\n",
    "        if use_static_cache:\n",
    "            kv_cache = StaticCache(model.config, max_cache_len=max_len, batch_size=bsz)\n",
    "        else:\n",
    "            kv_cache = DynamicCache(model.config)\n",
    "\n",
    "    for k, t in enumerate(input_ids):\n",
    "        tokens[k, : t.size()[0]] = t\n",
    "\n",
    "    prev_pos = torch.tensor(0, device=device)\n",
    "    eos_reached = torch.tensor([False] * bsz, device=device)\n",
    "    # to break generation if eos reached for all  prompt\n",
    "\n",
    "    input_text_mask = tokens != pad_id  # mask to fill generated values into batch\n",
    "\n",
    "    stop_tokens = torch.tensor(\n",
    "        getattr(model.config, \"eos_token_id\", 50256), device=device\n",
    "    )\n",
    "    for cur_pos in range(min_prompt_len, max_len):\n",
    "\n",
    "        # Get the model output\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=tokens[:, prev_pos:cur_pos],\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=use_cache,\n",
    "                kv_cache=kv_cache,\n",
    "                start_pos=prev_pos,\n",
    "            )\n",
    "        kv_cache = outputs.kv_cache\n",
    "        next_token_logits = outputs.logits[:, -1] / temperature\n",
    "\n",
    "        if do_sample:\n",
    "            next_token = torch.multinomial(next_token_logits, num_samples=1)\n",
    "        else:\n",
    "            _, next_token = torch.topk(next_token_logits, k=1, dim=-1)\n",
    "\n",
    "        next_token = next_token.reshape(-1)\n",
    "        # only replace token if prompt has already been generated\n",
    "        next_token = torch.where(\n",
    "            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "            torch.isin(next_token, stop_tokens)\n",
    "        )\n",
    "\n",
    "        if use_cache:\n",
    "            prev_pos = cur_pos\n",
    "\n",
    "        attention_mask = torch.cat(\n",
    "            [attention_mask, torch.ones((bsz, 1), device=device)], dim=-1\n",
    "        )\n",
    "        if all(eos_reached):\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:01:09.256655Z",
     "iopub.status.busy": "2025-02-09T06:01:09.256264Z",
     "iopub.status.idle": "2025-02-09T06:01:09.261274Z",
     "shell.execute_reply": "2025-02-09T06:01:09.260307Z",
     "shell.execute_reply.started": "2025-02-09T06:01:09.256622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = tokenizer(\n",
    "    [\"this is a test, blue\", \"Well, sir, you could\"], return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:01:09.988371Z",
     "iopub.status.busy": "2025-02-09T06:01:09.988074Z",
     "iopub.status.idle": "2025-02-09T06:01:09.994452Z",
     "shell.execute_reply": "2025-02-09T06:01:09.993745Z",
     "shell.execute_reply.started": "2025-02-09T06:01:09.988349Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 5661,   318,   257,  1332,    11,  4171],\n",
       "        [ 5779,    11, 15967,    11,   345,   714]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:01:10.240654Z",
     "iopub.status.busy": "2025-02-09T06:01:10.240332Z",
     "iopub.status.idle": "2025-02-09T06:01:10.245718Z",
     "shell.execute_reply": "2025-02-09T06:01:10.244905Z",
     "shell.execute_reply.started": "2025-02-09T06:01:10.240626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:01:10.542961Z",
     "iopub.status.busy": "2025-02-09T06:01:10.542689Z",
     "iopub.status.idle": "2025-02-09T06:01:10.547088Z",
     "shell.execute_reply": "2025-02-09T06:01:10.546363Z",
     "shell.execute_reply.started": "2025-02-09T06:01:10.542937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_ids, attention_mask = text[\"input_ids\"], text[\"attention_mask\"]\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:01:10.956052Z",
     "iopub.status.busy": "2025-02-09T06:01:10.955788Z",
     "iopub.status.idle": "2025-02-09T06:01:11.096755Z",
     "shell.execute_reply": "2025-02-09T06:01:11.096058Z",
     "shell.execute_reply.started": "2025-02-09T06:01:10.956028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a test, blue-day, and the  first time the same.  The boys were not a good deal.',\n",
       " 'Well, sir, you couldnt  be aint no, but I was a good deal. I was a good deal of']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = generate(\n",
    "    model, input_ids=input_ids, attention_mask=attention_mask, use_cache=False\n",
    ")\n",
    "\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:01:12.650286Z",
     "iopub.status.busy": "2025-02-09T06:01:12.649984Z",
     "iopub.status.idle": "2025-02-09T06:01:12.759615Z",
     "shell.execute_reply": "2025-02-09T06:01:12.758934Z",
     "shell.execute_reply.started": "2025-02-09T06:01:12.650260Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a test, blue-day, and the  first time the same.  The boys were not a good deal.',\n",
       " 'Well, sir, you couldnt  be aint no, but I was a good deal. I was a good deal of']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = generate(\n",
    "    model, input_ids=input_ids, attention_mask=attention_mask, use_cache=True\n",
    ")\n",
    "\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:01:18.618028Z",
     "iopub.status.busy": "2025-02-09T06:01:18.617745Z",
     "iopub.status.idle": "2025-02-09T06:01:18.732152Z",
     "shell.execute_reply": "2025-02-09T06:01:18.731468Z",
     "shell.execute_reply.started": "2025-02-09T06:01:18.618005Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a test, blue-day, and the  first time the same.  The boys were not a good deal.',\n",
       " 'Well, sir, you couldnt  be aint no, but I was a good deal. I was a good deal of']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = generate(\n",
    "    model,\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    use_cache=True,\n",
    "    use_static_cache=True,\n",
    ")\n",
    "\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3277362,
     "sourceId": 5699740,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 129796172,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
