{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:31.401396Z","iopub.status.busy":"2024-05-03T18:51:31.400601Z","iopub.status.idle":"2024-05-03T18:51:42.214641Z","shell.execute_reply":"2024-05-03T18:51:42.213706Z","shell.execute_reply.started":"2024-05-03T18:51:31.401364Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import re\n","import sys\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pickle\n","import os\n","import time\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import warnings\n","import gc\n","from accelerate import Accelerator\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","warnings.simplefilter(\"ignore\")\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import AutoModel, AutoTokenizer, AutoModelWithLMHead, AutoConfig\n","from tqdm.notebook import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:42.216786Z","iopub.status.busy":"2024-05-03T18:51:42.216463Z","iopub.status.idle":"2024-05-03T18:51:54.435990Z","shell.execute_reply":"2024-05-03T18:51:54.434747Z","shell.execute_reply.started":"2024-05-03T18:51:42.216760Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install einops"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:54.438464Z","iopub.status.busy":"2024-05-03T18:51:54.438147Z","iopub.status.idle":"2024-05-03T18:51:54.471980Z","shell.execute_reply":"2024-05-03T18:51:54.470924Z","shell.execute_reply.started":"2024-05-03T18:51:54.438435Z"},"trusted":true},"outputs":[],"source":["from einops import rearrange, reduce\n","from typing import Optional, Tuple, Union, List\n","from dataclasses import dataclass\n","\n","\n","class AbsoluteEncoding(nn.Module):\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        self.pos_embeddings = nn.Embedding(\n","            config.max_position_embeddings, config.hidden_size\n","        )\n","        self.register_buffer(\n","            \"position_ids\",\n","            torch.arange(config.max_position_embeddings).expand((1, -1)),\n","            persistent=False,\n","        )\n","        self.max_size = config.max_position_embeddings\n","\n","    def forward(self, size: int) -> torch.Tensor:\n","        if self.max_size < size:\n","            raise ValueError(\n","                f\"The hidden size ({size }) is more than the config max_position_embeddings {self.max_size}\"\n","            )\n","        return self.pos_embeddings(self.position_ids[:, :size])\n","\n","\n","class SinusoidalEncoding(nn.Module):\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        if config.hidden_size % 2 != 0:\n","            raise ValueError(\n","                f\"Cannot use SinusoidalEncoding with \"\n","                \"odd hidden dim got dim {config.hidden_size}\"\n","            )\n","        self.positional_encoding = torch.zeros(\n","            1, config.max_position_embeddings, config.hidden_size\n","        )\n","        self.position = torch.arange(0, config.max_position_embeddings).unsqueeze(1)\n","        self.div_term = torch.exp(\n","            (\n","                torch.arange(0, config.hidden_size, 2, dtype=torch.float)\n","                * -(torch.log(torch.tensor(10000.0)) / config.hidden_size)\n","            )\n","        )\n","\n","        self.positional_encoding[:, :, 0::2] = torch.sin(\n","            self.position.float() * self.div_term\n","        )\n","        self.positional_encoding[:, :, 1::2] = torch.cos(\n","            self.position.float() * self.div_term\n","        )\n","\n","    def forward(self, seq_len: int) -> torch.Tensor:\n","\n","        return self.positional_encoding[:, :seq_len]\n","\n","\n","# copied from transformer/models/gemma\n","class RotaryEmbedding(nn.Module):\n","    def __init__(self, config, base=10000, device=None):\n","        super().__init__()\n","\n","        self.dim = int(config.hidden_size // config.num_attention_heads)\n","        self.max_position_embeddings = config.max_position_embeddings\n","        self.base = base\n","        self.register_buffer(\n","            \"inv_freq\",\n","            1.0\n","            / (\n","                self.base\n","                ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim)\n","            ),\n","            persistent=False,\n","        )\n","        self.register_buffer(\n","            \"position_ids\",\n","            torch.arange(config.max_position_embeddings).expand((1, -1)),\n","            persistent=False,\n","        )\n","\n","    @torch.no_grad()\n","    def forward(self, seq_len: int = None):\n","        # x: [bs, num_attention_heads, seq_len, head_size]\n","        # size = x.size()[2]\n","        position_ids = torch.arange(seq_len).unsqueeze(0)\n","        # position_ids = self.position_ids[:, :size].float()\n","\n","        inv_freq_expanded = (\n","            self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n","        )\n","        position_ids_expanded = position_ids[:, None, :].float()\n","\n","        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(\n","            1, 2\n","        )\n","        return freqs\n","\n","\n","# Copied from transformers.models.llama.modeling_llama.rotate_half\n","def rotate_half(x):\n","    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n","    x1 = x[..., : x.shape[-1] // 2]\n","    x2 = x[..., x.shape[-1] // 2 :]\n","    return torch.cat((-x2, x1), dim=-1)\n","\n","\n","# def rotate_half(x):\n","#     x1, x2 = x.chunk(2, dim=-1)\n","#     return torch.cat((-x2, x1), dim=-1)\n","\n","\n","# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n","def apply_rotary_pos_emb(\n","    q, k, freqs, only_q: bool = False, unsqueeze_dim=1\n",") -> Tuple[torch.Tensor]:\n","    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n","\n","    Args:\n","        q (`torch.Tensor`): The query tensor.\n","        k (`torch.Tensor`): The key tensor.\n","        freqs: precalculated frqs for sin cos\n","        only_q: bool = False for encoder decoder\n","        unsqueeze_dim (`int`, *optional*, defaults to 1):\n","            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n","            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n","            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n","            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n","            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n","            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n","    Returns:\n","        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n","    \"\"\"\n","    emb = torch.cat((freqs, freqs), dim=-1)\n","    cos = emb.cos()\n","    sin = emb.sin()\n","    cos = cos.unsqueeze(unsqueeze_dim)\n","    sin = sin.unsqueeze(unsqueeze_dim)\n","    #     print(cos.size(),sin.size(),q.size(),k.size())\n","    if only_q:\n","        q_embed = (q * cos) + (rotate_half(q) * sin)\n","    else:\n","\n","        q_embed = (q * cos) + (rotate_half(q) * sin)\n","        k_embed = (k * cos) + (rotate_half(k) * sin)\n","        return q_embed, k_embed\n","\n","\n","# To do :  Alibi"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:54.475691Z","iopub.status.busy":"2024-05-03T18:51:54.475027Z","iopub.status.idle":"2024-05-03T18:51:54.506720Z","shell.execute_reply":"2024-05-03T18:51:54.505871Z","shell.execute_reply.started":"2024-05-03T18:51:54.475663Z"},"trusted":true},"outputs":[],"source":["def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n","    \"\"\"\n","    torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n","     num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n","    \"\"\"\n","    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n","    if n_rep == 1:\n","        return hidden_states\n","    hidden_states = hidden_states[:, :, None, :, :].expand(\n","        batch, num_key_value_heads, n_rep, slen, head_dim\n","    )\n","    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n","\n","\n","def repeat_kv_einops(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n","    \"\"\"\n","    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n","    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n","    \"\"\"\n","    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n","    if n_rep == 1:\n","        return hidden_states\n","    hidden_states = repeat(\n","        hidden_states,\n","        \"batch num_key_value_heads slen head_dim -> batch num_key_value_heads n_rep slen head_dim\",\n","        n_rep=n_rep,\n","    )  # hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n","    # return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n","    return rearrange(\n","        hidden_states,\n","        \"batch num_key_value_heads n_rep slen head_dim -> batch (num_key_value_heads n_rep) slen head_dim\",\n","    )\n","\n","\n","class DecoderAttention(nn.Module):\n","    def __init__(self, config, layer_idx: int) -> None:\n","        super().__init__()\n","        if config.hidden_size % config.num_attention_heads != 0:\n","            raise ValueError(\n","                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n","                f\"heads ({config.num_attention_heads})\"\n","            )\n","        self.head_size = int(config.hidden_size // config.num_attention_heads)\n","        self.attention_bias = getattr(config, \"attention_bias\", True)\n","        self.layer_idx = layer_idx\n","        # self.qkv = nn.Linear(config.hidden_size,3*config.hidden_size)\n","        self.q = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.k = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.v = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.out = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.num_attention_heads = config.num_attention_heads\n","        self.rotary_emb = (\n","            RotaryEmbedding(config=config) if getattr(config, \"is_rope\", None) else None\n","        )\n","        if self.rotary_emb != None and self.layer_idx == 0:  # avoid to print m times:\n","            print(\"Decoder Using Rotatry Embedding\")\n","        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n","        if not self.flash and self.layer_idx == 0:  # avoid to print m times:\n","            print(\"WARNING: Flash Attention requires PyTorch >= 2.0\")\n","\n","    def forward(\n","        self,\n","        hidden_state: torch.Tensor,\n","        attention_mask: torch.Tensor,\n","        freqs: Optional[torch.Tensor] = None,\n","        use_cache: Optional[bool] = False,\n","        start_pos: Optional[int] = 0,\n","    ) -> Tuple[torch.Tensor, object]:\n","        q = self.q(hidden_state)\n","        k = self.k(hidden_state)\n","        v = self.v(hidden_state)\n","        # q,k,v = self.qkv(hidden_state).chunk(3, dim = -1) #b X l X d dim =-1 or 2\n","        # place holder for RoPe operation\n","        q = rearrange(q, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n","        k = rearrange(k, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n","        v = rearrange(v, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n","\n","        if freqs is not None:\n","            q, k = apply_rotary_pos_emb(q, k, freqs)\n","\n","        if use_cache:\n","            cache = getattr(self, \"cache\", None)\n","            if cache is None:\n","                raise ValueError(\n","                    \"you need to setup cache for every attention layer with model.setup_cache()\"\n","                )\n","            k, v = cache.update(k, v, start_pos)\n","\n","        out = torch.nn.functional.scaled_dot_product_attention(\n","            query=q, key=k, value=v, attn_mask=attention_mask\n","        )\n","        out = rearrange(out, \"b h l d -> b l (h d)\")\n","        out = self.out(out)\n","        return out\n","\n","\n","class DecoderAttentionGqa(nn.Module):\n","    def __init__(self, config, layer_idx: int) -> None:\n","        super().__init__()\n","        if config.hidden_size % config.num_attention_heads != 0:\n","            raise ValueError(\n","                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n","                f\"heads ({config.num_attention_heads})\"\n","            )\n","        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n","        if not self.flash and self.layer_idx == 0:  # avoid to print m times\n","            print(\"WARNING: Flash Attention requires PyTorch >= 2.0\")\n","        self.layer_idx = layer_idx\n","        self.head_dim = int(config.hidden_size // config.num_attention_heads)\n","        self.num_attention_heads = config.num_attention_heads\n","        self.num_key_value_heads = getattr(config, \"num_key_value_heads\", 4)\n","        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads\n","        if (\n","            self.num_attention_heads % self.num_key_value_heads != 0\n","            or self.num_attention_heads < self.num_key_value_heads\n","        ):\n","            raise ValueError(\n","                f\"num_key_value_heads {self.num_key_value_heads }  should be less than equal num_attention_heads {config.num_attention_heads} and  multiple of num_attention_heads {config.num_attention_heads} \"\n","            )\n","        self.attention_bias = getattr(config, \"attention_bias\", True)\n","        self.out = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.q = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.k = nn.Linear(\n","            config.hidden_size,\n","            self.num_key_value_heads * self.head_dim,\n","            bias=self.attention_bias,\n","        )\n","        self.v = nn.Linear(\n","            config.hidden_size,\n","            self.num_key_value_heads * self.head_dim,\n","            bias=self.attention_bias,\n","        )\n","\n","    def forward(\n","        self,\n","        hidden_state: torch.Tensor,\n","        attention_mask: torch.Tensor,\n","        freqs: Optional[torch.Tensor] = None,\n","        use_cache: Optional[bool] = False,\n","        start_pos: Optional[int] = 0,\n","    ) -> torch.Tensor:\n","        q = self.q(hidden_state)\n","        k = self.k(hidden_state)\n","        v = self.v(hidden_state)\n","        q = rearrange(q, \"b l (h d) -> b h l d\", d=self.head_dim)\n","        k = rearrange(k, \"b l (h d) -> b h l d\", d=self.head_dim)\n","        v = rearrange(v, \"b l (h d) -> b h l d\", d=self.head_dim)\n","        if freqs is not None:\n","            q, k = apply_rotary_pos_emb(q, k, freqs)\n","\n","        if use_cache:\n","            cache = getattr(self, \"cache\", None)\n","            if cache is None:\n","                raise ValueError(\n","                    \"you need to setup cache for every attention layer with model._setup_cache() before using it\"\n","                )\n","            k, v = cache.update(k, v, start_pos)\n","\n","        k = repeat_kv(k, n_rep=self.num_key_value_groups)\n","        v = repeat_kv(v, n_rep=self.num_key_value_groups)\n","\n","        out = torch.nn.functional.scaled_dot_product_attention(\n","            query=q, key=k, value=v, attn_mask=attention_mask\n","        )\n","        out = rearrange(out, \"b h l d -> b l (h d)\")\n","        out = self.out(out)\n","        return out"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:54.508042Z","iopub.status.busy":"2024-05-03T18:51:54.507786Z","iopub.status.idle":"2024-05-03T18:51:54.521700Z","shell.execute_reply":"2024-05-03T18:51:54.520897Z","shell.execute_reply.started":"2024-05-03T18:51:54.508020Z"},"trusted":true},"outputs":[],"source":["_ACT_ = {\n","    \"gelu\": nn.GELU(),\n","    \"leaky_relu\": nn.LeakyReLU(),\n","    \"relu6\": nn.ReLU6(),\n","    \"sigmoid\": nn.Sigmoid(),\n","    \"silu\": nn.SiLU(),\n","    \"swish\": nn.SiLU(),\n","    \"tanh\": nn.Tanh(),\n","}\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, config, multiplier: Union[int, float] = 4) -> None:\n","        super().__init__()\n","        self.intermediate = nn.Linear(\n","            config.hidden_size, int(multiplier) * config.hidden_size\n","        )\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.layerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        if _ACT_.get(getattr(config, \"hidden_act\", None), None):\n","            self.act_fn = _ACT_[config.hidden_act]\n","        else:\n","            self.act_fn = nn.GELU()\n","        self.out = nn.Linear(int(multiplier) * config.hidden_size, config.hidden_size)\n","\n","    def forward(\n","        self, hidden_state: torch.Tensor, input_tensor: torch.Tensor\n","    ) -> torch.Tensor:\n","        output = self.intermediate(hidden_state)\n","        output = self.act_fn(output)\n","        output = self.out(output)\n","        output = self.dropout(output)\n","        output = self.layerNorm(output + input_tensor)\n","        return output"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:54.523065Z","iopub.status.busy":"2024-05-03T18:51:54.522793Z","iopub.status.idle":"2024-05-03T18:51:54.542022Z","shell.execute_reply":"2024-05-03T18:51:54.541213Z","shell.execute_reply.started":"2024-05-03T18:51:54.523042Z"},"trusted":true},"outputs":[],"source":["class DynamicCache:\n","    \"\"\"\n","    A cache that grows dynamically as more tokens are generated.\n","\n","    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n","    `[batch_size, num_heads, seq_len, head_dim]`.\n","    \"\"\"\n","\n","    def __init__(self, config) -> None:\n","        self.key_cache: torch.Tensor = None\n","        self.value_cache: torch.Tensor = None\n","        self._seen_tokens = False\n","\n","    def __len__(self) -> int:\n","        if self.key_cache is None:\n","            return 0\n","        \"\"\"\n","        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n","        to the number of layers in the model.\n","        \"\"\"\n","        return self.key_cache.shape[-2]\n","\n","    def update(\n","        self, key_states: torch.Tensor, value_states: torch.Tensor, start_pos: int = 0\n","    ) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n","\n","        Parameters:\n","            key_states (`torch.Tensor`):\n","                The new key states to cache.\n","            value_states (`torch.Tensor`):\n","                The new value states to cache.\n","            layer_idx (`int`):\n","                The index of the layer to cache the states for.\n","            cache_kwargs (`Dict[str, Any]`, `optional`):\n","                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n","\n","        Return:\n","            A tuple containing the updated key and value states.\n","        \"\"\"\n","\n","        # Update the cache first iteration\n","        if self.key_cache is None:\n","            self._seen_tokens = True\n","            self.key_cache = key_states.clone()\n","            self.value_cache = value_states.clone()\n","        else:\n","            self.key_cache = torch.cat([self.key_cache, key_states], dim=-2)\n","            self.value_cache = torch.cat([self.value_cache, value_states], dim=-2)\n","\n","        return self.key_cache, self.value_cache\n","\n","    def get(self) -> Tuple[torch.Tensor]:\n","        if self._seen_tokens:\n","            return self.key_cache, self.value_cache\n","        else:\n","            raise ValueError(\"there is no token available in kv-cache\")\n","\n","    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n","        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n","        if self.key_cache is None:\n","            return 0\n","        return self.key_cache.shape[-2]\n","\n","    def get_max_length(self) -> Optional[int]:\n","        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n","        return None\n","\n","\n","class StaticCache:\n","    \"\"\"\n","    A cache that is size fixed suitable for torch.compile\n","\n","    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n","    `[batch_size, num_heads, seq_len, head_dim]`.\n","    \"\"\"\n","\n","    def __init__(self, config) -> None:\n","        self.head_size = int(config.hidden_size // config.num_attention_heads)\n","        self.heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n","        self.key_cache: torch.Tensor = torch.zeros(\n","            1,\n","            self.heads,\n","            config.max_position_embeddings,\n","            self.head_size,\n","        )\n","        self.value_cache: torch.Tensor = torch.zeros(\n","            1,\n","            self.heads,\n","            config.max_position_embeddings,\n","            self.head_size,\n","        )\n","        self._seen_tokens = False\n","\n","    def update(\n","        self, k: torch.Tensor, v: torch.Tensor, start_pos: int = 0\n","    ) -> Tuple[torch.Tensor]:\n","        self._seen_tokens = True\n","        bsz, head, seqlen, _ = k.shape\n","        assert bsz == 1, \"Only support batch size 1\"\n","\n","        self.key_cache = self.key_cache.to(k)\n","        self.value_cache = self.value_cache.to(v)\n","\n","        self.key_cache[:bsz, :, start_pos : start_pos + seqlen] = k\n","        self.value_cache[:bsz, :, start_pos : start_pos + seqlen] = v\n","\n","        k = self.key_cache[:bsz, :, : start_pos + seqlen]\n","        v = self.value_cache[:bsz, :, : start_pos + seqlen]\n","\n","        return k, v\n","\n","    def get(self) -> Tuple[torch.Tensor]:\n","        if self._seen_tokens:\n","            return self.key_cache, self.value_cache\n","        else:\n","            raise ValueError(\"there is no token available in kv-cache\")\n","\n","    def __len__(self) -> int:\n","        if self._seen_tokens == False:\n","            return 0\n","        \"\"\"\n","        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n","        to the number of layers in the model.\n","        \"\"\"\n","        return self.key_cache.shape[2]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:54.543773Z","iopub.status.busy":"2024-05-03T18:51:54.543431Z","iopub.status.idle":"2024-05-03T18:51:54.575339Z","shell.execute_reply":"2024-05-03T18:51:54.574354Z","shell.execute_reply.started":"2024-05-03T18:51:54.543748Z"},"trusted":true},"outputs":[],"source":["# GPT style model Casual language modeling\n","\n","_position_embeddings = {\n","    \"absolute\": AbsoluteEncoding,\n","    \"sinusoidal\": SinusoidalEncoding,\n","}\n","\n","\n","@dataclass\n","class DecoderOutput(object):\n","    logits: torch.Tensor\n","    past_key_value: Optional[object]\n","\n","\n","@dataclass\n","class CLMOutput(object):\n","    hidden_state: torch.Tensor\n","    logits: torch.Tensor\n","\n","\n","class DecoderLayer(nn.Module):\n","\n","    def __init__(self, config, layer_idx: int, attention_type: str = None) -> None:\n","        super().__init__()\n","        self.attention = (\n","            DecoderAttentionGqa(config, layer_idx=layer_idx)\n","            if attention_type == \"gqa\"\n","            else DecoderAttention(config, layer_idx=layer_idx)\n","        )\n","        if attention_type == \"gqa\" and layer_idx == 0:  # avoid to print m times\n","            print(\"Decoder Using GQA Attention\")\n","        self.feed_forward = FeedForward(config)\n","        self.layer_idx = layer_idx\n","\n","    def forward(\n","        self,\n","        hidden_state: torch.Tensor,\n","        attention_mask: torch.Tensor,\n","        freqs: Optional[torch.Tensor] = None,\n","        use_cache: Optional[bool] = False,\n","        start_pos: Optional[int] = 0,\n","    ) -> torch.Tensor:\n","        out = self.attention(\n","            hidden_state=hidden_state,\n","            attention_mask=attention_mask,\n","            freqs=freqs,\n","            use_cache=use_cache,\n","            start_pos=start_pos,\n","        )\n","        out = self.feed_forward(out, hidden_state)\n","        return out\n","\n","\n","class LMHead(nn.Module):\n","    \"\"\"Head for masked language modelling\"\"\"\n","\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.layerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","\n","        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n","        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n","        self.decoder.bias = self.bias\n","\n","    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n","        x = self.dense(hidden_state)\n","        x = nn.GELU()(x)\n","        x = self.layerNorm(x)\n","\n","        # project back to size of vocabulary with bias\n","        x = self.decoder(x)\n","\n","        return x\n","\n","\n","class DecoderModel(nn.Module):\n","\n","    def __init__(\n","        self,\n","        config,\n","        pos_embedding_type: Optional[str] = \"absolute\",\n","        attention_type: str = None,\n","    ) -> None:\n","        super().__init__()\n","        self.word_embeddings = nn.Embedding(\n","            config.vocab_size,\n","            config.hidden_size,\n","            padding_idx=getattr(config, \"pad_token_id\", None),\n","        )\n","        if _position_embeddings.get(pos_embedding_type, None) is not None:\n","            self.position_embeddings = _position_embeddings.get(pos_embedding_type)(\n","                config\n","            )\n","        else:\n","            self.position_embeddings = None\n","        if pos_embedding_type == \"rope\":\n","            self.emb_freq = RotaryEmbedding(config)(config.max_position_embeddings)\n","            print(\n","                \"Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\"\n","            )\n","        self.all_layer = nn.ModuleList(\n","            [\n","                DecoderLayer(config, layer_idx, attention_type)\n","                for layer_idx in range(config.num_hidden_layers)\n","            ]\n","        )\n","        self.lm_head = LMHead(config=config)\n","\n","    def _init_weights(self, module: nn.Module) -> None:\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(\n","                module.weight, mean=0.0, std=0.02 / torch.sqrt(2 * len(self.all_layer))\n","            )\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(\n","                module.weight, mean=0.0, std=0.02 / torch.sqrt(2 * len(self.all_layer))\n","            )\n","\n","    def forward(\n","        self,\n","        input_ids: torch.Tensor,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        use_cache: Optional[bool] = False,\n","        start_pos: Optional[int] = 0,\n","    ) -> torch.Tensor:\n","        _bsz, seqlen = input_ids.shape\n","        hidden_state = self.word_embeddings(input_ids)\n","        freqs = None\n","        if self.position_embeddings is not None:\n","            pos_info = self.position_embeddings(start_pos + seqlen)[\n","                :, start_pos : start_pos + seqlen, :\n","            ].to(input_ids.device)\n","            hidden_state = hidden_state + pos_info\n","        else:\n","            freqs = self.emb_freq[:, start_pos : start_pos + seqlen].to(\n","                input_ids.device\n","            )\n","        mask = None\n","        if seqlen > 1:\n","            mask = self.create_mask_for_decoder(\n","                input_ids=input_ids, attention_mask=attention_mask, start_pos=start_pos\n","            )\n","            mask = (1.0 - mask) * torch.finfo(\n","                hidden_state.dtype\n","            ).min  # invert it to to add directly to attention score\n","\n","        for layer in self.all_layer:\n","            hidden_state = layer(\n","                hidden_state,\n","                mask,\n","                freqs=freqs,\n","                use_cache=use_cache,\n","                start_pos=start_pos,\n","            )\n","        logits = self.lm_head(hidden_state)\n","        return CLMOutput(hidden_state=hidden_state, logits=logits)\n","\n","    def create_mask_for_decoder(\n","        self,\n","        input_ids,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        start_pos: Optional[int] = 0,\n","    ) -> torch.Tensor:\n","        device = input_ids.device\n","        batch_size, seq_length = input_ids.shape\n","        if attention_mask is None:\n","            attention_mask = (\n","                torch.ones(seq_length + start_pos).repeat(batch_size, 1).to(device)\n","            )\n","        seq_ids = torch.arange(seq_length).to(device)\n","        causal_mask = (\n","            seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n","            <= seq_ids[None, :, None]\n","        )  # 1x1xl repeat bxlxl compare to 1xlx1\n","\n","        causal_mask = causal_mask.to(attention_mask.dtype)\n","\n","        if start_pos > 0:  # correct the attention mask  for kv-cache operation\n","            causal_mask = torch.cat(\n","                [\n","                    torch.ones(\n","                        (batch_size, seq_length, start_pos),\n","                        device=device,\n","                        dtype=causal_mask.dtype,\n","                    ),\n","                    causal_mask,\n","                ],\n","                axis=-1,\n","            )\n","\n","        extended_attention_mask = (\n","            causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n","        )  # # this is mainly if batch contains <PAD> tokens. stop casual procees before <PAD>\n","        return extended_attention_mask\n","\n","    @classmethod\n","    def from_config(\n","        cls,\n","        config,\n","        pos_embedding_type: Optional[str] = \"absolute\",\n","        attention_type: Optional[str] = None,\n","    ) -> nn.Module:\n","        return cls(config, pos_embedding_type, attention_type)\n","\n","    def _setup_cache(self, config, cls: Optional[object] = StaticCache) -> None:\n","        for layer in self.all_layer:\n","            layer.attention.cache = cls(config)\n","\n","    def _clean_cache(self) -> None:\n","        for layer in self.all_layer:\n","            layer.attention.cache = None"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:54.576882Z","iopub.status.busy":"2024-05-03T18:51:54.576546Z","iopub.status.idle":"2024-05-03T18:51:54.907058Z","shell.execute_reply":"2024-05-03T18:51:54.906213Z","shell.execute_reply.started":"2024-05-03T18:51:54.576851Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89ceacd73db04b45bb57226ee7292d88","version_major":2,"version_minor":0},"text/plain":["Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from einops import rearrange\n","\n","model_ckpt = \"roberta-base\"\n","config = AutoConfig.from_pretrained(model_ckpt)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:54.908437Z","iopub.status.busy":"2024-05-03T18:51:54.908161Z","iopub.status.idle":"2024-05-03T18:51:59.190387Z","shell.execute_reply":"2024-05-03T18:51:59.189612Z","shell.execute_reply.started":"2024-05-03T18:51:54.908414Z"},"trusted":true},"outputs":[],"source":["m = AutoModelWithLMHead.from_pretrained(\n","    \"../input/transformer-distilation-gpt-2/gpt2_6L\"\n",")\n","# config = AutoConfig.from_pretrained('../input/transformer-distilation-gpt-2/gpt2_6L')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:59.195034Z","iopub.status.busy":"2024-05-03T18:51:59.194767Z","iopub.status.idle":"2024-05-03T18:51:59.200529Z","shell.execute_reply":"2024-05-03T18:51:59.199535Z","shell.execute_reply.started":"2024-05-03T18:51:59.195011Z"},"trusted":true},"outputs":[],"source":["state_dict = m.state_dict()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:59.202242Z","iopub.status.busy":"2024-05-03T18:51:59.201908Z","iopub.status.idle":"2024-05-03T18:51:59.326376Z","shell.execute_reply":"2024-05-03T18:51:59.325458Z","shell.execute_reply.started":"2024-05-03T18:51:59.202212Z"},"trusted":true},"outputs":[{"data":{"text/plain":["6588596"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["string = open(\n","    \"/kaggle/input/mark-twain-books/Combine.txt\", encoding=\"utf8\", errors=\"ignore\"\n",").read()\n","new_str = re.sub(\"�\", \"\", string)\n","open(\"Train.txt\", \"w\").write(new_str)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:59.328478Z","iopub.status.busy":"2024-05-03T18:51:59.327817Z","iopub.status.idle":"2024-05-03T18:51:59.332545Z","shell.execute_reply":"2024-05-03T18:51:59.331659Z","shell.execute_reply.started":"2024-05-03T18:51:59.328442Z"},"trusted":true},"outputs":[],"source":["config.vocab_size = 50257\n","config.num_hidden_layers = 6\n","# config.is_rope = True"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:51:59.333884Z","iopub.status.busy":"2024-05-03T18:51:59.333576Z","iopub.status.idle":"2024-05-03T18:52:00.504232Z","shell.execute_reply":"2024-05-03T18:52:00.503403Z","shell.execute_reply.started":"2024-05-03T18:51:59.333860Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\n"]}],"source":["model = DecoderModel.from_config(config, pos_embedding_type=\"rope\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:00.505590Z","iopub.status.busy":"2024-05-03T18:52:00.505300Z","iopub.status.idle":"2024-05-03T18:52:00.519059Z","shell.execute_reply":"2024-05-03T18:52:00.518362Z","shell.execute_reply.started":"2024-05-03T18:52:00.505565Z"},"trusted":true},"outputs":[],"source":["model.word_embeddings = nn.Embedding.from_pretrained(\n","    state_dict[\"transformer.wte.weight\"], freeze=False\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**Copy Embedding for faster convergence**"]},{"cell_type":"markdown","metadata":{},"source":["**Data Source**\n","\n","https://www.kaggle.com/datasets/msinger007/mark-twain-books"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:00.520324Z","iopub.status.busy":"2024-05-03T18:52:00.520081Z","iopub.status.idle":"2024-05-03T18:52:02.549625Z","shell.execute_reply":"2024-05-03T18:52:02.548806Z","shell.execute_reply.started":"2024-05-03T18:52:00.520303Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"344c590ca20241f59853057724ad8127","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cd71b33c6274fd1a693fdaa937649b7","version_major":2,"version_minor":0},"text/plain":["Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"248c86f07736486c9a07d2e01c15385b","version_major":2,"version_minor":0},"text/plain":["Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07136c1fa83548b496c1221c0c84c43f","version_major":2,"version_minor":0},"text/plain":["Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a0c063f96024d9a98856dc4ebd101f9","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","\n","train_path = \"Train.txt\""]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:02.551190Z","iopub.status.busy":"2024-05-03T18:52:02.550831Z","iopub.status.idle":"2024-05-03T18:52:02.563361Z","shell.execute_reply":"2024-05-03T18:52:02.562298Z","shell.execute_reply.started":"2024-05-03T18:52:02.551156Z"},"trusted":true},"outputs":[],"source":["class TextDataset(Dataset):\n","\n","    def __init__(self, tokenizer, file_path: str, block_size: int):\n","        if os.path.isfile(file_path) is False:\n","            raise ValueError(f\"Input file path {file_path} not found\")\n","\n","        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n","        saved = False\n","        cache_dir = None\n","        directory, filename = os.path.split(file_path)\n","        cached_features_file = os.path.join(\n","            cache_dir if cache_dir is not None else directory,\n","            f\"cached_lm_{tokenizer.__class__.__name__}_{block_size}_{filename}\",\n","        )\n","\n","        if os.path.exists(cached_features_file) and saved:\n","            start = time.time()\n","            with open(cached_features_file, \"rb\") as handle:\n","                self.examples = pickle.load(handle)\n","        #                 logger.info(\n","        #                     f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n","        #                 )\n","\n","        else:\n","            #                 logger.info(f\"Creating features from dataset file at {directory}\")\n","\n","            self.examples = []\n","            with open(file_path, encoding=\"utf-8\") as f:\n","                text = f.read()\n","\n","            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n","\n","            for i in range(\n","                0, len(tokenized_text) - block_size + 1, block_size\n","            ):  # Truncate in block of block_size\n","                self.examples.append(\n","                    tokenizer.build_inputs_with_special_tokens(\n","                        tokenized_text[i : i + block_size]\n","                    )\n","                )\n","            # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n","            # If your dataset is small, first you should look for a bigger one :-) and second you\n","            # can change this behavior by adding (model specific) padding.\n","\n","            start = time.time()\n","            with open(cached_features_file, \"wb\") as handle:\n","                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","                saved = True\n","\n","    #                 logger.info(\n","    #                     f\"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]\"\n","    #                 )\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, i) -> torch.Tensor:\n","        return {\"input_ids\": torch.tensor(self.examples[i], dtype=torch.long)}"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:02.564660Z","iopub.status.busy":"2024-05-03T18:52:02.564397Z","iopub.status.idle":"2024-05-03T18:52:02.581615Z","shell.execute_reply":"2024-05-03T18:52:02.580735Z","shell.execute_reply.started":"2024-05-03T18:52:02.564638Z"},"trusted":true},"outputs":[],"source":["def collate(batch):\n","    labels = batch[\"input_ids\"].clone()\n","    if tokenizer.pad_token_id is not None:\n","        labels[labels == tokenizer.pad_token_id] = -100\n","    batch[\"labels\"] = labels\n","    return batch"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:02.583807Z","iopub.status.busy":"2024-05-03T18:52:02.582937Z","iopub.status.idle":"2024-05-03T18:52:11.391244Z","shell.execute_reply":"2024-05-03T18:52:11.390444Z","shell.execute_reply.started":"2024-05-03T18:52:02.583777Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1580900 > 1024). Running this sequence through the model will result in indexing errors\n"]}],"source":["train_loader = torch.utils.data.DataLoader(\n","    TextDataset(tokenizer, train_path, 128), batch_size=24, shuffle=True, num_workers=2\n",")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:19.956819Z","iopub.status.busy":"2024-05-03T18:52:19.956112Z","iopub.status.idle":"2024-05-03T18:52:19.993685Z","shell.execute_reply":"2024-05-03T18:52:19.992873Z","shell.execute_reply.started":"2024-05-03T18:52:19.956784Z"},"trusted":true},"outputs":[],"source":["lr = 5e-5\n","# train it fully\n","no_decay = [\"bias\", \"layerNorm.weight\", \"layerNorm.bias\"]\n","optimizer_grouped_parameters = [\n","    {\n","        \"params\": [\n","            p\n","            for n, p in model.named_parameters()\n","            if not any(nd in n for nd in no_decay)\n","        ],\n","        \"weight_decay\": 0.01,\n","    },\n","    {\n","        \"params\": [\n","            p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n","        ],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=lr)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:20.636726Z","iopub.status.busy":"2024-05-03T18:52:20.636352Z","iopub.status.idle":"2024-05-03T18:52:20.641854Z","shell.execute_reply":"2024-05-03T18:52:20.640933Z","shell.execute_reply.started":"2024-05-03T18:52:20.636690Z"},"trusted":true},"outputs":[],"source":["EPOCHS = 7\n","accumulation_steps = 2\n","num_train_optimization_steps = int(EPOCHS * len(train_loader) / accumulation_steps)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0.05 * num_train_optimization_steps,\n","    num_training_steps=num_train_optimization_steps,\n",")"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:21.200515Z","iopub.status.busy":"2024-05-03T18:52:21.200171Z","iopub.status.idle":"2024-05-03T18:52:21.207373Z","shell.execute_reply":"2024-05-03T18:52:21.206437Z","shell.execute_reply.started":"2024-05-03T18:52:21.200476Z"},"trusted":true},"outputs":[],"source":["def loss_fn(labels, prediction_scores):\n","    shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n","    labels = labels[:, 1:].contiguous()\n","    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n","    lm_loss = loss_fct(\n","        shifted_prediction_scores.view(-1, config.vocab_size), labels.view(-1)\n","    )\n","    return lm_loss"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:21.668845Z","iopub.status.busy":"2024-05-03T18:52:21.668498Z","iopub.status.idle":"2024-05-03T18:52:21.698604Z","shell.execute_reply":"2024-05-03T18:52:21.697404Z","shell.execute_reply.started":"2024-05-03T18:52:21.668819Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:22.159823Z","iopub.status.busy":"2024-05-03T18:52:22.159159Z","iopub.status.idle":"2024-05-03T18:52:22.163867Z","shell.execute_reply":"2024-05-03T18:52:22.162765Z","shell.execute_reply.started":"2024-05-03T18:52:22.159790Z"},"trusted":true},"outputs":[],"source":["# for step, data in enumerate(train_loader):\n","#     data =  collate(data)\n","#     x = data[\"input_ids\"].to(device)\n","#     y = data['labels'].to(device)\n","#     break"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:22.804071Z","iopub.status.busy":"2024-05-03T18:52:22.803710Z","iopub.status.idle":"2024-05-03T18:52:22.808552Z","shell.execute_reply":"2024-05-03T18:52:22.807462Z","shell.execute_reply.started":"2024-05-03T18:52:22.804040Z"},"trusted":true},"outputs":[],"source":["# model()"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:23.207224Z","iopub.status.busy":"2024-05-03T18:52:23.206821Z","iopub.status.idle":"2024-05-03T18:52:23.211951Z","shell.execute_reply":"2024-05-03T18:52:23.210831Z","shell.execute_reply.started":"2024-05-03T18:52:23.207184Z"},"trusted":true},"outputs":[],"source":["# out = model(input_ids = x)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:23.562740Z","iopub.status.busy":"2024-05-03T18:52:23.562337Z","iopub.status.idle":"2024-05-03T18:52:23.567200Z","shell.execute_reply":"2024-05-03T18:52:23.566125Z","shell.execute_reply.started":"2024-05-03T18:52:23.562708Z"},"trusted":true},"outputs":[],"source":["# out.logits.size()"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:24.270101Z","iopub.status.busy":"2024-05-03T18:52:24.269740Z","iopub.status.idle":"2024-05-03T18:52:24.281354Z","shell.execute_reply":"2024-05-03T18:52:24.280315Z","shell.execute_reply.started":"2024-05-03T18:52:24.270070Z"},"trusted":true},"outputs":[],"source":["def train(train_loader=train_loader, model=model):\n","    best_epoch_loss = np.inf\n","    model.to(device)\n","    accelerator = accelerator = Accelerator(log_with=\"tensorboard\", logging_dir=\".\")\n","    Config = {\n","        \"num_epoch\": EPOCHS,\n","        \"learning_rate\": lr,\n","        \"loss_function\": str(torch.nn.CrossEntropyLoss),\n","    }\n","    model.train()\n","    accelerator.init_trackers(f\"CLM_project\", config=Config)\n","    for epoch in range(EPOCHS):\n","        start_time = time.time()\n","        avg_loss = 0.0\n","        model.train()\n","        tbar = tqdm(train_loader, file=sys.stdout)\n","        loss_list = []\n","        tbar.set_description(f\"Epoch {epoch+1}\")\n","        for step, data in enumerate(tbar):\n","            data = collate(data)\n","            x = data[\"input_ids\"].to(device)\n","            y = data[\"labels\"].to(device)\n","            optimizer.zero_grad()\n","            pred = model(input_ids=x).logits\n","            loss = loss_fn(y, pred)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            accelerator.log({\"train_step\": loss.item()}, step=step)\n","            tbar.set_postfix(loss=loss.item())\n","            loss_list.append(loss.detach().cpu().item())\n","\n","        avg_loss = np.round(np.mean(loss_list), 4)\n","        accelerator.log({\"train_step_epoch\": avg_loss}, step=epoch + 1)\n","\n","        print(f\"Epoch--{epoch+1} ### Train loss---{avg_loss}\")\n","\n","    PATH = f\"decoder__{epoch}.pth\"\n","    torch.save(model.state_dict(), PATH)\n","    accelerator.end_training()\n","    del train_loader\n","    gc.collect()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T18:52:25.763932Z","iopub.status.busy":"2024-05-03T18:52:25.763090Z","iopub.status.idle":"2024-05-03T19:12:18.938290Z","shell.execute_reply":"2024-05-03T19:12:18.937455Z","shell.execute_reply.started":"2024-05-03T18:52:25.763894Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e384f293dd0c4a758fb325f015ddbcda","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/515 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch--1 ### Train loss---5.8968\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23fdb6cdd8da4ce1947ec7f30d281b9a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/515 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch--2 ### Train loss---4.5792\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"839f75c463ea4a159cc1c8e9a176d453","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/515 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch--3 ### Train loss---4.1407\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c0311a6c68249f693f82ab4902e1841","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/515 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch--4 ### Train loss---3.8287\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7093162ac8304743a623340c2c1d9923","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/515 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch--5 ### Train loss---3.6911\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b86763dc43f848a3ae531bd5bbf41fb1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/515 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch--6 ### Train loss---3.6913\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3875f7864f734508b3ae2b4f4b18a870","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/515 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch--7 ### Train loss---3.6913\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b69e305fb2d4caea8a68208838c1572","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/515 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch--8 ### Train loss---3.6911\n"]}],"source":["train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-03T18:52:11.421556Z","iopub.status.idle":"2024-05-03T18:52:11.422038Z","shell.execute_reply":"2024-05-03T18:52:11.421804Z","shell.execute_reply.started":"2024-05-03T18:52:11.421782Z"},"trusted":true},"outputs":[],"source":["# model = DecoderModel.from_config(config)\n","# model.load_state_dict(torch.load('/kaggle/input/update-gpt2-scratch-kv-cache-sdpa/gpt2_epoch__9.pth'))"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:42.040590Z","iopub.status.busy":"2024-05-03T19:12:42.040204Z","iopub.status.idle":"2024-05-03T19:12:42.048429Z","shell.execute_reply":"2024-05-03T19:12:42.047512Z","shell.execute_reply.started":"2024-05-03T19:12:42.040555Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DecoderModel(\n","  (word_embeddings): Embedding(50257, 768)\n","  (all_layer): ModuleList(\n","    (0-5): 6 x DecoderLayer(\n","      (attention): DecoderAttention(\n","        (q): Linear(in_features=768, out_features=768, bias=True)\n","        (k): Linear(in_features=768, out_features=768, bias=True)\n","        (v): Linear(in_features=768, out_features=768, bias=True)\n","        (out): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (feed_forward): FeedForward(\n","        (intermediate): Linear(in_features=768, out_features=3072, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (layerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (act_fn): GELU(approximate='none')\n","        (out): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","    )\n","  )\n","  (lm_head): LMHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (layerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (decoder): Linear(in_features=768, out_features=50257, bias=True)\n","  )\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["model.eval()"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:42.431883Z","iopub.status.busy":"2024-05-03T19:12:42.431444Z","iopub.status.idle":"2024-05-03T19:12:42.442536Z","shell.execute_reply":"2024-05-03T19:12:42.441540Z","shell.execute_reply.started":"2024-05-03T19:12:42.431849Z"},"trusted":true},"outputs":[],"source":["def generate(\n","    model: nn.Module,\n","    tokenize_text: torch.Tensor,\n","    max_new_tokens: Optional[int] = 128,\n","    temperature: Optional[float] = 1.0,\n","    do_sample: Optional[bool] = False,\n","    use_cache: Optional[bool] = False,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Take a conditioning sequence of indices idx (LongTensor of shape (1,t)) and complete\n","    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n","    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n","    \"\"\"\n","    #     text = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    idx = tokenize_text\n","    idx_next = idx\n","    index = 0\n","    take = -1\n","    #     for cur_pos in range(min_promp, total_len)\n","    for _ in range(max_new_tokens):\n","        if use_cache == False:\n","            with torch.no_grad():\n","                logits = model(input_ids=idx).logits\n","        else:\n","            with torch.no_grad():\n","                logits = model(\n","                    input_ids=idx_next, start_pos=index, use_cache=use_cache\n","                ).logits\n","\n","        if take != 0:\n","            logits = logits[:, take, :] / temperature\n","            if use_cache == True:\n","                take = 0\n","        else:\n","            logits = logits[:, -1] / temperature\n","        probs = torch.nn.functional.softmax(logits, dim=-1)\n","        # either sample from the distribution or take the most likely element\n","        if do_sample:\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","        else:\n","            _, idx_next = torch.topk(probs, k=1, dim=-1)\n","\n","        idx = torch.cat((idx, idx_next), dim=1)\n","        index = idx.size()[1] - 1  # model already have idx-1 kv-cache stored\n","\n","    return idx"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:42.839316Z","iopub.status.busy":"2024-05-03T19:12:42.838969Z","iopub.status.idle":"2024-05-03T19:12:42.843343Z","shell.execute_reply":"2024-05-03T19:12:42.842365Z","shell.execute_reply.started":"2024-05-03T19:12:42.839287Z"},"trusted":true},"outputs":[],"source":["# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{},"source":["**This just for demonstrated purpose you need more data for fluent model**"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:43.244581Z","iopub.status.busy":"2024-05-03T19:12:43.244214Z","iopub.status.idle":"2024-05-03T19:12:44.029448Z","shell.execute_reply":"2024-05-03T19:12:44.028520Z","shell.execute_reply.started":"2024-05-03T19:12:43.244551Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'this of the test.  The result was not a good deal. He was a good deal of a good deal of  the matter, and he was a good deal of it. He was a good deal of  course, and he was a good deal of a thing which he had been to  be a good deal of time. He had been a good deal of time, and he had  been a good deal of a good deal of time. He had been a good deal of  talk, and he had a good deal of time to get a chance to get a  chance. He had been a good deal of time, and he had'"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["text = tokenizer.encode(\n","    \"this of the test\", add_special_tokens=False, return_tensors=\"pt\"\n",").to(device)\n","out = generate(model=model, tokenize_text=text)\n","tokenizer.decode(out[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:44.031694Z","iopub.status.busy":"2024-05-03T19:12:44.031292Z","iopub.status.idle":"2024-05-03T19:12:44.798261Z","shell.execute_reply":"2024-05-03T19:12:44.797348Z","shell.execute_reply.started":"2024-05-03T19:12:44.031658Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'long before that I was  in the midst of a great many of the time, and I was so glad I had  to get a chance to get it out of my mind.I had to go to the  place, and I was going to be a good deal of my own.I had to  get a chance to get my mind to be a good deal of a time.  I was a good deal of a good deal, and I was so sorry I had been  in the way of the country.I had been a good deal of a good  time, but I had no time to get my mind to be so much more than'"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["text = tokenizer.encode(\n","    \"long before that I was\", add_special_tokens=False, return_tensors=\"pt\"\n",").to(device)\n","out = generate(model=model, tokenize_text=text)\n","tokenizer.decode(out[0])"]},{"cell_type":"markdown","metadata":{},"source":["**what is KV-Cache**\n","\n","https://www.dipkumar.dev/posts/gpt-kvcache/"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:44.800031Z","iopub.status.busy":"2024-05-03T19:12:44.799750Z","iopub.status.idle":"2024-05-03T19:12:45.526104Z","shell.execute_reply":"2024-05-03T19:12:45.525213Z","shell.execute_reply.started":"2024-05-03T19:12:44.800008Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'this of the test.  The result was not a good deal. He was a good deal of a good deal of  the matter, and he was a good deal of it. He was a good deal of  course, and he was a good deal of a thing which he had been to  be a good deal of time. He had been a good deal of time, and he had  been a good deal of a good deal of time. He had been a good deal of  talk, and he had a good deal of time to get a chance to get a  chance. He had been a good deal of time, and he had'"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["model._clean_cache()\n","model._setup_cache(config)\n","text = tokenizer.encode(\n","    \"this of the test\", add_special_tokens=False, return_tensors=\"pt\"\n",").to(device)\n","out = generate(model=model, tokenize_text=text, use_cache=True)\n","tokenizer.decode(out[0])"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:45.528214Z","iopub.status.busy":"2024-05-03T19:12:45.527933Z","iopub.status.idle":"2024-05-03T19:12:46.241289Z","shell.execute_reply":"2024-05-03T19:12:46.240372Z","shell.execute_reply.started":"2024-05-03T19:12:45.528190Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'long before that I was  in the midst of a great many of the time, and I was so glad I had  to get a chance to get it out of my mind.I had to go to the  place, and I was going to be a good deal of my own.I had to  get a chance to get my mind to be a good deal of a time.  I was a good deal of a good deal, and I was so sorry I had been  in the way of the country.I had been a good deal of a good  time, but I had no time to get my mind to be so much more than'"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["model._clean_cache()\n","model._setup_cache(config)\n","text = text = tokenizer.encode(\n","    \"long before that I was\", add_special_tokens=False, return_tensors=\"pt\"\n",").to(device)\n","out = generate(model=model, tokenize_text=text, use_cache=True)\n","tokenizer.decode(out[0])"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:46.243005Z","iopub.status.busy":"2024-05-03T19:12:46.242714Z","iopub.status.idle":"2024-05-03T19:12:46.927866Z","shell.execute_reply":"2024-05-03T19:12:46.926892Z","shell.execute_reply.started":"2024-05-03T19:12:46.242979Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'this of the test.  The result was not a good deal. He was a good deal of a good deal of  the matter, and he was a good deal of it. He was a good deal of  course, and he was a good deal of a thing which he had been to  be a good deal of time. He had been a good deal of time, and he had  been a good deal of a good deal of time. He had been a good deal of  talk, and he had a good deal of time to get a chance to get a  chance. He had been a good deal of time, and he had'"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["model._clean_cache()\n","model._setup_cache(config, cls=DynamicCache)\n","text = tokenizer.encode(\n","    \"this of the test\", add_special_tokens=False, return_tensors=\"pt\"\n",").to(device)\n","out = generate(model=model, tokenize_text=text, use_cache=True)\n","tokenizer.decode(out[0])"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:46.929601Z","iopub.status.busy":"2024-05-03T19:12:46.929290Z","iopub.status.idle":"2024-05-03T19:12:47.617940Z","shell.execute_reply":"2024-05-03T19:12:47.617054Z","shell.execute_reply.started":"2024-05-03T19:12:46.929575Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'long before that I was  in the midst of a great many of the time, and I was so glad I had  to get a chance to get it out of my mind.I had to go to the  place, and I was going to be a good deal of my own.I had to  get a chance to get my mind to be a good deal of a time.  I was a good deal of a good deal, and I was so sorry I had been  in the way of the country.I had been a good deal of a good  time, but I had no time to get my mind to be so much more than'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["model._clean_cache()\n","model._setup_cache(config, cls=DynamicCache)\n","text = text = tokenizer.encode(\n","    \"long before that I was\", add_special_tokens=False, return_tensors=\"pt\"\n",").to(device)\n","out = generate(model=model, tokenize_text=text, use_cache=True)\n","tokenizer.decode(out[0])"]},{"cell_type":"markdown","metadata":{},"source":["**Enable Sampling**"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:49.041847Z","iopub.status.busy":"2024-05-03T19:12:49.041467Z","iopub.status.idle":"2024-05-03T19:12:49.791386Z","shell.execute_reply":"2024-05-03T19:12:49.790428Z","shell.execute_reply.started":"2024-05-03T19:12:49.041819Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'this of the test.  We can only tell us more about as a mile and an open calf and pass  through the lower left of the woods, and as he is in the most  frenzied blood.I honestly said anything.We were glad it was  emptied.We could not use how much night when those were coming  and lonesome anywhere in the house but we were dead, not the  stricken haggard.I thanked him to a dog, and was done at all.  Mars Tom, I thought, open to that box, and I wish we lent out a sigh  from the doors Secret dressed the old pilot'"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["model._clean_cache()\n","model._setup_cache(config)\n","text = text = tokenizer.encode(\n","    \"this of the test\", add_special_tokens=False, return_tensors=\"pt\"\n",").to(device)\n","out = generate(model=model, tokenize_text=text, use_cache=True, do_sample=True)\n","tokenizer.decode(out[0])"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:50.327270Z","iopub.status.busy":"2024-05-03T19:12:50.326609Z","iopub.status.idle":"2024-05-03T19:12:51.095985Z","shell.execute_reply":"2024-05-03T19:12:51.095115Z","shell.execute_reply.started":"2024-05-03T19:12:50.327240Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"long before that I was watching him. When he said I was  tall--naulded:  No, I was not out of it. Let us kill us. I had to the rest of  the other.  He was up toiled and set at Quixby way into Peterse and returned. He aclysmed to come after the humanity, but he spoken here in the  people who smelled like their dust in it, and looked at him the others  entirely, as pleased, as he remembers the messenger of his oldhematically  grievousness. Won't you'll ask you for him? Are in the life of a  young\""]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["model._clean_cache()\n","model._setup_cache(config)\n","text = text = tokenizer.encode(\n","    \"long before that I was\", add_special_tokens=False, return_tensors=\"pt\"\n",").to(device)\n","out = generate(model=model, tokenize_text=text, use_cache=True, do_sample=True)\n","tokenizer.decode(out[0])"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T19:12:51.244925Z","iopub.status.busy":"2024-05-03T19:12:51.244280Z","iopub.status.idle":"2024-05-03T19:12:51.978999Z","shell.execute_reply":"2024-05-03T19:12:51.978077Z","shell.execute_reply.started":"2024-05-03T19:12:51.244892Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"Well, sir, you could seem to  wait a-lock three hours for four Jake. And it lacksenment that he  hurts Beckys concerned only of foreign accord. trulers loved him,  and killed him for distinguished countless others in the river along,  and he shook his head and stood rigid, anyway.And yet he and  pretty soon he came up and didnt get it. Satan found it, and we got  out for lack of music.  Don't you know it before him.  Why, thats anything? Get out and I dont know it.Don favor, yes.  Well, suggested Huck, its one ben for the summer\""]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["model._clean_cache()\n","model._setup_cache(config)\n","text = text = tokenizer.encode(\n","    \"Well, sir, you could\", add_special_tokens=False, return_tensors=\"pt\"\n",").to(device)\n","out = generate(model=model, tokenize_text=text, use_cache=True, do_sample=True)\n","tokenizer.decode(out[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3277362,"sourceId":5699740,"sourceType":"datasetVersion"},{"sourceId":129796172,"sourceType":"kernelVersion"}],"dockerImageVersionId":30476,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
