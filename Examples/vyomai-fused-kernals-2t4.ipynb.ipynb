{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9605963,"sourceType":"datasetVersion","datasetId":5860854},{"sourceId":117289270,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile fused_encoder_train.py\n\nimport gc\nimport os\nimport sys\nimport time\nimport random\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport pandas as pd\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom accelerate import Accelerator\nfrom transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom transformers import AutoConfig\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nimport warnings\nwarnings.simplefilter('ignore')\nfrom torch.amp import custom_fwd, custom_bwd\nfrom accelerate import Accelerator, DistributedDataParallelKwargs\ntorch._dynamo.config.capture_dynamic_output_shape_ops = True\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nfrom dataclasses import dataclass\n\n\n#to do\n#transform this into 2d matrix multiplication  like the rest of the layers\n@torch.compile(fullgraph=True)\ndef linear_fwd(input_features, weight, bias):\n    output = torch.einsum(\"...ik, ...jk -> ...ij\", input_features, weight)  # x*w^T\n       \n    if bias is not None:\n        output += bias.unsqueeze(0).expand_as(output)\n    return output\n\n@torch.compile(fullgraph=True)\ndef linear_bwd(grad_output,input_features, weight, bias):\n    grad_input = grad_weight = grad_bias = None\n\n    grad_input = torch.einsum(\"...ij, ...jk -> ...ik\", grad_output, weight)\n    \n    grad_weight = torch.einsum(\n        \"...ji, ...jk -> ...ik\", grad_output, input_features\n    )\n    \n    grad_bias = grad_output.sum(0)\n\n    return grad_input, grad_weight, grad_bias\n    \nclass LinearFunction(torch.autograd.Function):\n\n    @staticmethod\n    @custom_fwd( device_type='cuda')\n    def forward(ctx, input_features, weight, bias=None):\n        ctx.save_for_backward(input_features, weight, bias)\n        return linear_fwd(input_features, weight, bias)\n        \n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_output):\n     \n        input_features, weight, bias = ctx.saved_tensors\n        return linear_bwd(grad_output,input_features, weight, bias)\n\nclass MyLinear(nn.Module):\n    def __init__(self, input_features, output_features, bias=True):\n        super(MyLinear, self).__init__()\n        self.input_features = input_features\n        self.output_features = output_features\n\n        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n        torch.nn.init.xavier_uniform_(self.weight)\n        \n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(output_features))\n            torch.nn.init.zeros_(self.bias)\n        else:\n            # You should always register all possible parameters, but the\n            # optional ones can be None if you want.\n            self.register_parameter(\"bias\", None)\n\n      \n        self.weight = nn.init.xavier_normal_(self.weight)\n        if self.bias is not None:\n            self.bias =   nn.init.zeros_(self.bias)\n    def forward(self, x):\n        # See the autograd section for explanation of what happens here.\n        return LinearFunction.apply(x, self.weight, self.bias)\n\n    def extra_repr(self):\n        # Set the extra information about this module. You can test\n        # it by printing an object of this class.\n        return \"input_features={}, output_features={}, bias={}\".format(\n            self.input_features, self.output_features, self.bias is not None\n        )\n\nclass LayerNormFn(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, x, weight, bias, eps):\n        var, mean = torch.var_mean(x, dim=-1, keepdim=True)\n        \n        # mean = torch.mean(x, dim=-1, keepdim=True)\n        # var = torch.var(x, dim=-1, keepdim=True, unbiased=False)\n        \n        x_normalized = (x - mean) / torch.sqrt(var + eps)\n        y = x_normalized * weight + bias\n        ctx.save_for_backward(x, x_normalized, weight, mean, var)\n        ctx.eps = eps\n        return y\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_output):\n        x, x_normalized, weight, mean, var = ctx.saved_tensors\n        eps = ctx.eps\n\n        grad_weight = torch.sum(grad_output * x_normalized, dim=(0))\n        grad_bias = torch.sum(grad_output, dim=(0))\n\n        grad_x_normalized = grad_output * weight\n        grad_var = torch.sum(\n            grad_x_normalized * (x - mean) * (-0.5) * torch.pow(var + eps, -1.5),\n            dim=(-1),\n            keepdim=True,\n        )\n        grad_mean = torch.sum(\n            grad_x_normalized * (-1 / torch.sqrt(var + eps)), dim=(-1), keepdim=True\n        ) + grad_var * torch.mean(-2 * (x - mean), dim=(-1), keepdim=True)\n        grad_x = (\n            grad_x_normalized / torch.sqrt(var + eps)\n            + grad_var * 2 * (x - mean) / x.shape[-1]\n            + grad_mean / x.shape[-1]\n        )\n\n        return grad_x, grad_weight, grad_bias, None\n\n\nclass LayerNorm(torch.nn.Module):\n    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n        super().__init__()\n        if isinstance(normalized_shape, int):\n            normalized_shape = (normalized_shape,)\n        self.normalized_shape = tuple(normalized_shape)\n        self.eps = eps\n        self.elementwise_affine = elementwise_affine\n        if self.elementwise_affine:\n            self.weight = torch.nn.Parameter(torch.ones(normalized_shape))\n            self.bias = torch.nn.Parameter(torch.zeros(normalized_shape))\n            nn.init.ones_(self.weight)\n            nn.init.zeros_(self.bias)\n        else:\n            self.register_parameter(\"weight\", None)\n            self.register_parameter(\"bias\", None)\n\n    def forward(self, x):\n        if self.elementwise_affine:\n            return LayerNormFn.apply(x, self.weight, self.bias, self.eps)\n        else:\n            return LayerNormFn.apply(\n                x,\n                torch.ones_like(x.shape[-1:]),\n                torch.zeros_like(x.shape[-1:]),\n                self.eps,\n            )\n\n\n\n@torch.compile(fullgraph=True)\ndef rms_forward(x, weight, eps):\n    # Calculate the RMS\n    rms = x.float().pow(2).mean(-1, keepdim=True).add(eps).sqrt()\n    # Normalize\n    x_norm = x / rms\n    # Apply the gain (weight)\n    output = weight * x_norm.type_as(weight)\n    return output\n\n@torch.compile(fullgraph=True)\ndef rms_backward(grad_output, x, weight,eps):\n    \n    rms = x.float().pow(2).mean(-1, keepdim=True).add(eps).sqrt()\n    # Normalize\n    x_norm = x / rms\n    # Gradients calculations\n    grad_weight = torch.sum(grad_output * x_norm, dim=(0))\n\n    # Compute grad_x: we need to backpropagate through normalization and RMS\n    grad_x_norm = grad_output * weight  # Gradients w.r.t normalized input\n    grad_rms = (grad_x_norm * (-x_norm)).sum(\n        -1, keepdim=True\n    ) / rms  # Gradients w.r.t RMS\n\n    # Gradient w.r.t input x: the gradient of x involves the gradient of the normalization (grad_x_norm)\n    grad_x = grad_x_norm / rms + grad_rms * x_norm / x_norm.size(\n        -1\n    )  # RMS backpropagation\n\n    return grad_x, grad_weight\n    \nclass RMSNormFn(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, x, weight, eps):\n        output = rms_forward(x, weight, eps)\n        ctx.save_for_backward(x, weight)\n        ctx.eps = eps\n        return output\n        \n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_output):\n        x, weight = ctx.saved_tensors\n        eps = ctx.eps\n        grad_x, grad_weight = rms_backward(grad_output,x, weight,eps)\n\n        return grad_x, grad_weight, None\n\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim, eps=1e-6):\n        super().__init__()\n        self.weight = torch.nn.Parameter(torch.ones(dim))\n        self.eps = eps\n\n    def forward(self, x):\n        return RMSNormFn.apply(x, self.weight, self.eps)\n\n\n@torch.compile(fullgraph=True)\ndef linear_fwd_2d(x,weight, bias):\n    output = x.mm(weight.t())\n    if bias is not None:\n        output += bias.unsqueeze(0).expand_as(output)\n    return output\n\n\n@torch.compile(fullgraph=True)\ndef linear_bwd_2d(grad_rms,weight,x,bias):\n    grad_input = grad_rms.mm(weight)\n    grad_prev = grad_rms\n\n    grad_weight = grad_rms.t().mm(x)\n    grad_bias = None\n    if bias is not None:\n        grad_bias = grad_rms.sum(0)\n        \n    return grad_input, grad_prev, grad_weight, grad_bias\n    \nclass LinearRms(torch.autograd.Function):\n    @staticmethod\n    # @custom_fwd\n    def forward(ctx, x, prev, weight, bias=None, weight_rms=None, eps=None):\n        output = linear_fwd_2d(x,weight, bias)\n        output = rms_forward(output + prev, weight_rms, eps)\n        ctx.save_for_backward(\n            x, weight, bias, output + prev, weight_rms\n        )\n        ctx.eps = eps\n        return output\n\n    @staticmethod\n    # @custom_bwd\n    def backward(ctx, grad_output):\n        x, weight, bias, rms_x, weight_rms = ctx.saved_tensors\n        eps = ctx.eps\n        grad_weight = grad_bias = None\n\n        grad_rms, grad_weight_rms = rms_backward(\n            grad_output, rms_x, weight_rms,eps\n        )  \n\n        grad_input, grad_prev, grad_weight, grad_bias = linear_bwd_2d(grad_rms,weight,x,bias)\n        return grad_input, grad_prev, grad_weight, grad_bias, grad_weight_rms, None\n\nclass GELUFunction(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        cdf = 0.5 * (1.0 + torch.erf(x / 2**0.5))\n        ctx.save_for_backward(x)\n        return x * cdf #0.5 * (1.0 + torch.erf(x / 2**0.5))\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors\n        cdf = 0.5 * (1.0 + torch.erf(x / 2**0.5))\n        pdf = torch.exp(-0.5 * x**2) / (2 * 3.14159265359) ** 0.5\n        return grad_output * (cdf + x * pdf)\n\n\nclass GELU(torch.nn.Module):\n    def forward(self, x):\n        return GELUFunction.apply(x)\n\n\nimport torch\nfrom torch.autograd import Function\n\n@torch.compile(fullgraph=True)\ndef gelu_back_ward(x):\n    cdf = 0.5 * (1.0 + torch.erf(x / 2**0.5))\n    pdf = torch.exp(-0.5 * x**2) / (2 * 3.14159265359) ** 0.5\n    return (cdf + x * pdf)\n    \nclass LinearGeLU(Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, input,prev, weight, bias=None):\n        ctx.save_for_backward(input, weight, bias)\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        ctx.save_for_backward(input, weight, bias,output+prev)\n        return F.gelu(output+prev)  # output.clamp(min=0)  # ReLU activation\n        \n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_output):\n        input, weight, bias,output_prev = ctx.saved_tensors\n        grad_weight = grad_bias = None\n\n        grad_gelu = grad_output*gelu_back_ward(output_prev)\n\n        grad_input = grad_gelu.mm(weight)\n        grad_prev = grad_gelu\n\n        grad_weight = grad_gelu.t().mm(input)\n        grad_bias = None\n        if bias is not None:\n            grad_bias = grad_gelu.sum(0)\n\n        return grad_input,grad_prev , grad_weight, grad_bias\n\n\nclass LinearGeLUFused(torch.nn.Module):\n    def __init__(self, in_features, out_features):\n        super(LinearGeLUFused, self).__init__()\n        self.weight = torch.nn.Parameter(\n            torch.Tensor(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n        torch.nn.init.xavier_uniform_(self.weight)\n        torch.nn.init.zeros_(self.bias)\n\n    def forward(self, input,prev):\n        return LinearGeLU.apply(input,prev, self.weight, self.bias)\n\n\nclass LinearRMSFused(torch.nn.Module):\n\n    def __init__(self, in_features, out_features, eps=1e-6):\n        super(LinearRMSFused, self).__init__()\n        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n        torch.nn.init.xavier_uniform_(self.weight)\n        torch.nn.init.zeros_(self.bias)\n        self.weight_rms = torch.nn.Parameter(torch.ones(out_features))\n        self.eps = eps\n\n    def forward(self, input, prev):\n        return LinearRms.apply(\n            input, prev, self.weight, self.bias, self.weight_rms, self.eps\n        )\n\nimport torch\nfrom torch.autograd import Function\n\n\n@torch.compile(fullgraph=True)\ndef gelu_bwd(x):\n    cdf = 0.5 * (1.0 + torch.erf(x / 2**0.5))\n    pdf = torch.exp(-0.5 * x**2) / (2 * 3.14159265359) ** 0.5\n    return (cdf + x * pdf)\n\n@torch.compile(fullgraph=True)\ndef gelu_fwd(x):\n    cdf = 0.5 * (1.0 + torch.erf(x / 2**0.5))\n    return x * cdf\n    \n@torch.compile(fullgraph=True)\ndef ffn_gelu_fwd(x,weight1, bias1, weight2, bias2=None):\n\n    output1 = x.mm(weight1.t())\n\n    if bias1 is not None:\n        output1 += bias1.unsqueeze(0).expand_as(output1)\n\n    gelu_output = gelu_fwd(output1)  # output.clamp(min=0)  # ReLU activation\n    output2 = gelu_output.mm(weight2.t())\n\n    if bias2 is not None:\n        output2 += bias2.unsqueeze(0).expand_as(output2)\n\n    return output2,output1\n    \n@torch.compile(fullgraph=True)\ndef ffn_gelu_bwd(grad_output,x,weight1, bias1, weight2, bias2,gelu_input):\n    grad_bias1 = grad_bias2 = None\n\n    grad_gelu = gelu_bwd(gelu_input)\n\n    grad_x2 = grad_output.mm(weight2) * grad_gelu\n    grad_weight2 = grad_output.T.mm(gelu_input)\n\n    if bias2 is not None:\n        grad_bias2 = grad_output.sum(0)\n\n    grad_x1 = grad_x2.mm(weight1)\n    grad_weight1 = grad_x2.T.mm(x)\n\n    if bias1 is not None:\n        grad_bias1 = grad_x2.sum(0)\n\n    return grad_x1, grad_weight1, grad_bias1, grad_weight2, grad_bias2\n    \nclass FFNGeLU(Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, x, weight1, bias1, weight2, bias2=None):\n        output2,output1 = ffn_gelu_fwd(x,weight1, bias1, weight2, bias2)\n        ctx.save_for_backward(x, weight1, bias1, weight2, bias2, output1)\n        return output2\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_output):\n        x, weight1, bias1, weight2, bias2, gelu_input = ctx.saved_tensors\n        grad_x1, grad_weight1, grad_bias1, grad_weight2, grad_bias2 = ffn_gelu_bwd(grad_output,x,weight1, bias1, weight2, bias2,gelu_input)\n\n        return grad_x1, grad_weight1, grad_bias1, grad_weight2, grad_bias2\n\n\nclass FFNGeluModule(torch.nn.Module):\n    def __init__(self, in_features, mid_feature, out_features):\n        super(FFNGeluModule, self).__init__()\n        self.weight1 = torch.nn.Parameter(\n            torch.Tensor(mid_feature, in_features)\n        )\n        self.bias1 = torch.nn.Parameter(torch.Tensor(mid_feature))\n\n        self.weight2 = torch.nn.Parameter(\n            torch.Tensor(out_features, mid_feature)\n        )\n        self.bias2 = torch.nn.Parameter(torch.Tensor(out_features))\n\n        torch.nn.init.xavier_uniform_(self.weight1)\n        torch.nn.init.zeros_(self.bias1)\n        torch.nn.init.xavier_uniform_(self.weight2)\n        torch.nn.init.zeros_(self.bias2)\n\n    def forward(self, input):\n        return FFNGeLU.apply(input, self.weight1, self.bias1, self.weight2, self.bias2)\n\n\nimport torch\nfrom torch.autograd import Function, gradcheck\n\n\nclass MyEmbeddingFunction(Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, weight, indices,pad_index):\n        result = weight[indices]\n        ctx.save_for_backward(indices, weight)\n        ctx.pad_index = pad_index\n        return result\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_output):\n        indices, weight = ctx.saved_tensors\n        pad_index = ctx.pad_index \n        grad_weight = torch.zeros_like(weight)\n        #index_add_(dim, index, source)\n        \n        index = indices.flatten()\n        ignore_mask = index!=pad_index\n        index = index[ignore_mask]\n        grad_output = grad_output.view(-1, grad_output.size(-1))\n        grad_output = grad_output[ignore_mask]\n        \n        grad_weight.index_add_(0, index,grad_output)\n        grad_indices = None\n        return grad_weight, grad_indices, None\n\nclass MyEmbedding(torch.nn.Module):\n    def __init__(self, num_embeddings, embedding_dim,pad_index):\n        super(MyEmbedding, self).__init__()\n        self.weight = torch.nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n        self.pad_index = pad_index\n\n    def forward(self, indices):\n        \n        return MyEmbeddingFunction.apply(self.weight, indices,self.pad_index)\n\n@torch.compile(fullgraph=True)\ndef attention_forward(Q, K, V, mask=None):\n    d_k = Q.shape[-1]\n    scale = 1.0 / torch.sqrt(torch.tensor(d_k, dtype=Q.dtype, device=Q.device))  # avoid NumPy\n    EPSILON = 1e-10\n\n    Q_scaled = Q * scale\n    S = torch.einsum(\"... i d, ... j d -> ... i j\", Q_scaled, K)\n\n    if mask is not None:\n        S = S + mask\n\n    softmax = F.softmax(S, dim=-1)\n    P_V = torch.einsum(\"... i j, ... j d -> ... i d\", softmax, V)\n\n    return P_V, softmax\n    \n@torch.compile(fullgraph=True)\ndef attention_backward(Q, K, V, O, dO, softmax):\n    scale = 1.0 / torch.sqrt(torch.tensor(Q.shape[-1], dtype=Q.dtype, device=Q.device))  # avoid NumPy\n\n\n    P = softmax  # (1 / l) * torch.exp(S - m)\n   \n\n    dV = torch.einsum(\"... r c, ... r d -> ... c d\", P, dO)\n    dP = torch.einsum(\"... r d, ... c d -> ... r c\", dO, V)\n\n    D = torch.sum(dO * O, dim=-1, keepdim=True)\n    dS = P * (dP - D)\n\n    dQ = scale * torch.einsum(\"... r c, ... c d -> ... r d\", dS, K)\n    dK = scale * torch.einsum(\"... r c, ... r d -> ... c d\", dS, Q)\n    return dQ, dK, dV\n\n\nclass ScaledDotProductAttention(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, q, k, v, mask=None):\n        out, attn_weights = attention_forward(q,k,v, mask)\n        ctx.save_for_backward(q, k, v, mask,out, attn_weights)\n        return out\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, do):\n        Q, K, V, mask, O, softmax = ctx.saved_tensors\n        dq, dk, dv = attention_backward(Q, K, V,O, do, softmax)\n        return dq, dk, dv, None\n\n\n@torch.compile(fullgraph=True)\ndef rotate_half(x):\n    x1, x2 = x.chunk(2, dim=-1)\n    return torch.cat((-x2, x1), dim=-1)\n\n@torch.compile(fullgraph=True)\ndef rope_fwd(q, k, freqs):\n        # q, k: (batch_size, num_heads, seq_len, head_dim)\n        # cos, sin: (seq_len, head_dim) or (1, seq_len, head_dim) or (batch_size, num_heads, seq_len, head_dim)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        cos = emb.cos()\n        sin = emb.sin()\n        cos = cos.unsqueeze(1)\n        sin = sin.unsqueeze(1)\n        # forward Method\n        #qleft,qright = q1,q2\n        #qleft out = q1*cos-q2*sin\n        #qright out = q2*cos+q1*sin\n        #final out1 = concat(qleft out, qright out)\n        #final out2 = concat(q1,q2)*cos+concat(-q2,q1)*sin\n        #final out1 and final out2 both are same\n        q_rotated = (q * cos) + (rotate_half(q) * sin)\n        k_rotated = (k * cos) + (rotate_half(k) * sin)\n\n        return q_rotated, k_rotated, cos, sin\n\n@torch.compile(fullgraph=True)\ndef rope_bwd(grad_q_rotated, grad_k_rotated, cos, sin):\n        # backward Method\n        #qleft,qright = q1,q2\n        # y=(x1*cos+ x2*cos)+(−x2*sin + x1*sin)\n        # y = x1*(cos+sin)+x2(cos-sin)\n        # dy/dx1 = cos+sin\n        # dy/dx1 = cos-sin\n        #do/dq qleft out = out_grad*(cos+sin)\n        #do/dq right out = out_grad*(cos-sin)\n        #do/dq final out1 = concat(do/dq qleft, do/dq right out)\n        #do/dq final out2 = concat(q1,q2)*cos-concat(-q2,q1)*sin\n        #final out1 and final out2 both are same\n\n        grad_q = (grad_q_rotated * cos) - (\n            rotate_half(grad_q_rotated) * sin\n        )\n        grad_k = (grad_k_rotated * cos) - (\n           rotate_half(grad_k_rotated) * sin\n        )\n\n        #if freq is a parameter  we will need to calculate its grad as well\n\n        # grad_cos_q = torch.sum(grad_q_rotated * q, dim=(0, 1, 2), keepdim=True)\n        # grad_sin_q = torch.sum(\n        #     grad_q_rotated * RotaryEmbeddingFunction.rotate_half(q),\n        #     dim=(0, 1, 2),\n        #     keepdim=True,\n        # )\n\n        # grad_cos_k = torch.sum(grad_k_rotated * k, dim=(0, 1, 2), keepdim=True)\n        # grad_sin_k = torch.sum(\n        #     grad_k_rotated * RotaryEmbeddingFunction.rotate_half(k),\n        #     dim=(0, 1, 2),\n        #     keepdim=True,\n        # )\n\n        # grad_cos = grad_cos_q + grad_cos_k\n        # grad_sin = grad_sin_q + grad_sin_k\n\n        return grad_q, grad_k\n    \nclass RotaryEmbeddingFunction(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda',cast_inputs=torch.float32)\n    def forward(ctx, q, k, freqs):\n        q_rotated, k_rotated, cos, sin = rope_fwd(q, k, freqs)\n\n        ctx.save_for_backward(cos, sin)\n        return q_rotated, k_rotated\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_q_rotated, grad_k_rotated):\n        cos, sin = ctx.saved_tensors\n        grad_q, grad_k = rope_bwd(grad_q_rotated, grad_k_rotated, cos, sin)\n\n        return grad_q, grad_k, None\n\n\nclass RotaryEmbedding(nn.Module):\n    \"\"\"\n    RotaryEmbedding is a PyTorch module that implements rotary positional embeddings for attention mechanisms.\n    Args:\n        config (object): Configuration object containing the following attributes:\n            hidden_size (int): The hidden size of the model.\n            num_attention_heads (int): The number of attention heads.\n    Attributes:\n        inv_freq (torch.Tensor): A tensor containing the inverse frequencies for the rotary embeddings.\n    Methods:\n        forward(seq_len):\n            Computes the rotary positional embeddings for a given sequence length.\n            Args:\n                seq_len (int): The length of the input sequence.\n            Returns:\n                torch.Tensor: A tensor containing the rotary positional embeddings with shape (1, seq_len, dim).\n    \"\"\"\n\n    def __init__(self,config):\n        super().__init__()\n        dim = int(config.hidden_size // config.num_attention_heads)\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, seq_len):\n        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq)\n        freqs = torch.einsum(\"i, j -> i j\", t, self.inv_freq)\n\n        return freqs[None, :, :]\n\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange, reduce, repeat\nfrom typing import Optional, Tuple\n\n        \nclass EncoderAttention(nn.Module):\n    def __init__(self, config, layer_idx: int) -> None:\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.head_size = int(config.hidden_size // config.num_attention_heads)\n        self.attention_bias = getattr(config, \"attention_bias\", True)\n        self.layer_idx = layer_idx\n        # self.qkv = nn.Linear(config.hidden_size,3*config.hidden_size)\n        self.q = MyLinear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.k = MyLinear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.v = MyLinear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.out =MyLinear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.out =  LinearRMSFused(config.hidden_size,config.hidden_size) \n        self.num_attention_heads = config.num_attention_heads\n        self.dot_attn = ScaledDotProductAttention.apply\n        self.apply_rotary_pos_emb =  RotaryEmbeddingFunction.apply\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        q = self.q(hidden_state)\n        k = self.k(hidden_state)\n        v = self.v(hidden_state)\n        # q,k,v = self.qkv(hidden_state).chunk(3, dim = -1) #b X l X d dim =-1 or 2\n        # place holder for RoPe operation\n        q = rearrange(q, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        k = rearrange(k, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        v = rearrange(v, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        if freqs is not None:\n            q, k = self.apply_rotary_pos_emb(q, k, freqs)\n\n        # out = torch.nn.functional.scaled_dot_product_attention(\n        #     query=q, key=k, value=v, attn_mask=attention_mask, is_causal=False\n        # )\n        out = self.dot_attn(q,k,v,attention_mask)\n        out = rearrange(out, \"b h l d -> b l (h d)\")\n        \n        b,l,d = out.size()\n        \n        out = out.view(-1,d).contiguous()\n        hidden_state = hidden_state.view(-1,d).contiguous()\n        \n        out = self.out(out,hidden_state)\n        \n        return out.view(b,l,d).contiguous()\n\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple\n\nfrom dataclasses import dataclass\n\n\n\n@dataclass\nclass EncoderOutput(object):\n    logits: torch.Tensor\n\n\n@dataclass\nclass MLMOutput(object):\n    hidden_state: torch.Tensor\n    logits: torch.Tensor\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, config, layer_idx: int, attention_type: str = None) -> None:\n        super().__init__()\n        self.attention = (\n         \n            EncoderAttention(config, layer_idx=layer_idx)\n        )\n        # FeedForward(config) #F\n        # self.feed_forward = DoubleLinearModule(config.hidden_size, 4*config.hidden_size, config.hidden_size) #FeedForward(config)\n        self.feed_forward = FFNGeluModule(config.hidden_size, 4*config.hidden_size, config.hidden_size)\n        self.layer_idx = layer_idx\n        self.layernorm = RMSNorm(\n            config.hidden_size, eps=getattr(config, \"layer_norm_eps\", 1e-6)\n        )\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: torch.Tensor = None,\n    ) -> torch.Tensor:\n        out = self.attention(\n            hidden_state=hidden_state, attention_mask=attention_mask, freqs=freqs\n        )\n        b,l,d = out.size()\n        out1 = out.view(-1,d).contiguous()\n        out1 = self.feed_forward(out1)\n        out1 = out1.view(b,l,d).contiguous() #out # \n        return self.layernorm(out1 + out)\n\n\nclass EncoderModel(nn.Module):\n\n    def __init__(\n        self,\n        config,\n        pos_embedding_type: Optional[str] = \"rope\",\n        attention_type: str = None,\n    ) -> None:\n        super().__init__()\n        self.word_embeddings = MyEmbedding(config.vocab_size, config.hidden_size,config.pad_token_id)\n     \n        self.emb_freq = RotaryEmbedding(config)(config.max_position_embeddings)\n        \n        self.all_layer = nn.ModuleList(\n            [\n                EncoderLayer(config, layer_idx, attention_type)\n                for layer_idx in range(config.num_hidden_layers)\n            ]\n        )\n\n    def forward(\n        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n    ) -> torch.Tensor:\n        bsz, seqlen = input_ids.shape\n        hidden_state = self.word_embeddings(input_ids)\n        freqs = self.emb_freq[:, :seqlen].to(input_ids.device)\n\n        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2).type_as(hidden_state)\n        attention_mask = (1.0 - attention_mask) * torch.finfo(hidden_state.dtype).min\n\n        for layer in self.all_layer:\n            hidden_state = layer(hidden_state, attention_mask, freqs)\n        return EncoderOutput(hidden_state)\n\n    @classmethod\n    def from_config(\n        cls,\n        config,\n        pos_embedding_type: Optional[str] = \"absolute\",\n        attention_type: str = None,\n    ) -> nn.Module:\n        return cls(config, pos_embedding_type, attention_type)\n\n\ndef chunked_cross_entropy(logits,targets,chunk_size = 32,ignore_index=-100):\n    targets = targets.view(-1)\n    \n    logit_chunks = logits.split(chunk_size)\n    target_chunks = targets.split(chunk_size)\n    loss_chunks = [\n        torch.nn.functional.cross_entropy(\n            logit_chunk, target_chunk, ignore_index=ignore_index, reduction=\"none\"\n        )\n        for logit_chunk, target_chunk in zip(logit_chunks, target_chunks)\n    ]\n    non_masked_elems = (targets != ignore_index).sum()\n    loss = torch.cat(loss_chunks).sum() / non_masked_elems.maximum(\n        torch.ones_like(non_masked_elems)\n    )\n    return loss\n    \n@torch.compile\ndef linear_cross_entropy_fwd(inputs, weight, bias, targets, ignore_index=-100):\n    logits = F.linear(inputs, weight, bias)\n    shape = logits.size()\n    logits_flat = logits.view(-1, shape[-1])\n    targets_flat = targets.view(-1)\n    valid_mask = targets_flat != ignore_index\n    valid_logits = logits_flat[valid_mask]\n    valid_targets = targets_flat[valid_mask]\n\n    softmax = F.softmax(valid_logits, dim=-1)\n    log_probs = torch.log(softmax + 1e-12)  # Numerical stability\n    target_log_probs = log_probs[torch.arange(valid_logits.size(0)), valid_targets]\n    loss = -target_log_probs.mean()\n    return loss, softmax, valid_targets, valid_mask, logits_flat,shape\n\n@torch.compile\ndef linear_cross_entropy_bwd(grad_outputs,inputs, weight, bias, softmax, valid_targets, valid_mask, logits_flat,shape):\n    grad_logits = torch.zeros_like(logits_flat)\n    valid_grad_logits = softmax.clone()\n    valid_grad_logits[torch.arange(valid_grad_logits.size(0)), valid_targets] -= 1\n    valid_grad_logits /= valid_grad_logits.size(0)  # Normalize by batch size\n\n    grad_logits[valid_mask] = valid_grad_logits\n    grad_logits = grad_logits.view(*shape)\n    grad_input = grad_weight = grad_bias = None\n\n    grad_loss = grad_logits * grad_outputs\n\n    grad_input = grad_loss.matmul(weight)\n    grad_weight = grad_loss.transpose(-2, -1).matmul(inputs)\n    grad_bias = grad_loss.sum(dim=0)\n\n    # Return all grads corresponding to forward inputs: inputs, weight, bias, targets, ignore_index\n    return grad_input, grad_weight, grad_bias\n    \nclass LinearCrossEntropyIgnoreIndex(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, inputs, weight, bias, targets, ignore_index=-100):\n        loss, softmax, valid_targets, valid_mask, logits_flat,shape = linear_cross_entropy_fwd(inputs, weight, bias, targets, ignore_index=-100)\n\n        ctx.save_for_backward(inputs, weight, bias, softmax, valid_targets, valid_mask, logits_flat)\n        ctx.shape = shape\n        ctx.ignore_index = ignore_index\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_outputs):\n        inputs, weight, bias, softmax, valid_targets, valid_mask, logits_flat = ctx.saved_tensors\n        grad_input, grad_weight, grad_bias = linear_cross_entropy_bwd(grad_outputs,inputs, weight, bias, softmax, valid_targets, valid_mask, logits_flat, ctx.shape)\n        # Return all grads corresponding to forward inputs: inputs, weight, bias, targets, ignore_index\n        return grad_input, grad_weight, grad_bias, None, None\n\n\nclass MyLinearCrossEntropy(torch.nn.Module):\n    def __init__(self, in_features, out_features, ignore_index=-100):\n        super(MyLinearCrossEntropy, self).__init__()\n        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n        self.ignore_index = ignore_index\n\n        torch.nn.init.xavier_uniform_(self.weight)\n        torch.nn.init.zeros_(self.bias)\n\n    def forward(self, x, target=None):\n        if target is not None:\n            return LinearCrossEntropyIgnoreIndex.apply(x, self.weight, self.bias, target, self.ignore_index)\n        else:\n            if x.dim() != 2:\n                B, l, d = x.shape\n                x = x.view(-1, d).contiguous()\n            output = x.mm(self.weight.t())\n            # if bias is not None:\n            output += self.bias.unsqueeze(0).expand_as(output)\n            \n            return output\n\nclass callback:\n    def __init__(self):\n        self.loss = list()\n        self.model = list()\n    \n    def put(self, model, loss):\n        self.loss.append(loss)\n        self.model.append(model)\n\n    def get_model(self):\n        ind = np.argmin(self.loss)\n        return self.model[ind]\n\nclass ClinicModel(nn.Module):\n    def __init__(self, config,n_classes):\n        super(ClinicModel, self).__init__()\n        model = EncoderModel(config,pos_embedding_type='rope')\n        self.model = model\n        self.output = MyLinearCrossEntropy(768, n_classes)\n\n    def forward(self, ids, mask,labels=None):\n        sequence_output = self.model(ids, mask).logits[:, 0, :]\n#         sequence_output = sequence_output[:, 0, :]\n        logits = self.output(sequence_output,labels)\n        return logits\n\n\nclass ClinicDataset(Dataset):\n    def __init__(self, data,is_test=False):\n        self.X = data['Text'].values\n        self.Y = data['Target'].values\n        self.is_test = is_test\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.MAX_LEN = 128\n        \n    def __getitem__(self, idx):\n        inputs = self.tokenizer.encode_plus(self.X[idx],\n            add_special_tokens=True,\n            truncation=True,\n            max_length=self.MAX_LEN\n        )['input_ids'] \n\n        if not self.is_test:\n            target_value = self.Y[idx]\n      \n        mask = [1]*len(inputs) + [0] * (self.MAX_LEN - len(inputs)) \n        mask = torch.tensor(mask, dtype=torch.long)\n        \n        if len(inputs) != self.MAX_LEN:\n            inputs = inputs + [self.tokenizer.pad_token_id] * (self.MAX_LEN - len(inputs)) \n        ids = torch.tensor(inputs, dtype=torch.long)\n        \n        \n        \n        \n        if self.is_test:\n            return {\n                'ids': ids,\n                'mask': mask,\n            }\n        \n        else:\n            targets = torch.FloatTensor(target_value)\n            return {\n                'ids': ids,\n                'mask': mask,\n                'targets': targets\n            }\n        \n    def __len__(self):\n        return len(self.Y)\n\n\n\nclass ClinicDatasetV2(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return {'ids':item.get('input_ids'),'mask':item.get('attention_mask'),'labels':item.get('labels')}\n\n    def __len__(self):\n        return len(self.labels)\n\n\n\ndef valid_func(model, val_loader, accelerator):\n    model.eval()\n    loss_fn = torch.nn.CrossEntropyLoss()\n    PROB = []\n    TARGETS = []\n    losses = []\n    PREDS = []\n\n    for batch_idx, data in enumerate(val_loader):\n        input_ids = data[\"ids\"]\n        input_masks = data[\"mask\"]\n        targets = data[\"labels\"].long().view(-1)\n        with torch.no_grad():\n            logits = model(input_ids, input_masks)\n            \n        logits, targets = accelerator.gather_for_metrics((logits, targets))\n\n        PREDS += [torch.argmax(logits, 1).detach().cpu()]\n        TARGETS += [targets.detach().cpu()]\n\n        loss = loss_fn(logits, targets)\n        losses.append(loss.item())\n\n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n    accuracy = (PREDS == TARGETS).mean()\n\n    loss_valid = np.mean(losses)\n    return loss_valid, accuracy\n@dataclass\nclass Config:\n    hidden_size: int = 768\n    num_attention_heads: int = 12\n    num_key_value_heads: int = 4\n    max_position_embeddings: int = 514\n    num_hidden_layers: int = 4\n    vocab_size: int = 50265\n    hidden_dropout_prob: float = 0.1\n    initializer_range: float = 0.02\n    intermediate_size: int = 3072\n    layer_norm_eps: float = 1e-05\n    pad_token_id: int = 1\n    hidden_act: str = \"gelu\"\n    \ndef main():\n    ddp_kwargs = DistributedDataParallelKwargs() #find_unused_parameters=True\n    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])\n    \n    model_ckpt = \"roberta-base\"\n    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n    \n    device = accelerator.device\n    is_main_process = accelerator.is_main_process\n    \n    if is_main_process:\n        print(f\"Using device: {device}\")\n        print(f\"Number of processes: {accelerator.num_processes}\")\n        print(f\"Distributed type: {accelerator.distributed_type}\")\n    \n    epochs = 3\n    learning_rate = 1e-4\n    weight_decay = 0.0\n    BATCH_SIZE = 32\n    \n    base_dir = \"path\"\n    save_dir = os.path.join(base_dir, \"checkpoints\")\n    samples_dir = os.path.join(base_dir, \"samples\")\n    \n    # main process is from accelerator\n    if is_main_process:\n        os.makedirs(save_dir, exist_ok=True)\n        os.makedirs(samples_dir, exist_ok=True)\n        print(f\"Saving checkpoints to: {save_dir}\")\n        print(f\"Saving samples to: {samples_dir}\")\n\n    data_path = \"../input/data-for-distilation\"\n    train = pd.read_csv(\"../input/data-for-distilation/Clinc_Train.csv\")\n    valid = pd.read_csv(\"../input/data-for-distilation/Clinc_valid.csv\")\n    n_classes = np.unique(train.Target).shape[0]\n    \n    if is_main_process:\n        print(\"Initializing Encoder Model...\")\n    config = Config()\n    config.num_hidden_layers = 12\n    model = ClinicModel(config,n_classes)\n    \n    if is_main_process:\n        print(\"Loading data...\")\n\n    train_texts = train['Text'].values.tolist()\n    val_texts = valid['Text'].values.tolist()\n    train_labels = train['Target'].values.tolist()\n    val_labels = valid['Target'].values.tolist()\n    \n    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n    \n    train_loader = torch.utils.data.DataLoader(ClinicDatasetV2(train_encodings, train_labels),batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n    val_loader = torch.utils.data.DataLoader(ClinicDatasetV2(val_encodings, val_labels),batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n    \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * len(train_loader))\n    \n    model, optimizer, train_loader,val_loader, scheduler = accelerator.prepare(model, optimizer, train_loader,val_loader, scheduler)\n    \n    \n    if is_main_process:\n        print(\"Starting training...\")\n    global_step = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0.0\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", disable=not is_main_process)\n        for data in progress_bar:\n            input_ids = data['ids']\n            input_masks = data['mask']\n            targets = data['labels'].long().view(-1)\n          \n            \n            with accelerator.accumulate(model):\n                loss = model(input_ids,input_masks,targets)\n                accelerator.backward(loss)\n                \n                if accelerator.sync_gradients:\n                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n                \n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n            \n            loss_value = loss.detach().float().item()\n            epoch_loss += loss_value\n            \n            if is_main_process:\n                progress_bar.set_postfix({\"loss\": loss_value, \"lr\": optimizer.param_groups[0]['lr']})\n            \n            global_step += 1\n\n        gathered_epoch_loss = accelerator.gather(torch.tensor(epoch_loss, device=device)).sum().item()\n        avg_loss = gathered_epoch_loss / (len(train_loader) * accelerator.num_processes)\n        vloss, vaccuracy = valid_func(model, val_loader,accelerator)\n        \n        \n        if is_main_process:\n            accelerator.print(f\"Epoch {epoch+1} : loss = {avg_loss}, accuracy =  {vaccuracy}\")\n        \n        if is_main_process and ((epoch + 1) % 5 == 0 or epoch == epochs - 1):\n            unwrapped_model = accelerator.unwrap_model(model)\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': unwrapped_model.state_dict(),\n                'loss': avg_loss,\n            }, \n            f\"{save_dir}/stable_dit_model_epoch_{epoch+1}.pt\")\n            accelerator.print(f\"Checkpoint saved at epoch {epoch+1}\")\n        \n        accelerator.wait_for_everyone()\n    \n    if is_main_process:\n        accelerator.end_training()\n        print(\"Training completed!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T15:05:15.060638Z","iopub.execute_input":"2025-09-20T15:05:15.061000Z","iopub.status.idle":"2025-09-20T15:05:15.074147Z","shell.execute_reply.started":"2025-09-20T15:05:15.060971Z","shell.execute_reply":"2025-09-20T15:05:15.073196Z"}},"outputs":[{"name":"stdout","text":"Overwriting fused_encoder_train.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"! accelerate launch --num_processes=2 ../working/fused_encoder_train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T15:05:16.634659Z","iopub.execute_input":"2025-09-20T15:05:16.634956Z","iopub.status.idle":"2025-09-20T15:09:44.868402Z","shell.execute_reply.started":"2025-09-20T15:05:16.634933Z","shell.execute_reply":"2025-09-20T15:09:44.867618Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda:0\nNumber of processes: 2\nDistributed type: MULTI_GPU\nSaving checkpoints to: path/checkpoints\nSaving samples to: path/samples\nInitializing Encoder Model...\nLoading data...\nStarting training...\nEpoch 1/3:   0%|                                        | 0/239 [00:00<?, ?it/s][rank1]:W0920 15:05:36.368000 1336 torch/fx/experimental/symbolic_shapes.py:5124] [6/0] failed during evaluate_expr(Eq(151*u0, 0), hint=None, size_oblivious=False, forcing_spec=False\n[rank1]:E0920 15:05:36.369000 1336 torch/fx/experimental/recording.py:298] [6/0] failed while running evaluate_expr(*(Eq(151*u0, 0), None), **{'fx_node': False})\n[rank0]:W0920 15:05:36.375000 1335 torch/fx/experimental/symbolic_shapes.py:5124] [6/0] failed during evaluate_expr(Eq(151*u0, 0), hint=None, size_oblivious=False, forcing_spec=False\n[rank0]:E0920 15:05:36.375000 1335 torch/fx/experimental/recording.py:298] [6/0] failed while running evaluate_expr(*(Eq(151*u0, 0), None), **{'fx_node': False})\nEpoch 1/3: 100%|███████| 239/239 [01:20<00:00,  2.98it/s, loss=1.23, lr=7.49e-5]\nEpoch 1 : loss = 2.0658416109603817, accuracy =  0.6909677419354838\nEpoch 2/3: 100%|█████| 239/239 [01:17<00:00,  3.07it/s, loss=0.0746, lr=2.48e-5]\nEpoch 2 : loss = 0.5173055098146574, accuracy =  0.8138709677419355\nEpoch 3/3: 100%|██████| 239/239 [01:17<00:00,  3.08it/s, loss=0.257, lr=1.08e-9]\nEpoch 3 : loss = 0.10398227899144384, accuracy =  0.8496774193548388\nCheckpoint saved at epoch 3\nTraining completed!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":" !pip install -q -U triton --no-index --find-links ../input/triton-3-0-0/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:48:39.561454Z","iopub.execute_input":"2025-09-20T14:48:39.561837Z","iopub.status.idle":"2025-09-20T14:48:50.577936Z","shell.execute_reply.started":"2025-09-20T14:48:39.561808Z","shell.execute_reply":"2025-09-20T14:48:50.576913Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}