{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6019472,"sourceType":"datasetVersion","datasetId":3445072},{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile multimodel_train.py\nimport math\nfrom typing import List, Optional, Tuple, Union\nimport sys\nfrom PIL import Image\nfrom pathlib import Path\nimport torch\nimport os\nfrom accelerate import Accelerator\nfrom torch import nn\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    AutoConfig,\n    RobertaModel,\n    RobertaConfig,\n)\nimport sys\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport torch.nn as nn\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Generator, List, Optional, Tuple\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom transformers.tokenization_utils_base import AddedToken\nfrom transformers import AutoModel\nfrom accelerate import Accelerator, DistributedDataParallelKwargs\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport warnings\n\nwarnings.simplefilter(\"ignore\")\nfrom transformers import ViTFeatureExtractor, ViTModel\nfrom einops import rearrange\nclass RotaryEmbedding(nn.Module):\n    \"\"\"\n    RotaryEmbedding is a PyTorch module that implements rotary positional embeddings for attention mechanisms.\n    Args:\n        config (object): Configuration object containing the following attributes:\n            hidden_size (int): The hidden size of the model.\n            num_attention_heads (int): The number of attention heads.\n    Attributes:\n        inv_freq (torch.Tensor): A tensor containing the inverse frequencies for the rotary embeddings.\n    Methods:\n        forward(seq_len):\n            Computes the rotary positional embeddings for a given sequence length.\n            Args:\n                seq_len (int): The length of the input sequence.\n            Returns:\n                torch.Tensor: A tensor containing the rotary positional embeddings with shape (1, seq_len, dim).\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        dim = int(config.hidden_size // config.num_attention_heads)\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, seq_len):\n        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq)\n        freqs = torch.einsum(\"i, j -> i j\", t, self.inv_freq)\n\n        return freqs[None, :, :]\n\n\n# Copied from transformers\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, freqs, unsqueeze_dim=1) -> Tuple[torch.Tensor]:\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        freqs: precalculated frqs for sin cos\n        only_q: bool = False for encoder decoder\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    emb = torch.cat((freqs, freqs), dim=-1)\n    cos = emb.cos().to(dtype=q.dtype)\n    sin = emb.sin().to(dtype=q.dtype)\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n_ACT_ = {\n    \"gelu\": nn.GELU(),\n    \"leaky_relu\": nn.LeakyReLU(),\n    \"relu6\": nn.ReLU6(),\n    \"sigmoid\": nn.Sigmoid(),\n    \"silu\": nn.SiLU(),\n    \"swish\": nn.SiLU(),\n    \"tanh\": nn.Tanh(),\n}\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, config, multiplier: Union[int, float] = 4) -> None:\n        super().__init__()\n        intermediate_size = getattr(config, \"intermediate_size\", None)\n        self.intermediate_size = (\n            int(multiplier) * config.hidden_size\n            if intermediate_size is None\n            else intermediate_size\n        )\n\n        self.intermediate = nn.Linear(config.hidden_size, self.intermediate_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        if _ACT_.get(getattr(config, \"hidden_act\", None), None):\n            self.act_fn = _ACT_[config.hidden_act]\n        else:\n            self.act_fn = nn.GELU()\n        self.out = nn.Linear(self.intermediate_size, config.hidden_size)\n\n    def forward(\n        self, hidden_state: torch.Tensor, input_tensor: torch.Tensor\n    ) -> torch.Tensor:\n        output = self.intermediate(hidden_state)\n        output = self.act_fn(output)\n        output = self.out(output)\n        output = self.dropout(output)\n        output = self.layernorm(output + input_tensor)\n        return output\nclass AbsoluteEncoding(nn.Module):\n    def __init__(self, config) -> None:\n        super().__init__()\n        self.pos_embeddings = nn.Embedding(\n            config.max_position_embeddings,\n            config.hidden_size,\n            padding_idx=getattr(config, \"pad_token_id\", None),\n        )\n        self.register_buffer(\n            \"position_ids\",\n            torch.arange(config.max_position_embeddings).expand((1, -1)),\n            persistent=False,\n        )\n        self.max_size = config.max_position_embeddings\n\n    def forward(self, size: int) -> torch.Tensor:\n        if self.max_size < size:\n            raise ValueError(\n                f\"The hidden size ({size }) is more than the config max_position_embeddings {self.max_size}\"\n            )\n        return self.pos_embeddings(self.position_ids[:, :size])\n\n\n_position_embeddings = {\"absolute\": AbsoluteEncoding}\nclass AttentionSelfOutput(nn.Module):\n    def __init__(self, config, bias: Optional[bool] = True):\n        super().__init__()\n        self.dense = nn.Linear(\n            config.hidden_size,\n            config.hidden_size,\n            bias=bias,\n        )\n        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(\n        self, hidden_states: torch.Tensor, input_tensor: torch.Tensor\n    ) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.layernorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass DecoderAttention(nn.Module):\n    def __init__(self, config, layer_idx: int) -> None:\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.head_size = int(config.hidden_size // config.num_attention_heads)\n        self.attention_bias = getattr(config, \"attention_bias\", True)\n        self.layer_idx = layer_idx\n        # self.qkv = nn.Linear(config.hidden_size,3*config.hidden_size)\n        self.query = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.key = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.value = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.out = AttentionSelfOutput(config=config, bias=self.attention_bias)\n        self.num_attention_heads = config.num_attention_heads\n        self.scaling = 1 / math.sqrt(self.head_size)\n\n        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n        if not self.flash and self.layer_idx == 0:  # avoid to print m times:\n            print(\"WARNING: Flash Attention requires PyTorch >= 2.0\")\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = False,\n        start_pos: Optional[int] = 0,\n    ) -> Tuple[torch.Tensor, object]:\n        q = self.query(hidden_state)\n        k = self.key(hidden_state)\n        v = self.value(hidden_state)\n        # q,k,v = self.qkv(hidden_state).chunk(3, dim = -1) #b X l X d dim =-1 or 2\n        # place holder for RoPe operation\n        q = rearrange(q, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        k = rearrange(k, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        v = rearrange(v, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n\n        if freqs is not None:\n            q, k = apply_rotary_pos_emb(q, k, freqs)\n\n        if use_cache:\n            cache = getattr(self, \"cache\", None)\n            if cache is None:\n                raise ValueError(\n                    \"you need to setup cache for every attention layer with model.setup_cache()\"\n                )\n            k, v = cache.update(k, v, start_pos)\n\n        attn_weights = torch.einsum(\"b h i d , b h j d -> b h i j\", q, k) * self.scaling\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : k.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32)\n        # attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        out = torch.einsum(\"b h i j , b h j d -> b h i d\", attn_weights, v)\n\n        out = rearrange(out, \"b h l d -> b l (h d)\")\n\n        return self.out(out, hidden_state)\nclass DecoderLayer(nn.Module):\n    def __init__(self, config, layer_idx: int = 0, attention_type: str = None) -> None:\n        super().__init__()\n        self.attention = (\n            DecoderAttentionGqa(config, layer_idx=layer_idx)\n            if attention_type == \"gqa\"\n            else DecoderAttention(config, layer_idx=layer_idx)\n        )\n        if attention_type == \"gqa\" and layer_idx == 0:  # avoid to print m times\n            print(\"Decoder Using GQA Attention\")\n        if attention_type == \"gqa\" and layer_idx == 0:  # avoid to print m times\n            print(\"Using GQA in Cross Attention\")\n        self.feed_forward = FeedForward(config)\n        self.layer_idx = layer_idx\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = False,\n        start_pos: Optional[int] = 0,\n    ) -> torch.Tensor:\n        out = self.attention(\n            hidden_state=hidden_state,\n            attention_mask=attention_mask,\n            freqs=freqs,\n            use_cache=use_cache,\n            start_pos=start_pos,\n        )\n\n        out = self.feed_forward(out, hidden_state)\n        return out\n\n\nclass LMHead(nn.Module):\n    def __init__(self, config) -> None:\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        self.vocab = nn.Linear(config.hidden_size, config.vocab_size)\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n        self.vocab.bias = self.bias\n\n    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n        x = self.dense(hidden_state)\n        x = nn.GELU()(x)\n        x = self.layer_norm(x)\n\n        # project back to size of vocabulary with bias\n        x = self.vocab(x)\n\n        return x\n# mainly 2way to do one keep it into the model init like llama https://github.com/meta-llama/llama/blob/main/llama/model.py\n# every attention layer have its own kv-cache storage\n# or keep all attention layer kv-cache into single storage like Huggingface Transformer\n\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Generator, List, Optional, Tuple\nimport torch\n\n\nclass DynamicCache:\n    \"\"\"\n    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n\n    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n    `[batch_size, num_heads, seq_len, head_dim]`.\n    \"\"\"\n\n    def __init__(self, config, is_gqa: Optional[bool] = False) -> None:\n        self.key_cache: torch.Tensor = None\n        self.value_cache: torch.Tensor = None\n        self._seen_tokens = False\n        self.maxlen = config.max_position_embeddings\n\n    def __len__(self) -> int:\n        if self.key_cache is None:\n            return 0\n        \"\"\"\n        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n        to the number of layers in the model.\n        \"\"\"\n        return self.key_cache.shape[-2]\n\n    def update(\n        self, key_states: torch.Tensor, value_states: torch.Tensor, start_pos: int = 0\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n\n        # Update the cache first iteration\n        if self.key_cache is None:\n            self._seen_tokens = True\n            self.key_cache = key_states.clone()\n            self.value_cache = value_states.clone()\n        else:\n            self.key_cache = torch.cat([self.key_cache, key_states], dim=-2)\n            self.value_cache = torch.cat([self.value_cache, value_states], dim=-2)\n\n        return self.key_cache, self.value_cache\n\n    def get(self) -> Tuple[torch.Tensor]:\n        if self._seen_tokens:\n            return self.key_cache, self.value_cache\n        else:\n            raise ValueError(\"there is no token available in kv-cache\")\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        if self.key_cache is None:\n            return 0\n        return self.key_cache.shape[-2]\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n        return None\n\n\nclass StaticCache:\n    \"\"\"\n    A cache that is size fixed suitable for torch.compile\n\n    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n    `[batch_size, num_heads, seq_len, head_dim]`.\n    \"\"\"\n\n    def __init__(self, config, is_gqa: Optional[bool] = False) -> None:\n        self.head_size = int(config.hidden_size // config.num_attention_heads)\n        self.heads = None\n        if is_gqa:\n            self.heads = getattr(config, \"num_key_value_heads\", None)\n            if self.heads is None:\n                raise ValueError(\n                    \"you are using is_gqa=True and config.num_key_value_heads is not available\"\n                )\n        if self.heads is None:\n\n            self.heads = config.num_attention_heads\n\n        self.key_cache: torch.Tensor = torch.zeros(\n            1,\n            self.heads,\n            config.max_position_embeddings,\n            self.head_size,\n        )\n        self.value_cache: torch.Tensor = torch.zeros(\n            1,\n            self.heads,\n            config.max_position_embeddings,\n            self.head_size,\n        )\n        self._seen_tokens = False\n\n    def update(\n        self, k: torch.Tensor, v: torch.Tensor, start_pos: int = 0\n    ) -> Tuple[torch.Tensor]:\n        self._seen_tokens = True\n        bsz, head, seqlen, _ = k.shape\n        self.first_update_len = seqlen\n        if seqlen > self.key_cache.size()[2]:\n            raise ValueError(\n                f\"{k.shape} is more than init k_cache size {self.key_cache}\"\n            )\n\n        assert bsz == 1, \"Only support batch size 1\"\n\n        self.key_cache = self.key_cache.to(k)\n        self.value_cache = self.value_cache.to(v)\n\n        self.key_cache[:bsz, :, start_pos : start_pos + seqlen] = k\n        self.value_cache[:bsz, :, start_pos : start_pos + seqlen] = v\n\n        k = self.key_cache[:bsz, :, : start_pos + seqlen]\n        v = self.value_cache[:bsz, :, : start_pos + seqlen]\n\n        return k, v\n\n    def get(self) -> Tuple[torch.Tensor]:\n        if self._seen_tokens:\n            k = self.key_cache[:, :, : self.first_update_len]\n            v = self.value_cache[:, :, : self.first_update_len]\n\n            return k, v\n        else:\n            raise ValueError(\"there is no token available in kv-cache\")\n\n    def __len__(self) -> int:\n        if self._seen_tokens == False:\n            return 0\n        \"\"\"\n        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n        to the number of layers in the model.\n        \"\"\"\n        return self.key_cache.shape[2]\n@dataclass\nclass DecoderOutput(object):\n    logits: torch.Tensor\n\n\nclass LMHead(nn.Module):\n    \"\"\"Head for masked language modelling\"\"\"\n\n    def __init__(self, config) -> None:\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        self.vocab = nn.Linear(config.hidden_size, config.vocab_size)\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n        self.vocab.bias = self.bias\n\n    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n        x = self.dense(hidden_state)\n        x = nn.GELU()(x)\n        x = self.layer_norm(x)\n\n        # project back to size of vocabulary with bias\n        x = self.vocab(x)\n\n        return x\n\n\nclass DecoderModel(nn.Module):\n    def __init__(\n        self,\n        config,\n        pos_embedding_type: Optional[str] = \"absolute\",\n        attention_type: str = None,\n    ) -> None:\n        super().__init__()\n        self.word_embeddings = nn.Embedding(\n            config.vocab_size,\n            config.hidden_size,\n            padding_idx=getattr(config, \"pad_token_id\", None),\n        )\n        if _position_embeddings.get(pos_embedding_type, None) is not None:\n            self.position_embeddings = _position_embeddings.get(pos_embedding_type)(\n                config\n            )\n        else:\n            self.position_embeddings = None\n        if pos_embedding_type == \"rope\":\n            self.emb_freq = RotaryEmbedding(config)(config.max_position_embeddings)\n            print(\n                \"Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\"\n            )\n        self.all_layer = nn.ModuleList(\n            [\n                DecoderLayer(config, layer_idx, attention_type=attention_type)\n                for layer_idx in range(config.num_hidden_layers)\n            ]\n        )\n\n    def _init_weights(self, module: nn.Module) -> None:\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(\n                module.weight, mean=0.0, std=0.02 / torch.sqrt(2 * len(self.all_layer))\n            )\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(\n                module.weight, mean=0.0, std=0.02 / torch.sqrt(2 * len(self.all_layer))\n            )\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        use_cache: Optional[bool] = False,\n        start_pos: Optional[int] = 0,\n    ) -> torch.Tensor:\n        _bsz, seqlen, _ = hidden_state.shape\n\n        if self.position_embeddings is not None:\n            pos_info = pos_info = self.position_embeddings(start_pos + seqlen)[\n                :, start_pos : start_pos + seqlen, :\n            ].to(inputs_embeds.device)\n            hidden_state = inputs_embeds + pos_info\n        else:\n            freqs = self.emb_freq[:, start_pos : start_pos + seqlen].to(\n                hidden_state.device\n            )\n\n        for layer in self.all_layer:\n            hidden_state = layer(\n                hidden_state=hidden_state,\n                attention_mask=attention_mask,\n                freqs=freqs,\n                use_cache=use_cache,\n                start_pos=start_pos,\n            )\n        return hidden_state\n\n    @classmethod\n    def from_config(cls, config) -> nn.Module:\n        return cls(config)\ndef _update_causal_mask(\n    attention_mask,\n    token_type_ids,\n    inputs_embeds,\n    cache_position,\n    is_training: bool = False,\n):\n\n\n    dtype = inputs_embeds.dtype\n\n\n    min_dtype = torch.finfo(dtype).min\n\n\n    sequence_length = inputs_embeds.shape[1]\n\n\n    target_length = attention_mask.shape[-1]\n\n    causal_mask = torch.full(\n        (sequence_length, target_length),\n        fill_value=min_dtype,\n        dtype=dtype,\n        device=inputs_embeds.device,\n    )\n\n\n    # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n    if sequence_length != 1:\n        if is_training:\n\n\n            causal_mask = torch.triu(causal_mask, diagonal=1)\n        else:\n\n\n            causal_mask[:, :sequence_length] = 0.0  # when using kv-cache\n\n\n    causal_mask *= torch.arange(\n        target_length, device=inputs_embeds.device\n    ) > cache_position.reshape(-1, 1)\n\n\n    causal_mask = causal_mask[None, None, :, :].expand(\n        inputs_embeds.shape[0], 1, -1, -1\n    )\n\n\n    if attention_mask is not None:\n\n\n        causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n\n\n        mask_length = attention_mask.shape[-1]\n\n\n        padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[\n            :, None, None, :\n        ].to(causal_mask.device)\n\n\n        padding_mask = padding_mask == 0\n\n\n        causal_mask[:, :, :, :mask_length] = causal_mask[\n            :, :, :, :mask_length\n        ].masked_fill(padding_mask, min_dtype)\n    return causal_mask\nclass VisionLanguageModel(nn.Module):\n\n    def __init__(\n        self,\n        encoder,\n        decoder_config,\n        decoder_pos_embedding_type: Optional[str] = \"absolute\",\n        decoder_attention_type: str = None,\n    ) -> None:\n        super().__init__()\n        self.is_gqa = True if decoder_attention_type == \"gqa\" else False\n        self.encoder = encoder\n        self.decoder = DecoderModel(\n            config=decoder_config,\n            pos_embedding_type=decoder_pos_embedding_type,\n            attention_type=decoder_attention_type,\n        )\n        self.lm_head = LMHead(config=decoder_config)\n        self.image_token_index = 128001\n\n    def forward(\n        self,\n        pixel_values: Optional[torch.LongTensor] = None,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.LongTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = False,\n        start_pos: Optional[int] = 0,\n    ):\n\n        inputs_embeds = self.decoder.word_embeddings(input_ids)\n        is_training = use_cache == False and token_type_ids is not None\n        cache_position = None\n        if is_training == True:\n            cache_position = torch.arange(\n                inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n\n        else:\n\n            if pixel_values is not None:\n                cache_position = torch.arange(\n                    inputs_embeds.shape[1], device=inputs_embeds.device\n                )\n\n            else:\n                past_seen_tokens = (\n                    start_pos  # previous stored information attn_mask.size-1\n                )\n                cache_position = torch.arange(\n                    past_seen_tokens,\n                    past_seen_tokens + inputs_embeds.shape[1],\n                    device=inputs_embeds.device,\n                )\n\n        if pixel_values is not None:\n            image_features = self.get_encoder_output(pixel_values)\n\n            special_image_mask = (input_ids == self.image_token_index).unsqueeze(-1)\n            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(\n                inputs_embeds.device\n            )\n\n            image_features = image_features.to(\n                inputs_embeds.device, inputs_embeds.dtype\n            )\n            inputs_embeds = inputs_embeds.masked_scatter(\n                special_image_mask, image_features\n            )\n\n        # if position_ids is None:\n        position_ids = cache_position.unsqueeze(0)  # +1 not improving\n\n        causal_mask = _update_causal_mask(\n            attention_mask, token_type_ids, inputs_embeds, cache_position, is_training\n        )\n\n        decoder_output = self.decoder(\n            hidden_state=inputs_embeds,\n            attention_mask=causal_mask,\n            use_cache=use_cache,\n            start_pos=start_pos,\n        )\n        decoder_output = self.lm_head(decoder_output)\n        return DecoderOutput(logits=decoder_output)\n\n    def get_decoder(self) -> DecoderModel:\n        return self.decoder\n\n    def get_encoder_output(self, pixel_values: torch.Tensor) -> object:\n        return self.encoder(pixel_values=pixel_values).last_hidden_state\n\n    def _setup_cache(self, config, cls: Optional[object] = StaticCache) -> None:\n        for layer in self.decoder.all_layer:\n            layer.attention.cache = cls(config, is_gqa=self.is_gqa)\n\n    def _clean_cache(self) -> None:\n        for layer in self.decoder.all_layer:\n            layer.attention.cache = None\n\n    @classmethod\n    def from_config(\n        cls,\n        encoder,\n        decoder_config,\n        decoder_pos_embedding_type: Optional[str] = \"absolute\",\n        decoder_attention_type: str = None,\n    ) -> nn.Module:\n        return cls(\n            encoder,\n            decoder_config,\n            decoder_pos_embedding_type,\n            decoder_attention_type,\n        )\n\ndef build_string_from_input(\n    prompt,\n    bos_token,\n    image_seq_len,\n    image_token,\n    num_images=1,\n):\n\n\n    return f\"{prompt}{image_token * image_seq_len * num_images}\"\n\n\n\ndef get_model_inputs(prompt,tokenizer, suffix=None, max_length=248):\n\n\n    return_token_type_ids = False\n\n\n    if suffix:\n        suffix = suffix\n\n\n        return_token_type_ids = True\n\n    image_seq_length = 197  #\n    \n    IMAGE_TOKEN = \"<image>\"\n    input_string = build_string_from_input(prompt,tokenizer.bos_token,image_seq_length,IMAGE_TOKEN)\n\n\n    return_token_type_ids = True if suffix is not None else False\n    \n\n\n    inputs = tokenizer(\n        input_string,\n        text_pair=suffix,\n        return_token_type_ids=return_token_type_ids,\n        padding=\"max_length\",\n        add_special_tokens=False,\n        max_length=max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    return inputs\n\nclass ImgDataset(Dataset):\n    def __init__(self, df, tokenizer, feature_extractor):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.feature_extractor = feature_extractor\n\n    def __len__(\n        self,\n    ):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        caption = self.df.caption.iloc[idx] + self.tokenizer.eos_token\n        img_path = self.df.image.iloc[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        prompt = self.tokenizer.bos_token + \"<Caption>\"\n        \n        inputs = get_model_inputs(prompt,self.tokenizer, caption)\n        pixel_values = self.feature_extractor(img, return_tensors=\"pt\").pixel_values\n        pixel_values = pixel_values #.to(dtype)\n       \n        inputs_ = {\"pixel_values\": pixel_values.squeeze()}\n        for k, v in inputs.items():\n            inputs_[k] = v.squeeze()\n\n       \n        return inputs_\n\n\n\ndef loss_fn(logits, labels, attention_mask,config):\n    shift_logits = logits[..., :-1, :]\n    shift_labels = labels[..., 1:]\n    if attention_mask is not None:\n        # we use the input attention mask to shift the logits and labels, because it is 2D.\n        # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n        shift_attention_mask = attention_mask[:, -shift_logits.shape[1] :].to(\n            logits.device\n        )\n        shift_logits = shift_logits[\n            shift_attention_mask.to(logits.device) != 0\n        ].contiguous()\n        shift_labels = shift_labels[\n            shift_attention_mask.to(shift_labels.device) != 0\n        ].contiguous()\n    else:\n        shift_logits = shift_logits.contiguous()\n        shift_labels = shift_labels.contiguous()\n\n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n\n    flat_logits = shift_logits.view(-1, config.vocab_size)\n    flat_labels = shift_labels.view(-1)\n    loss = loss_fct(flat_logits, flat_labels)\n    return loss\n\n\ndef prep(batch_size=8):\n    df = pd.read_csv(\"../input/flickr30k/captions.txt\")\n    df[\"image\"] = \"../input/flickr30k/Images/\" + df[\"image\"]\n    BASE_PATH = \"../input/coco-image-caption\"\n    import json\n    \n    with open(\n        f\"{BASE_PATH}/annotations_trainval2014/annotations/captions_train2014.json\", \"r\"\n    ) as f:\n        data = json.load(f)\n        data = data[\"annotations\"]\n    \n    img_cap_pairs = []\n    \n    for sample in data:\n        img_name = \"%012d.jpg\" % sample[\"image_id\"]\n        img_cap_pairs.append([img_name, sample[\"caption\"]])\n    \n    df1 = pd.DataFrame(img_cap_pairs, columns=[\"image\", \"caption\"])\n    df1[\"image\"] = df1[\"image\"].apply(\n        lambda x: f\"{BASE_PATH}/train2014/train2014/COCO_train2014_{x}\"\n    )\n    # captions = captions.sample(70000)\n    df1 = df1.reset_index(drop=True)\n    df1.head()\n    df = pd.concat([df, df1])\n    \n    df = df.sample(frac=1)\n    df = df.dropna()\n    \n    feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n    model_ckpt = \"microsoft/deberta-v3-base\"\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_ckpt, truncation_side=\"right\"\n    )\n    \n    \n    image_seq_length = 197  #\n    \n    \n    IMAGE_TOKEN = \"<image>\"\n    \n    \n    image_token = AddedToken(IMAGE_TOKEN, normalized=False, special=True)\n    \n    \n    tokens_to_add = {\"additional_special_tokens\": [image_token]}\n    \n    \n    tokenizer.add_special_tokens(tokens_to_add)\n    \n    \n    image_token_id = tokenizer.convert_tokens_to_ids(IMAGE_TOKEN)\n    vit_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n    \n    \n    model = AutoModel.from_pretrained(model_ckpt)\n    model.resize_token_embeddings(len(tokenizer))\n    config = model.config\n    config.num_hidden_layers = 8\n    multimodel = VisionLanguageModel(vit_encoder, config, decoder_pos_embedding_type=\"rope\")\n    multimodel.decoder.word_embeddings.weight = model.embeddings.word_embeddings.weight\n    # df.head()\n    train_dataset = ImgDataset(\n        df,\n        tokenizer=tokenizer,\n        feature_extractor=feature_extractor,\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n    return train_loader ,multimodel, config\n    \n\ndef main():\n    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])\n    \n    \n    device = accelerator.device\n    \n    if accelerator.is_main_process:\n        accelerator.print(f\"Using device: {device}\")\n        accelerator.print(f\"Number of processes: {accelerator.num_processes}\")\n        accelerator.print(f\"Distributed type: {accelerator.distributed_type}\")\n    \n    epochs = 1\n    learning_rate = 1e-5\n    weight_decay = 0.0\n    \n    base_dir = \"path\"\n    save_dir = os.path.join(base_dir, \"checkpoints\")\n    samples_dir = os.path.join(base_dir, \"samples\")\n    \n    # main process is from accelerator\n    if accelerator.is_main_process:\n        os.makedirs(save_dir, exist_ok=True)\n        accelerator.print(f\"Saving checkpoints to: {save_dir}\")\n       \n\n    \n    \n    if accelerator.is_main_process:\n        accelerator.print(\"Initializing Dataloader and Model...\")\n        \n    train_loader, model, config = prep()\n    \n    \n    optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n    \n    model, optimizer, train_loader= accelerator.prepare(model, optimizer, train_loader)\n    \n    \n    if accelerator.is_main_process:\n        accelerator.print(\"Starting training...\")\n    global_step = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0.0\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\",disable=not accelerator.is_main_process, file=sys.stdout) #\n        for data in progress_bar:\n            labels = data[\"input_ids\"].masked_fill(data[\"input_ids\"] == 128001, -100)\n            labels = torch.where(data[\"input_ids\"] == config.pad_token_id, -100, labels)\n          \n            \n            with accelerator.accumulate(model):\n                pred = model(**data)\n                loss = loss_fn(pred.logits, labels, data[\"attention_mask\"],config)\n                accelerator.backward(loss)\n                \n                if accelerator.sync_gradients:\n                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n                \n                optimizer.step()\n                optimizer.zero_grad()\n            \n            loss_value = loss.detach().float().item()\n            epoch_loss += loss_value\n            \n            if accelerator.is_main_process:\n                progress_bar.set_postfix({\"loss\": loss_value, \"lr\": optimizer.param_groups[0]['lr']})\n            \n            global_step += 1\n\n        gathered_epoch_loss = accelerator.gather(torch.tensor(epoch_loss, device=device)).sum().item()\n        avg_loss = gathered_epoch_loss / (len(train_loader) * accelerator.num_processes)\n        \n        if accelerator.is_main_process and global_step%5==0:\n            accelerator.print(f\" step -{global_step+1}--loss--{epoch_loss}\")\n\n        accelerator.print(f\" step -{global_step+1}--loss--{epoch_loss}\")\n        \n        \n        if accelerator.is_main_process:\n            accelerator.print(f\"Epoch {epoch+1} : loss = {avg_loss}\")\n        \n        if accelerator.is_main_process and epoch == epochs - 1:\n            unwrapped_model = accelerator.unwrap_model(model)\n            torch.save({\n                'model_state_dict': unwrapped_model.state_dict()\n                \n            }, \n            f\"{save_dir}/multimodel_rope_{epoch+1}.pt\")\n            accelerator.print(f\"Checkpoint saved at epoch {epoch+1}\")\n        \n        accelerator.wait_for_everyone()\n    \n    if accelerator.is_main_process:\n        accelerator.end_training()\n        print(\"Training completed!\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:20:54.322562Z","iopub.execute_input":"2025-07-13T14:20:54.323218Z","iopub.status.idle":"2025-07-13T14:20:54.341998Z","shell.execute_reply.started":"2025-07-13T14:20:54.323184Z","shell.execute_reply":"2025-07-13T14:20:54.341260Z"}},"outputs":[{"name":"stdout","text":"Overwriting multimodel_train.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"! accelerate launch --num_processes=2 ../working/multimodel_train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:20:54.453060Z","iopub.execute_input":"2025-07-13T14:20:54.453807Z"}},"outputs":[{"name":"stdout","text":"2025-07-13 14:21:07.598200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-07-13 14:21:07.598199: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752416467.766157      98 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752416467.766146      99 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752416467.816873      98 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1752416467.816883      99 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing device: cuda:0\nNumber of processes: 2\nDistributed type: DistributedType.MULTI_GPU\nSaving checkpoints to: path/checkpoints\nInitializing Dataloader and Model...\npreprocessor_config.json: 100%|████████████████| 160/160 [00:00<00:00, 1.45MB/s]\ntokenizer_config.json: 100%|██████████████████| 52.0/52.0 [00:00<00:00, 392kB/s]\nconfig.json: 100%|█████████████████████████████| 579/579 [00:00<00:00, 4.12MB/s]\nspm.model: 100%|███████████████████████████| 2.46M/2.46M [00:00<00:00, 3.46MB/s]\nconfig.json: 69.7kB [00:00, 78.6MB/s]\nmodel.safetensors: 100%|██████████████████████| 346M/346M [00:01<00:00, 182MB/s]\nSome weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\npytorch_model.bin: 100%|██████████████████████| 371M/371M [00:01<00:00, 209MB/s]\nmodel.safetensors: 100%|██████████████████████| 371M/371M [00:02<00:00, 164MB/s]\nEncoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\nEncoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\nStarting training...\nEpoch 1/1:   0%|          | 0/35815 [00:00<?, ?it/s]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}