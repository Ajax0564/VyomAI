{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":206037,"sourceType":"datasetVersion","datasetId":89296}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:33.814851Z","iopub.execute_input":"2025-02-28T07:12:33.815177Z","iopub.status.idle":"2025-02-28T07:12:38.510029Z","shell.execute_reply.started":"2025-02-28T07:12:33.815149Z","shell.execute_reply":"2025-02-28T07:12:38.508743Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom einops import rearrange, reduce, repeat\nfrom typing import Optional, Tuple\nfrom tqdm.notebook import tqdm\nimport math\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom accelerate import Accelerator\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\nimport numpy as np\n\nEPOCHS = 5\nlr = 1e-4\nSEED = 42\nBATCH_SIZE = 64","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:38.511502Z","iopub.execute_input":"2025-02-28T07:12:38.511802Z","iopub.status.idle":"2025-02-28T07:12:43.704454Z","shell.execute_reply.started":"2025-02-28T07:12:38.511774Z","shell.execute_reply":"2025-02-28T07:12:43.703653Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class VitAbsoluteEncoding(nn.Module):\n    \"\"\"Construct the Absolute embeddings for vision model\"\"\"\n\n    def __init__(self, config) -> None:\n        super().__init__()\n        image_height, image_width = config.image_size\n        patch_height, patch_width = config.patch_size\n        assert (\n            image_height % patch_height == 0 and image_width % patch_width == 0\n        ), \"Image dimensions must be divisible by the patch size.\"\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = config.num_channels * patch_height * patch_width\n        self.pos_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, patch_dim)) #/ patch_dim ** 0.5\n        self.register_buffer(\n            \"num_patches\",\n            torch.arange(num_patches + 1).expand((1, -1)),\n            persistent=False,\n        )\n\n    def forward(self, img_seq: torch.Tensor) -> torch.Tensor:\n        b, n, _ = img_seq.shape\n        \n        return self.pos_embeddings[:, :n]","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:43.706338Z","iopub.execute_input":"2025-02-28T07:12:43.706778Z","iopub.status.idle":"2025-02-28T07:12:43.713029Z","shell.execute_reply.started":"2025-02-28T07:12:43.706753Z","shell.execute_reply":"2025-02-28T07:12:43.711923Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"_ACT_ = {\n    \"gelu\": nn.GELU(),\n    \"leaky_relu\": nn.LeakyReLU(),\n    \"relu6\": nn.ReLU6(),\n    \"sigmoid\": nn.Sigmoid(),\n    \"silu\": nn.SiLU(),\n    \"swish\": nn.SiLU(),\n    \"tanh\": nn.Tanh(),\n}\n\nfrom typing import Optional, Tuple, Union\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, config, multiplier: Union[int, float] = 4) -> None:\n        super().__init__()\n        self.intermediate = nn.Linear(\n            config.hidden_size, int(multiplier) * config.hidden_size\n        )\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        # if _ACT_.get(getattr(config, \"hidden_act\", None), None):\n        #     self.act_fn = _ACT_[config.hidden_act]\n        # else:\n        self.act_fn = nn.ReLU()\n        self.out = nn.Linear(int(multiplier) * config.hidden_size, config.hidden_size)\n\n    def forward(\n        self, hidden_state: torch.Tensor, input_tensor: torch.Tensor\n    ) -> torch.Tensor:\n        output = self.intermediate(hidden_state)\n        output = self.act_fn(output)\n        output = self.out(output)\n        output = self.dropout(output)\n        output = self.layernorm(output + input_tensor)\n        return output","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:43.714358Z","iopub.execute_input":"2025-02-28T07:12:43.714650Z","iopub.status.idle":"2025-02-28T07:12:43.731194Z","shell.execute_reply.started":"2025-02-28T07:12:43.714626Z","shell.execute_reply":"2025-02-28T07:12:43.730356Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class AttentionSelfOutput(nn.Module):\n    def __init__(\n        self, config, bias: Optional[bool] = True, out_features: Optional[int] = None\n    ):\n        super().__init__()\n        self.dense = nn.Linear(\n            config.hidden_size,\n            config.hidden_size if out_features is None else out_features,\n            bias=bias,\n        )\n        self.layernorm = nn.LayerNorm(\n            config.hidden_size, eps=getattr(config, \"layer_norm_eps\", 1e-6)\n        )\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(\n        self, hidden_states: torch.Tensor, input_tensor: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            hidden_states: torch.FloatTensor of shape (batch, seq_len, embed_dim)`\n            input_tensor: torch.FloatTensor of shape (batch, seq_len, embed_dim)`\n\n        return:\n               hidden_states: torch.FloatTensor of shape (batch, seq_len, embed_dim)\n\n        \"\"\"\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.layernorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass VisionAttention(nn.Module):\n    def __init__(self, config, layer_idx: int) -> None:\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.head_size = int(config.hidden_size // config.num_attention_heads)\n        self.attention_bias = getattr(config, \"attention_bias\", True)\n        self.layer_idx = layer_idx\n        self.qkv = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n        self.out = AttentionSelfOutput(config=config, bias=self.attention_bias)\n        self.num_attention_heads = config.num_attention_heads\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            hidden_states: torch.Tensor of shape (batch, seq_len, embed_dim)`\n            attention_mask: torch.Tensor of shape (batch,1, seq_len, seqlen)`\n            freqs: Positional freqs in case of RoPE embedding\n        return:\n               hidden_states: torch.Tensor of shape (batch, seq_len, embed_dim)\n\n        \"\"\"\n\n        q, k, v = self.qkv(hidden_state).chunk(3, dim=-1)\n        # transform it into batch_size x no_of_heads x seqlen x head_dim for Multihead Attention\n        q = rearrange(q, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        k = rearrange(k, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        v = rearrange(v, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        if freqs is not None:\n            q, k = apply_rotary_pos_emb(\n                q,\n                k,\n                freqs,\n            )  # apply RoPE if freqs is available\n\n        out = torch.nn.functional.scaled_dot_product_attention(\n            query=q, key=k, value=v, attn_mask=attention_mask, is_causal=False\n        )\n        # transform it back into batch_size x seqlen x hidden_dim\n        out = rearrange(out, \"b h l d -> b l (h d)\")\n        return self.out(out, hidden_state)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:43.731990Z","iopub.execute_input":"2025-02-28T07:12:43.732213Z","iopub.status.idle":"2025-02-28T07:12:43.752475Z","shell.execute_reply.started":"2025-02-28T07:12:43.732193Z","shell.execute_reply":"2025-02-28T07:12:43.751621Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple\n\nfrom dataclasses import dataclass\nfrom einops import rearrange, repeat\n\n\n_position_embeddings = {\n    \"absolute\": VitAbsoluteEncoding\n    # \"sinusoidal\": SinusoidalEncoding,\n}\n\n\n@dataclass\nclass EncoderOutput(object):\n    logits: torch.Tensor\n\n\n@dataclass\nclass MLMOutput(object):\n    hidden_state: torch.Tensor\n    logits: torch.Tensor\n\n\nclass EncoderLayer(nn.Module):\n    \"encoder layer for encoder model\"\n\n    def __init__(\n        self, config, layer_idx: int, attention_type: Optional[str] = None\n    ) -> None:\n        super().__init__()\n        self.attention = VisionAttention(config, layer_idx=layer_idx)\n\n        self.feed_forward = FeedForward(config)\n        self.layer_idx = layer_idx\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            hidden_state: torch.Tensor of shape (batch, seq_len,embd_dim)\n            attention_mask: torch.Tensor of shape (batch,1,1,seqlen)\n            freqs: positionl information to use in RoPE\n        return:\n               hidden_state: torch.Tensor of shape (batch, seq_len, embed_dim) of last layer\n\n        \"\"\"\n        out = self.attention(\n            hidden_state=hidden_state, attention_mask=attention_mask, freqs=freqs\n        )\n        out = self.feed_forward(out, hidden_state)\n        return out\n\n\nclass VisionModel(nn.Module):\n\n    def __init__(\n        self,\n        config,\n        pos_embedding_type: Optional[str] = \"absolute\",\n    ) -> None:\n        super().__init__()\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n        self.num_channels = config.num_channels\n        self.num_patches = (self.image_size[0] // self.patch_size[0]) * (\n            self.image_size[1] // self.patch_size[1]\n        )\n        if _position_embeddings.get(pos_embedding_type, None) is not None:\n            self.position_embeddings = _position_embeddings.get(pos_embedding_type)(\n                config\n            )\n        else:\n            self.position_embeddings = None\n\n        self.all_layer = nn.ModuleList(\n            [\n                EncoderLayer(config, layer_idx)\n                for layer_idx in range(config.num_hidden_layers)\n            ]\n        )\n        self.pixel_seq = nn.Conv2d(\n            in_channels=self.num_channels,\n            out_channels=config.hidden_size,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n        )\n        patch_dim = config.num_channels * self.patch_size[0] * self.patch_size[1]\n        cls_token = nn.Parameter(torch.randn(1, 1, patch_dim))/patch_dim**0.5\n        self.register_parameter(\"cls_token\", nn.Parameter(cls_token))\n\n    def _init_weights(self, module: nn.Module) -> None:\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(\n                module.weight, mean=0.0, std=0.02 / torch.sqrt(2 * len(self.all_layer))\n            )\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(\n                module.weight, mean=0.0, std=0.02 / torch.sqrt(2 * len(self.all_layer))\n            )\n\n    def forward(\n        self, pixel_values: torch.Tensor, attention_mask: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input_ids: torch.LongTensor of shape (batch, seq_len) for encoder`\n            attention_mask: torch.Tensor of shape (batch,seqlen) for encoder\n        return:\n               logits: torch.Tensor of shape (batch, seq_len, embed_dim) of last layer\n\n        \"\"\"\n\n        img_seq = self.pixel_seq(pixel_values)\n        hidden_state = rearrange(img_seq, \"b d c1 c2 -> b (c1 c2) d\")\n        bsz, seqlen, _ = hidden_state.shape\n\n        # rearrange(image, \"b c (h p1) (w p2) -> b (h w) (c p1 p2)\", p1=p1, p2=p2)\n        cls_tokens = repeat(self.cls_token, \"1 1 d -> b 1 d\", b=bsz).to(\n            hidden_state.device\n        )\n        hidden_state = torch.cat((cls_tokens, hidden_state), dim=1)\n\n        freqs = None\n        if self.position_embeddings is not None:\n            pos_info = self.position_embeddings(hidden_state)\n            hidden_state = hidden_state + pos_info #\n        else:\n            freqs = self.emb_freq[:, :seqlen].to(pixel_values.device)\n\n        if attention_mask is None:\n            encoder_batch_size, encoder_sequence_length, _ = hidden_state.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            attention_mask = torch.ones(\n                encoder_hidden_shape, device=pixel_values.device\n            )\n\n        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2).type_as(hidden_state)\n        attention_mask = (1.0 - attention_mask) * torch.finfo(\n            hidden_state.dtype\n        ).min  # invert it to to add directly to attention score\n\n        for layer in self.all_layer:\n            hidden_state = layer(hidden_state, attention_mask, freqs)\n        return EncoderOutput(hidden_state)\n\n    @classmethod\n    def from_config(\n        cls,\n        config,\n        pos_embedding_type: Optional[str] = \"absolute\",\n    ) -> nn.Module:\n        return cls(config, pos_embedding_type)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:43.753556Z","iopub.execute_input":"2025-02-28T07:12:43.753928Z","iopub.status.idle":"2025-02-28T07:12:43.773899Z","shell.execute_reply.started":"2025-02-28T07:12:43.753783Z","shell.execute_reply":"2025-02-28T07:12:43.773000Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"@dataclass\nclass Config:\n    hidden_size: int = 768\n    num_attention_heads: int = 12\n    image_size: Tuple[int] = (224, 224)\n    patch_size: Tuple[int] = (16, 16)\n    num_channels: int = 3\n    num_hidden_layers: int = 8\n    hidden_dropout_prob: float = 0.1\n    initializer_range: float = 0.02\n    intermediate_size: int = 3072\n    layer_norm_eps: float = 1e-05\n    hidden_act: str = \"gelu\"","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:43.774690Z","iopub.execute_input":"2025-02-28T07:12:43.774988Z","iopub.status.idle":"2025-02-28T07:12:43.794100Z","shell.execute_reply.started":"2025-02-28T07:12:43.774942Z","shell.execute_reply":"2025-02-28T07:12:43.793339Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"config = Config()\nvit = VisionModel(config)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:43.797081Z","iopub.execute_input":"2025-02-28T07:12:43.797340Z","iopub.status.idle":"2025-02-28T07:12:44.395939Z","shell.execute_reply.started":"2025-02-28T07:12:43.797318Z","shell.execute_reply":"2025-02-28T07:12:44.394725Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"image = torch.rand((4, 3, 224, 224))\nvit(image).logits.size()","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:44.397657Z","iopub.execute_input":"2025-02-28T07:12:44.398063Z","iopub.status.idle":"2025-02-28T07:12:47.416074Z","shell.execute_reply.started":"2025-02-28T07:12:44.398022Z","shell.execute_reply":"2025-02-28T07:12:47.414899Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 197, 768])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import Dataset, random_split, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as T\n\n\nclass MyDataset(Dataset):\n    def __init__(self, root_dir, df, transform=None):\n        self.transform = transform\n        self.root_dir = root_dir\n        self.image_path = df[\"image_name\"].values\n        self.label = df[\"label\"].values\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        img_path = self.image_path[idx]\n        img = Image.open(self.root_dir + \"/\" + img_path)\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, self.label[idx]\n\n\nimagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:47.417128Z","iopub.execute_input":"2025-02-28T07:12:47.417727Z","iopub.status.idle":"2025-02-28T07:12:48.037186Z","shell.execute_reply.started":"2025-02-28T07:12:47.417685Z","shell.execute_reply":"2025-02-28T07:12:48.036137Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!ls '../input/train-scene classification'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:12:48.038239Z","iopub.execute_input":"2025-02-28T07:12:48.038513Z","iopub.status.idle":"2025-02-28T07:12:48.193679Z","shell.execute_reply.started":"2025-02-28T07:12:48.038489Z","shell.execute_reply":"2025-02-28T07:12:48.192514Z"}},"outputs":[{"name":"stdout","text":"train  train.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"../input/train-scene classification/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:48.194880Z","iopub.execute_input":"2025-02-28T07:12:48.195159Z","iopub.status.idle":"2025-02-28T07:12:48.747273Z","shell.execute_reply.started":"2025-02-28T07:12:48.195133Z","shell.execute_reply":"2025-02-28T07:12:48.746400Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:48.748116Z","iopub.execute_input":"2025-02-28T07:12:48.748361Z","iopub.status.idle":"2025-02-28T07:12:48.773640Z","shell.execute_reply.started":"2025-02-28T07:12:48.748339Z","shell.execute_reply":"2025-02-28T07:12:48.772887Z"},"trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"  image_name  label\n0      0.jpg      0\n1      1.jpg      4\n2      2.jpg      5\n3      4.jpg      0\n4      7.jpg      4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.jpg</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.jpg</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.jpg</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:48.774666Z","iopub.execute_input":"2025-02-28T07:12:48.775022Z","iopub.status.idle":"2025-02-28T07:12:49.332885Z","shell.execute_reply.started":"2025-02-28T07:12:48.774982Z","shell.execute_reply":"2025-02-28T07:12:49.331978Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"imagenet_stats = (\n    [0.485, 0.456, 0.406],\n    [0.229, 0.224, 0.225],\n)  # mean and std values of the Imagenet Dataset so that pretrained models could also be used\n\n# setting a set of transformations to transform the images\ntransform = T.Compose(\n    [\n        T.Resize((224, 224)),\n        T.RandomHorizontalFlip(),\n        T.RandomRotation(2),\n        T.ToTensor(),\n        T.Normalize(*imagenet_stats),\n    ]\n)\ntest_transform = T.Compose(\n    [T.Resize((224, 224)), T.ToTensor(), T.Normalize(*imagenet_stats)]\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:49.333728Z","iopub.execute_input":"2025-02-28T07:12:49.334232Z","iopub.status.idle":"2025-02-28T07:12:49.340142Z","shell.execute_reply.started":"2025-02-28T07:12:49.334202Z","shell.execute_reply":"2025-02-28T07:12:49.339095Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"data_dir = \"../input/train-scene classification/train\"\ntrain_loader = DataLoader(\n    MyDataset(data_dir, train, transform=transform),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n)\nval_loader = DataLoader(\n    MyDataset(data_dir, test, transform=test_transform),\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:49.341119Z","iopub.execute_input":"2025-02-28T07:12:49.341340Z","iopub.status.idle":"2025-02-28T07:12:49.355670Z","shell.execute_reply.started":"2025-02-28T07:12:49.341320Z","shell.execute_reply":"2025-02-28T07:12:49.354759Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df[\"label\"].nunique()","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:49.356545Z","iopub.execute_input":"2025-02-28T07:12:49.356799Z","iopub.status.idle":"2025-02-28T07:12:49.378011Z","shell.execute_reply.started":"2025-02-28T07:12:49.356776Z","shell.execute_reply":"2025-02-28T07:12:49.377130Z"},"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"class VitModel(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.output = nn.Linear(768, 6)\n\n    def forward(self, ids):\n        sequence_output = self.model(ids).logits[:, 0, :]  # taking cls token info\n        logits = self.output(sequence_output)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:49.379028Z","iopub.execute_input":"2025-02-28T07:12:49.379272Z","iopub.status.idle":"2025-02-28T07:12:49.394759Z","shell.execute_reply.started":"2025-02-28T07:12:49.379251Z","shell.execute_reply":"2025-02-28T07:12:49.393923Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def valid_func(model, val_loader, val_bar):\n    model.eval()\n    loss_fn = torch.nn.CrossEntropyLoss()\n    PROB = []\n    TARGETS = []\n    losses = []\n    PREDS = []\n\n    for batch_idx, data in enumerate(val_loader):\n        val_bar.update(1)\n        pixel_values = data[0].cuda()\n        targets = data[1].long().view(-1).cuda()\n        with torch.no_grad():\n            logits = model(pixel_values)\n\n        PREDS += [torch.argmax(logits, 1).detach().cpu()]\n        TARGETS += [targets.detach().cpu()]\n\n        loss = loss_fn(logits, targets)\n        losses.append(loss.item())\n        val_bar.set_description(f'step: {batch_idx+1} loss: {\"%.4f\" % loss}')\n\n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n    accuracy = (PREDS == TARGETS).mean()\n\n    loss_valid = np.mean(losses)\n    return loss_valid, accuracy","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:49.395772Z","iopub.execute_input":"2025-02-28T07:12:49.396114Z","iopub.status.idle":"2025-02-28T07:12:49.416954Z","shell.execute_reply.started":"2025-02-28T07:12:49.396089Z","shell.execute_reply":"2025-02-28T07:12:49.416152Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\n\ndef single_gpu(name):\n    use_amp = True\n    debug = False\n    gc.collect()\n    best_epoch_loss = np.inf\n\n    net = VitModel(vit)\n    net.cuda()\n\n    accelerator = Accelerator(log_with=\"tensorboard\", project_dir=\".\")\n    Config = {\n        \"num_epoch\": EPOCHS,\n        \"learning_rate\": lr,\n        \"loss_function\": str(torch.nn.CrossEntropyLoss),\n    }\n\n    accelerator.init_trackers(f\"{name}_project\", config=Config)\n    accumulation_steps = 2\n    loss_fn = torch.nn.CrossEntropyLoss()\n    no_decay = [\"bias\", \"layernorm.weight\", \"layernorm.bias\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [\n                p\n                for n, p in net.named_parameters()\n                if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.01,\n        },\n        {\n            \"params\": [\n                p for n, p in net.named_parameters() if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n\n    num_train_optimization_steps = int(EPOCHS * len(train_loader) / accumulation_steps)\n\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", patience=1, factor=0.8\n    )\n\n    epoch_check = len(train_loader)\n    total_step = epoch_check * EPOCHS\n    train_bar = tqdm(total=total_step, dynamic_ncols=True)\n    val_bar = tqdm(total=len(val_loader), leave=True, dynamic_ncols=True)\n    t_step = 1\n    for epoch in range(EPOCHS):\n        avg_loss = 0.0\n        net.train()\n        loss_list = []\n        for step, data in enumerate(train_loader):\n            train_bar.update(1)\n            optimizer.zero_grad()\n            pixel_values = data[0].cuda()\n\n            targets = data[1].long().view(-1).cuda()\n\n            pred = net(pixel_values)\n            loss = loss_fn(pred, targets)\n            loss.backward()\n            optimizer.step()\n\n            accelerator.log({\"training_loss_step\": loss}, step=t_step)\n            t_step += 1\n\n            loss_list.append(loss.detach().cpu().item())\n        avg_loss = np.round(np.mean(loss_list), 4)\n        accelerator.log({\"train_epoch\": avg_loss}, step=epoch)\n        print(f'train_epoch-{epoch} loss: {\"%.4f\" % avg_loss}')\n        vloss, vaccuracy = valid_func(net, val_loader, val_bar)\n        accelerator.log({\"vloss_epoch\": loss}, step=epoch)\n        accelerator.log({\"vaccuracy_epoch\": vaccuracy}, step=epoch)\n        print(f'Epoc: {epoch} loss: {\"%.4f\" % vloss},accuracy: {\"%.4f\" % vaccuracy}')\n        val_bar.reset()\n        lr_scheduler.step(vloss)\n    accelerator.end_training()","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:49.417884Z","iopub.execute_input":"2025-02-28T07:12:49.418173Z","iopub.status.idle":"2025-02-28T07:12:49.549104Z","shell.execute_reply.started":"2025-02-28T07:12:49.418150Z","shell.execute_reply":"2025-02-28T07:12:49.548238Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"single_gpu(name=\"vit_classification\")","metadata":{"execution":{"iopub.status.busy":"2025-02-28T07:12:49.549991Z","iopub.execute_input":"2025-02-28T07:12:49.550230Z","iopub.status.idle":"2025-02-28T07:28:27.292273Z","shell.execute_reply.started":"2025-02-28T07:12:49.550209Z","shell.execute_reply":"2025-02-28T07:28:27.291306Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1065 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"159795cd824444febaaaac02a465fcf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2edf1a878ed14a2d9f300f0e314059ef"}},"metadata":{}},{"name":"stdout","text":"train_epoch-0 loss: 1.3337\nEpoc: 0 loss: 0.9611,accuracy: 0.6290\ntrain_epoch-1 loss: 0.8865\nEpoc: 1 loss: 0.7554,accuracy: 0.7147\ntrain_epoch-2 loss: 0.7272\nEpoc: 2 loss: 0.6647,accuracy: 0.7614\ntrain_epoch-3 loss: 0.6357\nEpoc: 3 loss: 0.6593,accuracy: 0.7490\ntrain_epoch-4 loss: 0.5800\nEpoc: 4 loss: 0.5769,accuracy: 0.7875\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**Next Step**\n\n\n*extend it to use 2D Absolute  and 2D Rope embeding*","metadata":{}}]}