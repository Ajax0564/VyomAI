{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10755595,"sourceType":"datasetVersion","datasetId":6670631}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.cache_utils import DynamicCache\nimport torch\nfrom tqdm import tqdm\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ncheckpoint = \"HuggingFaceTB/SmolLM-135M\"\n\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint,torch_dtype=torch.bfloat16).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:17:34.102227Z","iopub.execute_input":"2025-02-21T06:17:34.102603Z","iopub.status.idle":"2025-02-21T06:18:10.940817Z","shell.execute_reply.started":"2025-02-21T06:17:34.102575Z","shell.execute_reply":"2025-02-21T06:18:10.939861Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"853312ae29b540dca057aac688f48563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"399db7b2e3924ae0b666ae3da3c9236a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"668d758175174cd784b7045ea227ca0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a554d7e5e97742b8ba4696d88aecd514"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4c173d94714035ab5aee84d1df2da7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b663cd3de2de4b989479fb881058a06c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c89a5b47ccb54b68944ae45759b05974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"653e5106bf90457a94db4463703cbe3c"}},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"**We will use huggingface model pretrained on large amount of text via next token prediction (Casual language modelling)**\n\n\nfor ref: https://github.com/Ajax0564/VyomAI/blob/main/Examples/vyom-ai-decoder_clm.ipynb\n","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\nwith open('../input/sft_data.json', \"r\", encoding=\"utf-8\") as file:\n        sft_data = json.load(file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:10.942235Z","iopub.execute_input":"2025-02-21T06:18:10.942899Z","iopub.status.idle":"2025-02-21T06:18:10.953994Z","shell.execute_reply.started":"2025-02-21T06:18:10.942872Z","shell.execute_reply":"2025-02-21T06:18:10.953359Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print(\"Example entry:\\n\", sft_data[2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:10.955152Z","iopub.execute_input":"2025-02-21T06:18:10.955364Z","iopub.status.idle":"2025-02-21T06:18:11.200148Z","shell.execute_reply.started":"2025-02-21T06:18:10.955346Z","shell.execute_reply":"2025-02-21T06:18:11.199263Z"}},"outputs":[{"name":"stdout","text":"Example entry:\n {'instruction': 'Describe the structure of an atom.', 'input': '', 'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def format_query(entry):\n    instruction_text = (f\"Write a response that appropriately completes the request.\"f\"\\n### Instruction:\\n{entry['instruction']}\")\n    input_text = f\"\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n    return instruction_text+input_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.201306Z","iopub.execute_input":"2025-02-21T06:18:11.201603Z","iopub.status.idle":"2025-02-21T06:18:11.214662Z","shell.execute_reply.started":"2025-02-21T06:18:11.201581Z","shell.execute_reply":"2025-02-21T06:18:11.214015Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model_input = format_query(sft_data[2])\ndesired_response = f\"\\n### Assistant:\\n{sft_data[2]['output']}\"\n\nprint(model_input + desired_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.215560Z","iopub.execute_input":"2025-02-21T06:18:11.215852Z","iopub.status.idle":"2025-02-21T06:18:11.230322Z","shell.execute_reply.started":"2025-02-21T06:18:11.215824Z","shell.execute_reply":"2025-02-21T06:18:11.229658Z"}},"outputs":[{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nDescribe the structure of an atom.\n### Assistant:\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_portion = int(len(sft_data) * 0.95)  # 95% for training\nval_portion = len(sft_data) - train_portion \n\ntrain_data = sft_data[:train_portion]\nval_data = sft_data[train_portion:]\nprint(\"Training set length:\", len(train_data))\nprint(\"Validation set length:\", len(val_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.231067Z","iopub.execute_input":"2025-02-21T06:18:11.231360Z","iopub.status.idle":"2025-02-21T06:18:11.249683Z","shell.execute_reply.started":"2025-02-21T06:18:11.231333Z","shell.execute_reply":"2025-02-21T06:18:11.248916Z"}},"outputs":[{"name":"stdout","text":"Training set length: 285\nValidation set length: 15\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"we can use padding on every pair inside SFTDataset but for large and small pairs it will be insufficient\n\n\nbest way to use collate func to handle the batch and make each batch dynamic in length based on on max seq present in that batch","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\n\nclass SFTDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n\n        # Store Pre-tokenize texts\n        self.encoded_texts = []\n        for entry in data:\n            instruction_plus_input = format_query(entry)\n            response_text = f\"\\n### Assistant:\\n{entry['output']}\"\n            full_text = instruction_plus_input + response_text\n            self.encoded_texts.append(\n                tokenizer.encode(full_text)\n            )\n\n    def __getitem__(self, index):\n        return self.encoded_texts[index]\n\n    def __len__(self):\n         return len(self.data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.250556Z","iopub.execute_input":"2025-02-21T06:18:11.250797Z","iopub.status.idle":"2025-02-21T06:18:11.264557Z","shell.execute_reply.started":"2025-02-21T06:18:11.250778Z","shell.execute_reply":"2025-02-21T06:18:11.263751Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(tokenizer.encode(\"<|endoftext|>\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.266788Z","iopub.execute_input":"2025-02-21T06:18:11.267051Z","iopub.status.idle":"2025-02-21T06:18:11.284477Z","shell.execute_reply.started":"2025-02-21T06:18:11.267031Z","shell.execute_reply":"2025-02-21T06:18:11.283813Z"}},"outputs":[{"name":"stdout","text":"[0]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"tokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.285988Z","iopub.execute_input":"2025-02-21T06:18:11.286219Z","iopub.status.idle":"2025-02-21T06:18:11.289766Z","shell.execute_reply.started":"2025-02-21T06:18:11.286199Z","shell.execute_reply":"2025-02-21T06:18:11.288974Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def sft_collate(\n    batch,\n    pad_token_id=0,\n    ignore_index=-100,\n    allowed_max_length=None,\n    device=device\n):\n    # Find the longest sequence in the batch\n    batch_max_length = max(len(item)+1 for item in batch)\n\n    # Pad and prepare inputs and targets\n    inputs_lst, targets_lst,mask_ = [], [],[]\n\n    for item in batch:\n        new_item = item.copy()\n        \n        # Pad sequences to max_length\n        padded = (new_item + [pad_token_id] *(batch_max_length - len(item))) #right padding\n        attn_mask = torch.ones(len(padded)) \n        attn_mask[len(item):] = 0\n        inputs = torch.tensor(padded)  \n        targets = torch.tensor(padded)  \n\n        # Replace all but the first padding tokens in targets by ignore_index\n        mask = targets == pad_token_id\n        indices = torch.nonzero(mask).squeeze()\n        if indices.numel() > 1:\n            targets[indices] = ignore_index\n\n        # Optionally truncate to maximum sequence length\n        if allowed_max_length is not None:\n            inputs = inputs[:allowed_max_length]\n            targets = targets[:allowed_max_length]\n            attn_mask = attn_mask[:allowed_max_length]\n            \n\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n        mask_.append(attn_mask)\n\n    # Convert list of inputs and targets to tensors and transfer to target device\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n    attn_mask = torch.stack(mask_).to(device)\n\n    return inputs_tensor,attn_mask, targets_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.290604Z","iopub.execute_input":"2025-02-21T06:18:11.290890Z","iopub.status.idle":"2025-02-21T06:18:11.306212Z","shell.execute_reply.started":"2025-02-21T06:18:11.290860Z","shell.execute_reply":"2025-02-21T06:18:11.305509Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n\nnum_workers = 0\nbatch_size = 4\n\ntorch.manual_seed(123)\n\ntrain_dataset = SFTDataset(train_data, tokenizer)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    collate_fn=sft_collate,\n    shuffle=True,\n    drop_last=True,\n    num_workers=num_workers\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.307010Z","iopub.execute_input":"2025-02-21T06:18:11.307304Z","iopub.status.idle":"2025-02-21T06:18:11.417643Z","shell.execute_reply.started":"2025-02-21T06:18:11.307276Z","shell.execute_reply":"2025-02-21T06:18:11.417003Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"val_dataset = SFTDataset(val_data, tokenizer)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    collate_fn=sft_collate,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.418370Z","iopub.execute_input":"2025-02-21T06:18:11.418606Z","iopub.status.idle":"2025-02-21T06:18:11.428575Z","shell.execute_reply.started":"2025-02-21T06:18:11.418586Z","shell.execute_reply":"2025-02-21T06:18:11.427912Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"for i,(inputs,attn_mask, targets) in enumerate(train_loader):\n    print(inputs.shape,attn_mask.shape, targets.shape)\n    if i==5:\n        break\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.429427Z","iopub.execute_input":"2025-02-21T06:18:11.429724Z","iopub.status.idle":"2025-02-21T06:18:11.472468Z","shell.execute_reply.started":"2025-02-21T06:18:11.429694Z","shell.execute_reply":"2025-02-21T06:18:11.471549Z"}},"outputs":[{"name":"stdout","text":"torch.Size([4, 127]) torch.Size([4, 127]) torch.Size([4, 127])\ntorch.Size([4, 522]) torch.Size([4, 522]) torch.Size([4, 522])\ntorch.Size([4, 292]) torch.Size([4, 292]) torch.Size([4, 292])\ntorch.Size([4, 134]) torch.Size([4, 134]) torch.Size([4, 134])\ntorch.Size([4, 180]) torch.Size([4, 180]) torch.Size([4, 180])\ntorch.Size([4, 339]) torch.Size([4, 339]) torch.Size([4, 339])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def make_query(q):\n    prompt = f\"Write a response that appropriately completes the request.\"f\"\\n### Instruction:\\n{q}\"\n    return prompt+f\"\\n### Assistant:\\n\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.473526Z","iopub.execute_input":"2025-02-21T06:18:11.473847Z","iopub.status.idle":"2025-02-21T06:18:11.477524Z","shell.execute_reply.started":"2025-02-21T06:18:11.473815Z","shell.execute_reply":"2025-02-21T06:18:11.476747Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"**Model Without SFT**","metadata":{}},{"cell_type":"code","source":"messages = \"Describe the structure of an atom\"\ninput_text = make_query(messages)\ninput_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.478274Z","iopub.execute_input":"2025-02-21T06:18:11.478544Z","iopub.status.idle":"2025-02-21T06:18:11.491487Z","shell.execute_reply.started":"2025-02-21T06:18:11.478515Z","shell.execute_reply":"2025-02-21T06:18:11.490645Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'Write a response that appropriately completes the request.\\n### Instruction:\\nDescribe the structure of an atom\\n### Assistant:\\n'"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"messages = [\"Describe the structure of an atom\"]\ninput_text = make_query(messages[0])\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=20,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:11.492359Z","iopub.execute_input":"2025-02-21T06:18:11.492628Z","iopub.status.idle":"2025-02-21T06:18:13.198695Z","shell.execute_reply.started":"2025-02-21T06:18:11.492607Z","shell.execute_reply":"2025-02-21T06:18:13.197910Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nDescribe the structure of an atom\n### Assistant:\n\n### Instructions:\n\n### 1. Read the following information about the atom.\n\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"messages = \"Generate a list of ten items a person might need for a camping trip\"\ninput_text = make_query(messages)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=20,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:13.199567Z","iopub.execute_input":"2025-02-21T06:18:13.199864Z","iopub.status.idle":"2025-02-21T06:18:13.972477Z","shell.execute_reply.started":"2025-02-21T06:18:13.199833Z","shell.execute_reply":"2025-02-21T06:18:13.971525Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nGenerate a list of ten items a person might need for a camping trip\n### Assistant:\n\n### Directions:\n\n### Instructions:\n\n### Directions:\n\n### Instructions:\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def sft_loss_fn(labels, prediction_scores,vocab_size=49152):\n    prediction_scores = prediction_scores[:, :-1, :].contiguous()\n    labels = labels[:, 1:].contiguous()\n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n    lm_loss = loss_fct(\n        prediction_scores.view(-1,vocab_size), labels.view(-1)\n    )\n    return lm_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:13.973304Z","iopub.execute_input":"2025-02-21T06:18:13.973541Z","iopub.status.idle":"2025-02-21T06:18:13.978149Z","shell.execute_reply.started":"2025-02-21T06:18:13.973522Z","shell.execute_reply":"2025-02-21T06:18:13.977299Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ndef sft_train(model=model,train_loader=train_loader):\n    model.train()\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n    Epochs = 3\n    \n    epoch_check = len(train_loader)\n    total_step = epoch_check * Epochs\n    \n    train_bar = tqdm(total=total_step, dynamic_ncols=True)\n    \n    for epoch in range( Epochs):\n        loss_list = []\n        for step, (inputs,attn_mask, targets) in enumerate(train_loader):\n            train_bar.update(1)\n            optimizer.zero_grad()\n            pred = model(input_ids=inputs,attention_mask=attn_mask).logits\n            loss = sft_loss_fn(targets, pred)\n            loss.backward()\n            optimizer.step()\n            loss_list.append(loss.detach().cpu().item())\n            # if step>0 and step%50==0:\n            #      avg_loss = np.round(np.mean(loss_list), 4)\n            #      print(f\"Epoch--{epoch+1}--step--{step} ### Train loss---{avg_loss}\")\n\n        avg_loss = np.round(np.mean(loss_list), 4)\n        print(f\"Epoch--{epoch+1} ### Train loss---{avg_loss}\")\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:13.978918Z","iopub.execute_input":"2025-02-21T06:18:13.979246Z","iopub.status.idle":"2025-02-21T06:18:13.990381Z","shell.execute_reply.started":"2025-02-21T06:18:13.979214Z","shell.execute_reply":"2025-02-21T06:18:13.989445Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"sft_train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:18:13.991242Z","iopub.execute_input":"2025-02-21T06:18:13.991550Z","iopub.status.idle":"2025-02-21T06:20:12.381237Z","shell.execute_reply.started":"2025-02-21T06:18:13.991528Z","shell.execute_reply":"2025-02-21T06:20:12.380325Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/355 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85abc83f58034d6e9505e98f60070fb8"}},"metadata":{}},{"name":"stdout","text":"Epoch--1 ### Train loss---2.0443\nEpoch--2 ### Train loss---1.7663\nEpoch--3 ### Train loss---1.6916\nEpoch--4 ### Train loss---1.6504\nEpoch--5 ### Train loss---1.6256\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"**Model after SFT**","metadata":{}},{"cell_type":"code","source":"messages = \"Describe the structure of an atom\"\ninput_text = make_query(messages)\ninput_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:12.383353Z","iopub.execute_input":"2025-02-21T06:20:12.383605Z","iopub.status.idle":"2025-02-21T06:20:12.388622Z","shell.execute_reply.started":"2025-02-21T06:20:12.383584Z","shell.execute_reply":"2025-02-21T06:20:12.387999Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'Write a response that appropriately completes the request.\\n### Instruction:\\nDescribe the structure of an atom\\n### Assistant:\\n'"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:12.389506Z","iopub.execute_input":"2025-02-21T06:20:12.389757Z","iopub.status.idle":"2025-02-21T06:20:12.410108Z","shell.execute_reply.started":"2025-02-21T06:20:12.389737Z","shell.execute_reply":"2025-02-21T06:20:12.409473Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 576)\n    (layers): ModuleList(\n      (0-29): 30 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((576,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"messages = \"Describe the structure of an atom\"\ninput_text = make_query(messages)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:12.414038Z","iopub.execute_input":"2025-02-21T06:20:12.414278Z","iopub.status.idle":"2025-02-21T06:20:14.296770Z","shell.execute_reply.started":"2025-02-21T06:20:12.414259Z","shell.execute_reply":"2025-02-21T06:20:14.295972Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nDescribe the structure of an atom\n### Assistant:\nAn atom is a subatomic particle that consists of a positively charged nucleus surrounded by a cloud of negatively charged electrons. The nucleus is made up of protons and neutrons, while the electrons orbit the nucleus in a circular path. The outermost shell of an atom\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"messages = \"Give tips for staying healthy\"\ninput_text = make_query(messages)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:14.298360Z","iopub.execute_input":"2025-02-21T06:20:14.298601Z","iopub.status.idle":"2025-02-21T06:20:16.192687Z","shell.execute_reply.started":"2025-02-21T06:20:14.298580Z","shell.execute_reply":"2025-02-21T06:20:16.191936Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nGive tips for staying healthy\n### Assistant:\nEating a balanced diet is important for staying healthy. Eating a variety of fruits and vegetables, whole grains, lean proteins, and healthy fats can help you get the nutrients you need. It's also important to stay hydrated by drinking plenty of water throughout the\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"messages = 'Explain the use of word embeddings in Natural Language Processing'\ninput_text = make_query(messages)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:16.193356Z","iopub.execute_input":"2025-02-21T06:20:16.193561Z","iopub.status.idle":"2025-02-21T06:20:18.049101Z","shell.execute_reply.started":"2025-02-21T06:20:16.193543Z","shell.execute_reply":"2025-02-21T06:20:18.048353Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nExplain the use of word embeddings in Natural Language Processing\n### Assistant:\nWord embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a type of word representation that is used to represent words in a vector space. Word embeddings are used in Natural Language Processing (NLP)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"model.save_pretrained('./')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:18.049796Z","iopub.execute_input":"2025-02-21T06:20:18.050040Z","iopub.status.idle":"2025-02-21T06:20:18.731526Z","shell.execute_reply.started":"2025-02-21T06:20:18.050021Z","shell.execute_reply":"2025-02-21T06:20:18.730815Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"**DPO**","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\nwith open('../input/Neural-DPO.jsonl', \"r\", encoding=\"utf-8\") as file:\n        data = json.load(file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:43.430550Z","iopub.execute_input":"2025-02-21T06:20:43.430879Z","iopub.status.idle":"2025-02-21T06:20:43.456451Z","shell.execute_reply.started":"2025-02-21T06:20:43.430853Z","shell.execute_reply":"2025-02-21T06:20:43.455791Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"data[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:44.635448Z","iopub.execute_input":"2025-02-21T06:20:44.635753Z","iopub.status.idle":"2025-02-21T06:20:44.640935Z","shell.execute_reply.started":"2025-02-21T06:20:44.635731Z","shell.execute_reply":"2025-02-21T06:20:44.640080Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'system': 'You are an A.I assistant that has access to a vast library of information about neural networks',\n 'question': \"What is the significance of the parameter-efficient expert Ai(x) formula presented in the paper 'Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks'?\",\n 'chosen': 'The parameter-efficient expert Ai(x) formula presented in the paper introduces adapters with minimal parameters to the model, enabling differentiation between experts and scaling of model capacity while preserving sparsity.',\n 'rejected': 'The Ai(x) formula increases the complexity of the model by adding a large number of parameters, contrary to what the paper suggests.'}"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"dpo_data = []\nfor entry in data:\n    if entry.get('rejected',None):\n        dpo_data.append(entry)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:46.172829Z","iopub.execute_input":"2025-02-21T06:20:46.173152Z","iopub.status.idle":"2025-02-21T06:20:46.177532Z","shell.execute_reply.started":"2025-02-21T06:20:46.173127Z","shell.execute_reply":"2025-02-21T06:20:46.176663Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"train_portion = int(len(dpo_data) * 0.95)  # 85% for training\n\nval_portion = len(dpo_data) - train_portion  # Remaining \ntrain_data = dpo_data[:train_portion]\nval_data = dpo_data[train_portion:]\nprint(\"Training set length:\", len(train_data))\nprint(\"Validation set length:\", len(val_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:49.102009Z","iopub.execute_input":"2025-02-21T06:20:49.102374Z","iopub.status.idle":"2025-02-21T06:20:49.108128Z","shell.execute_reply.started":"2025-02-21T06:20:49.102345Z","shell.execute_reply":"2025-02-21T06:20:49.107274Z"}},"outputs":[{"name":"stdout","text":"Training set length: 1008\nValidation set length: 54\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"def format_query(entry):\n    instruction_text = (f\"Write a response that appropriately completes the request.\"f\"\\n### Instruction:\\n{entry['question']}\")\n\n    return instruction_text \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:49.980055Z","iopub.execute_input":"2025-02-21T06:20:49.980362Z","iopub.status.idle":"2025-02-21T06:20:49.984159Z","shell.execute_reply.started":"2025-02-21T06:20:49.980340Z","shell.execute_reply":"2025-02-21T06:20:49.983354Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\n\nclass DPODataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n\n        # Pre-tokenize texts\n        self.encoded_texts = []\n        for entry in data:\n            prompt = format_query(entry)\n            rejected_response = entry[\"rejected\"]\n            chosen_response = entry[\"chosen\"]\n\n            prompt_tokens = tokenizer.encode(prompt)\n            \n            chosen_full_text = prompt+f\"\\n### Assistant:\\n{chosen_response}\"\n            rejected_full_text = prompt+f\"\\n### Assistant:\\n{rejected_response}\"\n            chosen_full_tokens = tokenizer.encode(chosen_full_text)\n            rejected_full_tokens = tokenizer.encode(rejected_full_text)\n\n            self.encoded_texts.append({\n                \"prompt\": prompt_tokens,\n                \"chosen\": chosen_full_tokens,\n                \"rejected\": rejected_full_tokens,\n            })\n\n    def __getitem__(self, index):\n        return self.encoded_texts[index]\n\n    def __len__(self):\n        return len(self.data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:52.836741Z","iopub.execute_input":"2025-02-21T06:20:52.837077Z","iopub.status.idle":"2025-02-21T06:20:52.842880Z","shell.execute_reply.started":"2025-02-21T06:20:52.837053Z","shell.execute_reply":"2025-02-21T06:20:52.841911Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def dpo_collate(\n    batch,\n    pad_token_id=0,\n    allowed_max_length=None,\n    mask_prompt_tokens=True,\n    device=device\n):\n    # Initialize lists to hold batch data\n    batch_data = {\n        \"prompt\": [],\n        \"chosen\": [],\n        \"rejected\": [],\n        \"rejected_mask\": [],\n        \"chosen_mask\": []\n\n    }\n\n    # Determine the longest sequence to set a common padding length\n    max_length_common = 0\n    if batch:\n        for key in [\"chosen\", \"rejected\"]:\n            current_max = max(len(item[key])+1 for item in batch)\n            max_length_common = max(max_length_common, current_max)\n    # Process each item in the batch\n    for item in batch:\n        prompt = torch.tensor(item[\"prompt\"])\n        batch_data[\"prompt\"].append(prompt)\n\n        for key in [\"chosen\", \"rejected\"]:\n            # Adjust padding according to the common maximum length\n            sequence = item[key]\n            padded = sequence + [pad_token_id] * (max_length_common - len(sequence)) #right padding\n            mask = torch.ones(len(padded))\n\n            # Set mask for all padding tokens to False\n            mask[len(sequence):] = 0\n\n            # Set mask for all input tokens to False\n            # +1 sets the 1 newline (\"\\n\") tokens before \"### Assistant\" to False\n            if mask_prompt_tokens:\n                mask[:prompt.shape[0]+1] =0\n\n            batch_data[key].append(torch.tensor(padded))\n            batch_data[f\"{key}_mask\"].append(mask)\n\n    # Final processing\n    for key in [\"chosen\", \"rejected\", \"chosen_mask\", \"rejected_mask\"]:\n        # Stack all sequences into a tensor for the given key\n        tensor_stack = torch.stack(batch_data[key])\n\n        # Optionally truncate to maximum sequence length\n        if allowed_max_length is not None:\n            tensor_stack = tensor_stack[:, :allowed_max_length]\n\n        # Move to the specified device\n        batch_data[key] = tensor_stack.to(device)\n\n    return batch_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:20:53.557508Z","iopub.execute_input":"2025-02-21T06:20:53.557833Z","iopub.status.idle":"2025-02-21T06:20:53.564909Z","shell.execute_reply.started":"2025-02-21T06:20:53.557805Z","shell.execute_reply":"2025-02-21T06:20:53.564106Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# from functools import partial\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(\"Device:\", device)\n\n# collate_fn = partial(\n#     my_collate_fn,\n#     device=device,            # Put the data directly on a GPU if available\n#     mask_prompt_tokens=True,  # This is optional\n#     allowed_max_length=768   # The supported context length of the model\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:03.118146Z","iopub.execute_input":"2025-02-21T06:21:03.118438Z","iopub.status.idle":"2025-02-21T06:21:03.122071Z","shell.execute_reply.started":"2025-02-21T06:21:03.118414Z","shell.execute_reply":"2025-02-21T06:21:03.121249Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"num_workers = 0\nbatch_size = 4\n\ntorch.manual_seed(123)\n\ntrain_dataset =DPODataset(train_data, tokenizer)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    collate_fn=dpo_collate,\n    shuffle=True,\n    drop_last=True,\n    num_workers=num_workers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:07.470455Z","iopub.execute_input":"2025-02-21T06:21:07.470785Z","iopub.status.idle":"2025-02-21T06:21:08.040155Z","shell.execute_reply.started":"2025-02-21T06:21:07.470756Z","shell.execute_reply":"2025-02-21T06:21:08.039205Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"val_dataset = DPODataset(val_data, tokenizer)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    collate_fn=dpo_collate,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:09.497309Z","iopub.execute_input":"2025-02-21T06:21:09.497633Z","iopub.status.idle":"2025-02-21T06:21:09.541343Z","shell.execute_reply.started":"2025-02-21T06:21:09.497605Z","shell.execute_reply":"2025-02-21T06:21:09.540520Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"print(\"Train loader:\")\nfor batch in train_loader:\n    print(\n        batch[\"chosen\"].shape,\n        batch[\"chosen_mask\"].shape,\n        batch[\"rejected\"].shape,\n    )\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:10.929818Z","iopub.execute_input":"2025-02-21T06:21:10.930160Z","iopub.status.idle":"2025-02-21T06:21:10.938501Z","shell.execute_reply.started":"2025-02-21T06:21:10.930132Z","shell.execute_reply":"2025-02-21T06:21:10.937539Z"}},"outputs":[{"name":"stdout","text":"Train loader:\ntorch.Size([4, 467]) torch.Size([4, 467]) torch.Size([4, 467])\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"- Now, we are almost ready to get to the DPO part\n- As mentioned at the beginning of this notebook, DPO works with two LLMs: a policy model (the LLM that we want to optimize) and a reference model (the original model that we keep unchanged)\n- Below, we rename the model as policy_model and instantiate a second instance of the model we refer to as the reference_model","metadata":{}},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained('../working',torch_dtype=torch.bfloat16).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:24.943704Z","iopub.execute_input":"2025-02-21T06:21:24.944025Z","iopub.status.idle":"2025-02-21T06:21:25.180458Z","shell.execute_reply.started":"2025-02-21T06:21:24.944000Z","shell.execute_reply":"2025-02-21T06:21:25.179792Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:26.319093Z","iopub.execute_input":"2025-02-21T06:21:26.319402Z","iopub.status.idle":"2025-02-21T06:21:26.327379Z","shell.execute_reply.started":"2025-02-21T06:21:26.319381Z","shell.execute_reply":"2025-02-21T06:21:26.326669Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 576)\n    (layers): ModuleList(\n      (0-29): 30 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((576,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef compute_dpo_loss(\n      model_chosen_logprobs,\n      model_rejected_logprobs,\n      reference_chosen_logprobs,\n      reference_rejected_logprobs,\n      beta=0.1,\n    ):\n    \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n\n    Args:\n        policy_chosen_logprobs: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n        policy_rejected_logprobs: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n        reference_chosen_logprobs: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n        reference_rejected_logprobs: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n        beta: Temperature parameter for the DPO loss; typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.\n        label_smoothing: conservativeness for DPO loss.\n\n    Returns:\n        A tuple of three tensors: (loss, chosen_rewards, rejected_rewards).\n    \"\"\"\n\n    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n    logits = model_logratios - reference_logratios\n\n    # DPO (Eq. 7 of https://arxiv.org/pdf/2305.18290.pdf)\n    losses = -F.logsigmoid(beta * logits)\n\n    # Optional values to track progress during training\n    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n\n    # .mean() to average over the samples in the batch\n    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:29.112259Z","iopub.execute_input":"2025-02-21T06:21:29.112549Z","iopub.status.idle":"2025-02-21T06:21:29.117568Z","shell.execute_reply.started":"2025-02-21T06:21:29.112527Z","shell.execute_reply":"2025-02-21T06:21:29.116737Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def compute_logprobs(logits, labels, selection_mask=None):\n    \"\"\"\n    Compute log probabilities.\n\n    Args:\n      logits: Tensor of shape (batch_size, num_tokens, vocab_size)\n      labels: Tensor of shape (batch_size, num_tokens)\n      selection_mask: Tensor for shape (batch_size, num_tokens)\n\n    Returns:\n      mean_log_prob: Mean log probability excluding padding tokens.\n    \"\"\"\n\n    # Labels are the inputs shifted by one\n    labels = labels[:, 1:].clone()\n\n    # Truncate logits to match the labels num_tokens\n    logits = logits[:, :-1, :]\n\n    log_probs = F.log_softmax(logits, dim=-1)\n\n    # Gather the log probabilities for the actual labels\n    selected_log_probs = torch.gather(\n        input=log_probs,\n        dim=-1,\n        index=labels.unsqueeze(-1)\n    ).squeeze(-1)\n\n   \n    mask = selection_mask[:, 1:].clone()\n\n        # Apply the mask to filter out padding tokens\n    selected_log_probs = selected_log_probs * mask\n\n        # Calculate the average log probability excluding padding tokens\n        # This averages over the tokens, so the shape is (batch_size, num_tokens) #where is value is probability of token\n    avg_log_prob = selected_log_probs.sum(-1) / mask.sum(-1)\n\n    return avg_log_prob\n\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:29.433913Z","iopub.execute_input":"2025-02-21T06:21:29.434262Z","iopub.status.idle":"2025-02-21T06:21:29.439275Z","shell.execute_reply.started":"2025-02-21T06:21:29.434237Z","shell.execute_reply":"2025-02-21T06:21:29.438482Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def compute_dpo_loss_batch(batch, model, ref_model, beta):\n    \"\"\"Compute the DPO loss on an input batch\"\"\"\n\n\n    policy_chosen_log_probas = compute_logprobs(\n        logits=model(input_ids=batch[\"chosen\"]).logits,\n        labels=batch[\"chosen\"],\n        selection_mask=batch[\"chosen_mask\"]\n    )\n    policy_rejected_log_probas = compute_logprobs(\n        logits=model(input_ids=batch[\"rejected\"]).logits,\n        labels=batch[\"rejected\"],\n        selection_mask=batch[\"rejected_mask\"]\n    )\n    \n    with torch.no_grad():#ref model will remain same\n        ref_chosen_log_probas = compute_logprobs(\n            logits=ref_model(input_ids=batch[\"chosen\"]).logits,\n            labels=batch[\"chosen\"],\n            selection_mask=batch[\"chosen_mask\"]\n        )\n        ref_rejected_log_probas = compute_logprobs(\n            logits=ref_model(input_ids=batch[\"rejected\"]).logits,\n            labels=batch[\"rejected\"],\n            selection_mask=batch[\"rejected_mask\"]\n        )\n    loss, chosen_rewards, rejected_rewards = compute_dpo_loss(\n        model_chosen_logprobs=policy_chosen_log_probas,\n        model_rejected_logprobs=policy_rejected_log_probas,\n        reference_chosen_logprobs=ref_chosen_log_probas,\n        reference_rejected_logprobs=ref_rejected_log_probas,\n        beta=beta\n    )\n    return loss, chosen_rewards, rejected_rewards\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:31.850693Z","iopub.execute_input":"2025-02-21T06:21:31.851018Z","iopub.status.idle":"2025-02-21T06:21:31.856569Z","shell.execute_reply.started":"2025-02-21T06:21:31.850991Z","shell.execute_reply":"2025-02-21T06:21:31.855850Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"out = base_model(input_ids=batch[\"chosen\"]).logits\nout.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:32.154036Z","iopub.execute_input":"2025-02-21T06:21:32.154338Z","iopub.status.idle":"2025-02-21T06:21:32.234273Z","shell.execute_reply.started":"2025-02-21T06:21:32.154313Z","shell.execute_reply":"2025-02-21T06:21:32.233595Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 467, 49152])"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"with torch.no_grad():\n    loss = compute_dpo_loss_batch(batch, base_model, model, beta=0.1)\nprint(loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:21:34.845569Z","iopub.execute_input":"2025-02-21T06:21:34.845856Z","iopub.status.idle":"2025-02-21T06:21:35.569629Z","shell.execute_reply.started":"2025-02-21T06:21:34.845835Z","shell.execute_reply":"2025-02-21T06:21:35.568704Z"}},"outputs":[{"name":"stdout","text":"(tensor(0.6931, device='cuda:0'), tensor(0., device='cuda:0'), tensor(0., device='cuda:0'))\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import numpy as np\n\ndef dpo_train(model=base_model,ref_model=model,train_loader=train_loader):\n    model.train()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n    EPOCHS = 1\n    \n    epoch_check = len(train_loader)\n    total_step = epoch_check * EPOCHS\n    train_bar = tqdm(total=total_step, dynamic_ncols=True)\n    for epoch in range( EPOCHS):\n        loss_list = []\n        cr,rr = [],[]\n        for step, batch in enumerate(train_loader):\n            optimizer.zero_grad()\n            train_bar.update(1)\n            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n                batch,\n                model,\n                ref_model,\n                beta=0.1\n            )\n            loss.backward()\n            optimizer.step()\n            loss_list.append(loss.detach().cpu().item())\n            cr.append(chosen_rewards.detach().cpu().item())\n            rr.append(rejected_rewards.detach().cpu().item())\n            # if step>0 and step%50==0:\n            #      avg_loss = np.round(np.mean(loss_list), 4)\n            #      avg_c = np.round(np.mean(cr), 4)\n            #      avg_r = np.round(np.mean(rr), 4)\n            #      print(f\"Epoch--{epoch+1}--step--{step} ### Train loss---{avg_loss}\")\n            #      print(f\"Epoch--{epoch+1}--step--{step} ### chosen---{avg_c} ### rejected---{avg_r}\")\n\n        avg_loss = np.round(np.mean(loss_list), 4)\n        avg_c = np.round(np.mean(cr), 4)\n        avg_r = np.round(np.mean(rr), 4)\n        print(f\"Epoch--{epoch+1} ### Train loss---{avg_loss}\")\n        print(f\"Epoch--{epoch+1}--step--{step} ### chosen---{avg_c} ### rejected---{avg_r}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:22:51.098345Z","iopub.execute_input":"2025-02-21T06:22:51.098657Z","iopub.status.idle":"2025-02-21T06:22:51.106694Z","shell.execute_reply.started":"2025-02-21T06:22:51.098634Z","shell.execute_reply":"2025-02-21T06:22:51.105686Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"dpo_train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:22:53.700909Z","iopub.execute_input":"2025-02-21T06:22:53.701236Z","iopub.status.idle":"2025-02-21T06:24:37.727525Z","shell.execute_reply.started":"2025-02-21T06:22:53.701213Z","shell.execute_reply":"2025-02-21T06:24:37.726739Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/252 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f287b884cf694854b82d42943a4d3d85"}},"metadata":{}},{"name":"stdout","text":"Epoch--1--step--50 ### Train loss---0.693\nEpoch--1--step--50 ### chosen----0.0026 ### rejected----0.0062\nEpoch--1--step--100 ### Train loss---0.6927\nEpoch--1--step--100 ### chosen----0.0053 ### rejected----0.0133\nEpoch--1--step--150 ### Train loss---0.6925\nEpoch--1--step--150 ### chosen----0.0116 ### rejected----0.0253\nEpoch--1--step--200 ### Train loss---0.6921\nEpoch--1--step--200 ### chosen----0.0188 ### rejected----0.0393\nEpoch--1--step--250 ### Train loss---0.6916\nEpoch--1--step--250 ### chosen----0.0288 ### rejected----0.059\nEpoch--1 ### Train loss---0.6916\nEpoch--1--step--251 ### chosen----0.0291 ### rejected----0.0594\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"base_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:24:44.439418Z","iopub.execute_input":"2025-02-21T06:24:44.439739Z","iopub.status.idle":"2025-02-21T06:24:44.448438Z","shell.execute_reply.started":"2025-02-21T06:24:44.439716Z","shell.execute_reply":"2025-02-21T06:24:44.447566Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 576)\n    (layers): ModuleList(\n      (0-29): 30 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((576,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":52},{"cell_type":"markdown","source":"**base model is out finetunned model after DPO\\\nwhere model is previous SFT model**","metadata":{}},{"cell_type":"code","source":"messages = \"Give tips for staying healthy\"\ninput_text = make_query(messages)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:24:47.614574Z","iopub.execute_input":"2025-02-21T06:24:47.614883Z","iopub.status.idle":"2025-02-21T06:24:49.533823Z","shell.execute_reply.started":"2025-02-21T06:24:47.614855Z","shell.execute_reply":"2025-02-21T06:24:49.533047Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nGive tips for staying healthy\n### Assistant:\n\nStay hydrated and eat a balanced diet.\n### Assistant:\n\nDrink plenty of water throughout the day.\n### Assistant:\n\nEat a balanced diet with plenty of fruits, vegetables, whole grains, lean proteins, and healthy fats\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"messages = \"Give tips for staying healthy\"\ninput_text = make_query(messages)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = base_model.generate(inputs, max_new_tokens=50,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:26:15.682650Z","iopub.execute_input":"2025-02-21T06:26:15.683019Z","iopub.status.idle":"2025-02-21T06:26:17.586098Z","shell.execute_reply.started":"2025-02-21T06:26:15.682988Z","shell.execute_reply":"2025-02-21T06:26:17.585157Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nGive tips for staying healthy\n### Assistant:\n\n* Eat a balanced diet rich in fruits, vegetables, lean proteins, and whole grains.\n* Get regular exercise, such as walking, swimming, or playing sports.\n* Practice stress management techniques like meditation, deep breathing, or yoga.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"messages = \"Describe the structure of an atom\"\ninput_text = make_query(messages)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:26:28.395731Z","iopub.execute_input":"2025-02-21T06:26:28.396092Z","iopub.status.idle":"2025-02-21T06:26:30.314033Z","shell.execute_reply.started":"2025-02-21T06:26:28.396063Z","shell.execute_reply":"2025-02-21T06:26:30.313215Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nDescribe the structure of an atom\n### Assistant:\nAn atom is a subatomic particle that consists of a nucleus and electrons. The nucleus contains protons and neutrons, while the electrons orbit the nucleus. The nucleus is made up of protons and neutrons, while the electrons orbit the nucleus. The nucleus is made\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"messages = \"Describe the structure of an atom\"\ninput_text = make_query(messages)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = base_model.generate(inputs, max_new_tokens=50,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:26:30.315285Z","iopub.execute_input":"2025-02-21T06:26:30.315599Z","iopub.status.idle":"2025-02-21T06:26:32.207491Z","shell.execute_reply.started":"2025-02-21T06:26:30.315573Z","shell.execute_reply":"2025-02-21T06:26:32.206574Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nDescribe the structure of an atom\n### Assistant:\nAn atom is a fundamental unit of matter. It consists of a nucleus surrounded by electrons, which are negatively charged particles. The nucleus is the center of an atom, and it contains protons and neutrons. Electrons are negatively charged particles, and they orbit\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"messages = \"Explain the use of word embeddings in Natural Language Processing\"\ninput_text = make_query(messages)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:26:36.195833Z","iopub.execute_input":"2025-02-21T06:26:36.196178Z","iopub.status.idle":"2025-02-21T06:26:38.084892Z","shell.execute_reply.started":"2025-02-21T06:26:36.196150Z","shell.execute_reply":"2025-02-21T06:26:38.084005Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nExplain the use of word embeddings in Natural Language Processing\n### Assistant:\nWord embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a type of word representation that is a directed acyclic graph (DAG) of words. Word embeddings are used in Natural Language Processing\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"messages = \"Explain the use of word embeddings in Natural Language Processing\"\ninput_text = make_query(messages)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = base_model.generate(inputs, max_new_tokens=50,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T06:26:38.086102Z","iopub.execute_input":"2025-02-21T06:26:38.086431Z","iopub.status.idle":"2025-02-21T06:26:39.998026Z","shell.execute_reply.started":"2025-02-21T06:26:38.086399Z","shell.execute_reply":"2025-02-21T06:26:39.997093Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write a response that appropriately completes the request.\n### Instruction:\nExplain the use of word embeddings in Natural Language Processing\n### Assistant:\nWord embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a type of word representation that is used in Natural Language Processing (NLP) to represent words in a way that is easier to understand and\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}