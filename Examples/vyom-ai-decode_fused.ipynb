{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1190,"sourceType":"datasetVersion","datasetId":623},{"sourceId":5699740,"sourceType":"datasetVersion","datasetId":3277362},{"sourceId":9605963,"sourceType":"datasetVersion","datasetId":5860854},{"sourceId":129796172,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport sys\nimport torch\nfrom torch.utils.data import Dataset,DataLoader\nimport pickle\nimport os\nimport time \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport warnings\nimport gc\nfrom accelerate import Accelerator\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nwarnings.simplefilter('ignore')\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM,AutoConfig\nfrom tqdm import tqdm\nfrom torch.amp import custom_fwd, custom_bwd\n","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:54:47.073780Z","iopub.execute_input":"2025-07-05T08:54:47.074569Z","iopub.status.idle":"2025-07-05T08:54:57.737636Z","shell.execute_reply.started":"2025-07-05T08:54:47.074541Z","shell.execute_reply":"2025-07-05T08:54:57.736834Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":" !pip install -q -U triton --no-index --find-links ../input/triton-3-0-0/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:54:57.738750Z","iopub.execute_input":"2025-07-05T08:54:57.739131Z","iopub.status.idle":"2025-07-05T08:55:01.357304Z","shell.execute_reply.started":"2025-07-05T08:54:57.739112Z","shell.execute_reply":"2025-07-05T08:55:01.356311Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"@torch.compile(fullgraph=True)\ndef rms_forward(x, weight, eps):\n    # Calculate the RMS\n    rms = x.float().pow(2).mean(-1, keepdim=True).add(eps).sqrt()\n    # Normalize\n    x_norm = x / rms\n    # Apply the gain (weight)\n    output = weight * x_norm.type_as(weight)\n    return output\n\n@torch.compile(fullgraph=True)\ndef rms_backward(grad_output, x, weight,eps):\n    \n    rms = x.float().pow(2).mean(-1, keepdim=True).add(eps).sqrt()\n    # Normalize\n    x_norm = x / rms\n    # Gradients calculations\n    grad_weight = torch.sum(grad_output * x_norm, dim=(0))\n\n    # Compute grad_x: we need to backpropagate through normalization and RMS\n    grad_x_norm = grad_output * weight  # Gradients w.r.t normalized input\n    grad_rms = (grad_x_norm * (-x_norm)).sum(\n        -1, keepdim=True\n    ) / rms  # Gradients w.r.t RMS\n\n    # Gradient w.r.t input x: the gradient of x involves the gradient of the normalization (grad_x_norm)\n    grad_x = grad_x_norm / rms + grad_rms * x_norm / x_norm.size(\n        -1\n    )  # RMS backpropagation\n\n    return grad_x, grad_weight\n    \nclass RMSNormFn(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, x, weight, eps):\n        output = rms_forward(x, weight, eps)\n        ctx.save_for_backward(x, weight)\n        ctx.eps = eps\n        return output\n        \n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_output):\n        x, weight = ctx.saved_tensors\n        eps = ctx.eps\n        grad_x, grad_weight = rms_backward(grad_output,x, weight,eps)\n\n        return grad_x, grad_weight, None\n\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim, eps=1e-6):\n        super().__init__()\n        self.weight = torch.nn.Parameter(torch.ones(dim))\n        self.eps = eps\n\n    def forward(self, x):\n        return RMSNormFn.apply(x, self.weight, self.eps)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:01.358401Z","iopub.execute_input":"2025-07-05T08:55:01.358710Z","iopub.status.idle":"2025-07-05T08:55:01.431908Z","shell.execute_reply.started":"2025-07-05T08:55:01.358678Z","shell.execute_reply":"2025-07-05T08:55:01.431279Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"@torch.compile(fullgraph=True)\ndef gelu_bwd(x):\n    cdf = 0.5 * (1.0 + torch.erf(x / 2**0.5))\n    pdf = torch.exp(-0.5 * x**2) / (2 * 3.14159265359) ** 0.5\n    return (cdf + x * pdf)\n\n@torch.compile(fullgraph=True)\ndef gelu_fwd(x):\n    cdf = 0.5 * (1.0 + torch.erf(x / 2**0.5))\n    return x * cdf\n    \n@torch.compile(fullgraph=True)\ndef ffn_gelu_fwd(x,weight1, bias1, weight2, bias2=None):\n\n    output1 = x.mm(weight1.t())\n\n    if bias1 is not None:\n        output1 += bias1.unsqueeze(0).expand_as(output1)\n\n    gelu_output = gelu_fwd(output1)  # output.clamp(min=0)  # ReLU activation\n    output2 = gelu_output.mm(weight2.t())\n\n    if bias2 is not None:\n        output2 += bias2.unsqueeze(0).expand_as(output2)\n\n    return output2,output1\n    \n@torch.compile(fullgraph=True)\ndef ffn_gelu_bwd(grad_output,x,weight1, bias1, weight2, bias2,gelu_input):\n    grad_bias1 = grad_bias2 = None\n\n    grad_gelu = gelu_bwd(gelu_input)\n\n    grad_x2 = grad_output.mm(weight2) * grad_gelu\n    grad_weight2 = grad_output.T.mm(gelu_input)\n\n    if bias2 is not None:\n        grad_bias2 = grad_output.sum(0)\n\n    grad_x1 = grad_x2.mm(weight1)\n    grad_weight1 = grad_x2.T.mm(x)\n\n    if bias1 is not None:\n        grad_bias1 = grad_x2.sum(0)\n\n    return grad_x1, grad_weight1, grad_bias1, grad_weight2, grad_bias2\n\nclass GELUFunction(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return gelu_fwd(x)\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors\n        return grad_output *  gelu_bwd(x)\n\n\nclass GELU(torch.nn.Module):\n    def forward(self, x):\n        return GELUFunction.apply(x)\n        \nclass FFNGeLU(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, x, weight1, bias1, weight2, bias2=None):\n        output2,output1 = ffn_gelu_fwd(x,weight1, bias1, weight2, bias2)\n        ctx.save_for_backward(x, weight1, bias1, weight2, bias2, output1)\n        return output2\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_output):\n        x, weight1, bias1, weight2, bias2, gelu_input = ctx.saved_tensors\n        grad_x1, grad_weight1, grad_bias1, grad_weight2, grad_bias2 = ffn_gelu_bwd(grad_output,x,weight1, bias1, weight2, bias2,gelu_input)\n\n        return grad_x1, grad_weight1, grad_bias1, grad_weight2, grad_bias2\n\n\nclass FFNGeluModule(torch.nn.Module):\n    def __init__(self, in_features, mid_feature, out_features):\n        super(FFNGeluModule, self).__init__()\n        self.weight1 = torch.nn.Parameter(\n            torch.Tensor(mid_feature, in_features)\n        )\n        self.bias1 = torch.nn.Parameter(torch.Tensor(mid_feature))\n\n        self.weight2 = torch.nn.Parameter(\n            torch.Tensor(out_features, mid_feature)\n        )\n        self.bias2 = torch.nn.Parameter(torch.Tensor(out_features))\n\n        torch.nn.init.xavier_uniform_(self.weight1)\n        torch.nn.init.zeros_(self.bias1)\n        torch.nn.init.xavier_uniform_(self.weight2)\n        torch.nn.init.zeros_(self.bias2)\n\n    def forward(self, input):\n        return FFNGeLU.apply(input, self.weight1, self.bias1, self.weight2, self.bias2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:01.433828Z","iopub.execute_input":"2025-07-05T08:55:01.434043Z","iopub.status.idle":"2025-07-05T08:55:01.449286Z","shell.execute_reply.started":"2025-07-05T08:55:01.434026Z","shell.execute_reply":"2025-07-05T08:55:01.448509Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"@torch.compile(fullgraph=True)\ndef rotate_half(x):\n    x1, x2 = x.chunk(2, dim=-1)\n    return torch.cat((-x2, x1), dim=-1)\n\n@torch.compile(fullgraph=True)\ndef rope_fwd(q, k, freqs):\n        # q, k: (batch_size, num_heads, seq_len, head_dim)\n        # cos, sin: (seq_len, head_dim) or (1, seq_len, head_dim) or (batch_size, num_heads, seq_len, head_dim)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        cos = emb.cos()\n        sin = emb.sin()\n        cos = cos.unsqueeze(1)\n        sin = sin.unsqueeze(1)\n        # forward Method\n        #qleft,qright = q1,q2\n        #qleft out = q1*cos-q2*sin\n        #qright out = q2*cos+q1*sin\n        #final out1 = concat(qleft out, qright out)\n        #final out2 = concat(q1,q2)*cos+concat(-q2,q1)*sin\n        #final out1 and final out2 both are same\n        q_rotated = (q * cos) + (rotate_half(q) * sin)\n        k_rotated = (k * cos) + (rotate_half(k) * sin)\n\n        return q_rotated, k_rotated, cos, sin\n\n@torch.compile(fullgraph=True)\ndef rope_bwd(grad_q_rotated, grad_k_rotated, cos, sin):\n        # backward Method\n        #qleft,qright = q1,q2\n        # y=(x1*cos+ x2*cos)+(−x2*sin + x1*sin)\n        # y = x1*(cos+sin)+x2(cos-sin)\n        # dy/dx1 = cos+sin\n        # dy/dx1 = cos-sin\n        #do/dq qleft out = out_grad*(cos+sin)\n        #do/dq right out = out_grad*(cos-sin)\n        #do/dq final out1 = concat(do/dq qleft, do/dq right out)\n        #do/dq final out2 = concat(q1,q2)*cos-concat(-q2,q1)*sin\n        #final out1 and final out2 both are same\n\n        grad_q = (grad_q_rotated * cos) - (\n            rotate_half(grad_q_rotated) * sin\n        )\n        grad_k = (grad_k_rotated * cos) - (\n           rotate_half(grad_k_rotated) * sin\n        )\n\n        #if freq is a parameter  we will need to calculate its grad as well\n\n        # grad_cos_q = torch.sum(grad_q_rotated * q, dim=(0, 1, 2), keepdim=True)\n        # grad_sin_q = torch.sum(\n        #     grad_q_rotated * RotaryEmbeddingFunction.rotate_half(q),\n        #     dim=(0, 1, 2),\n        #     keepdim=True,\n        # )\n\n        # grad_cos_k = torch.sum(grad_k_rotated * k, dim=(0, 1, 2), keepdim=True)\n        # grad_sin_k = torch.sum(\n        #     grad_k_rotated * RotaryEmbeddingFunction.rotate_half(k),\n        #     dim=(0, 1, 2),\n        #     keepdim=True,\n        # )\n\n        # grad_cos = grad_cos_q + grad_cos_k\n        # grad_sin = grad_sin_q + grad_sin_k\n\n        return grad_q, grad_k\n    \nclass RotaryEmbeddingFunction(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda',cast_inputs=torch.float32)\n    def forward(ctx, q, k, freqs):\n        q_rotated, k_rotated, cos, sin = rope_fwd(q, k, freqs)\n\n        ctx.save_for_backward(cos, sin)\n        return q_rotated, k_rotated\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad_q_rotated, grad_k_rotated):\n        cos, sin = ctx.saved_tensors\n        grad_q, grad_k = rope_bwd(grad_q_rotated, grad_k_rotated, cos, sin)\n\n        return grad_q, grad_k, None\n\n\nclass RotaryEmbedding(nn.Module):\n    \"\"\"\n    RotaryEmbedding is a PyTorch module that implements rotary positional embeddings for attention mechanisms.\n    Args:\n        config (object): Configuration object containing the following attributes:\n            hidden_size (int): The hidden size of the model.\n            num_attention_heads (int): The number of attention heads.\n    Attributes:\n        inv_freq (torch.Tensor): A tensor containing the inverse frequencies for the rotary embeddings.\n    Methods:\n        forward(seq_len):\n            Computes the rotary positional embeddings for a given sequence length.\n            Args:\n                seq_len (int): The length of the input sequence.\n            Returns:\n                torch.Tensor: A tensor containing the rotary positional embeddings with shape (1, seq_len, dim).\n    \"\"\"\n\n    def __init__(self,config):\n        super().__init__()\n        dim = int(config.hidden_size // config.num_attention_heads)\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, seq_len):\n        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq)\n        freqs = torch.einsum(\"i, j -> i j\", t, self.inv_freq)\n\n        return freqs[None, :, :]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:01.450189Z","iopub.execute_input":"2025-07-05T08:55:01.450508Z","iopub.status.idle":"2025-07-05T08:55:01.468536Z","shell.execute_reply.started":"2025-07-05T08:55:01.450466Z","shell.execute_reply":"2025-07-05T08:55:01.467755Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from einops import rearrange, reduce\nfrom typing import Optional, Tuple,Union,List\nfrom dataclasses import dataclass","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:01.469458Z","iopub.execute_input":"2025-07-05T08:55:01.469690Z","iopub.status.idle":"2025-07-05T08:55:01.505476Z","shell.execute_reply.started":"2025-07-05T08:55:01.469672Z","shell.execute_reply":"2025-07-05T08:55:01.504702Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class LinearRms(torch.autograd.Function):\n    @staticmethod\n    # @custom_fwd\n    def forward(ctx, x, prev, weight, bias=None, weight_rms=None, eps=None):\n        output = x.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        ctx.save_for_backward(x, weight, bias, output + prev)\n        output = rms_forward(output + prev, weight_rms, eps)\n        ctx.save_for_backward(\n            x, weight, bias, output + prev, weight_rms\n        )\n        ctx.eps = eps\n        return output\n\n    @staticmethod\n    # @custom_bwd\n    def backward(ctx, grad_output):\n        x, weight, bias, rms_x, weight_rms = ctx.saved_tensors\n        eps = ctx.eps\n        grad_weight = grad_bias = None\n\n        grad_rms, grad_weight_rms = rms_backward(\n            grad_output, rms_x, weight_rms,eps\n        )  \n\n        grad_input = grad_rms.mm(weight)\n        grad_prev = grad_rms\n\n        grad_weight = grad_rms.t().mm(x)\n        grad_bias = None\n        if bias is not None:\n            grad_bias = grad_rms.sum(0)\n\n        return grad_input, grad_prev, grad_weight, grad_bias, grad_weight_rms, None\n\nclass LinearRMSFused(torch.nn.Module):\n\n    def __init__(self, in_features, out_features, eps=1e-6):\n        super(LinearRMSFused, self).__init__()\n        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n        torch.nn.init.xavier_uniform_(self.weight)\n        torch.nn.init.zeros_(self.bias)\n        self.weight_rms = torch.nn.Parameter(torch.ones(out_features))\n        self.eps = eps\n\n    def forward(self, input, prev):\n        return LinearRms.apply(\n            input, prev, self.weight, self.bias, self.weight_rms, self.eps\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:01.506306Z","iopub.execute_input":"2025-07-05T08:55:01.506524Z","iopub.status.idle":"2025-07-05T08:55:01.515032Z","shell.execute_reply.started":"2025-07-05T08:55:01.506508Z","shell.execute_reply":"2025-07-05T08:55:01.514263Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"@torch.compile(fullgraph=True)\ndef attention_forward(Q, K, V, mask=None):\n    d_k = Q.shape[-1]\n    scale = 1.0 / torch.sqrt(torch.tensor(d_k, dtype=Q.dtype, device=Q.device))  # avoid NumPy\n    EPSILON = 1e-10\n\n    Q_scaled = Q * scale\n    S = torch.einsum(\"... i d, ... j d -> ... i j\", Q_scaled, K)\n\n    if mask is not None:\n        S = S + mask\n\n    softmax = F.softmax(S, dim=-1)\n    P_V = torch.einsum(\"... i j, ... j d -> ... i d\", softmax, V)\n\n    return P_V, softmax\n    \n@torch.compile(fullgraph=True)\ndef attention_backward(Q, K, V, O, dO, softmax):\n    scale = 1.0 / torch.sqrt(torch.tensor(Q.shape[-1], dtype=Q.dtype, device=Q.device))  # avoid NumPy\n\n\n    P = softmax  # (1 / l) * torch.exp(S - m)\n   \n\n    dV = torch.einsum(\"... r c, ... r d -> ... c d\", P, dO)\n    dP = torch.einsum(\"... r d, ... c d -> ... r c\", dO, V)\n\n    D = torch.sum(dO * O, dim=-1, keepdim=True)\n    dS = P * (dP - D)\n\n    dQ = scale * torch.einsum(\"... r c, ... c d -> ... r d\", dS, K)\n    dK = scale * torch.einsum(\"... r c, ... r d -> ... c d\", dS, Q)\n    return dQ, dK, dV\n\n\nclass ScaledDotProductAttention(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, q, k, v, mask=None):\n        out, attn_weights = attention_forward(q,k,v, mask)\n        ctx.save_for_backward(q, k, v, mask,out, attn_weights)\n        return out\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, do):\n        Q, K, V, mask, O, softmax = ctx.saved_tensors\n        dq, dk, dv = attention_backward(Q, K, V,O, do, softmax)\n        return dq, dk, dv, None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:01.515842Z","iopub.execute_input":"2025-07-05T08:55:01.516088Z","iopub.status.idle":"2025-07-05T08:55:01.530754Z","shell.execute_reply.started":"2025-07-05T08:55:01.516068Z","shell.execute_reply":"2025-07-05T08:55:01.529903Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\nclass DecoderAttention(nn.Module):\n    def __init__(self, config, layer_idx: int) -> None:\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.head_size = int(config.hidden_size // config.num_attention_heads)\n        self.attention_bias = getattr(config, \"attention_bias\", True)\n        self.layer_idx = layer_idx\n        # self.qkv = nn.Linear(config.hidden_size,3*config.hidden_size)\n        self.query = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.key = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.value = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.out = LinearRMSFused(config.hidden_size,config.hidden_size)\n        self.num_attention_heads = config.num_attention_heads\n        self.apply_rotary_pos_emb =  RotaryEmbeddingFunction.apply\n        self.dot_attn = ScaledDotProductAttention.apply\n        \n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = False,\n        kv_cache: List[torch.FloatTensor] = None,\n        start_pos: Optional[int] = 0,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            hidden_states: torch.Tensor of shape (batch, seq_len, embed_dim)`\n            Attention_mask: torch.Tensor of shape (batch,1, seq_len, seqlen)`\n            freqs: Positional freqs in case of RoPE embedding\n            use_cace: Optional to use kvCache\n            start_pos: in case of kvCache to get store kv-cache at start_pos\n        return:\n               hidden_states: torch.Tensor of shape (batch, seq_len, embed_dim)\n\n        \"\"\"\n        q = self.query(hidden_state)\n        k = self.key(hidden_state)\n        v = self.value(hidden_state)\n        # transform it into batch_size x no_of_heads x seqlen x head_dim for Multihead Attention\n        q = rearrange(q, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        k = rearrange(k, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        v = rearrange(v, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n\n        if freqs is not None:\n            q, k = self.apply_rotary_pos_emb(q, k, freqs)  # apply RoPE if freqs is available\n\n        if use_cache:\n            if kv_cache is None:\n                raise ValueError(\"you need to pass kv_cache\")\n            k, v = kv_cache.update(self.layer_idx, k, v, start_pos)\n\n        # out = torch.nn.functional.scaled_dot_product_attention(\n        #     query=q, key=k, value=v, attn_mask=attention_mask\n        # )\n        out = self.dot_attn(q,k,v,attention_mask)\n        # transform it back into batch_size x seqlen x hidden_dim\n        out = rearrange(out, \"b h l d -> b l (h d)\")\n        b,l,d = out.size()\n        \n        out = out.view(-1,d).contiguous()\n        hidden_state = hidden_state.view(-1,d).contiguous()\n        \n        out = self.out(out,hidden_state)\n        \n        return out.view(b,l,d).contiguous(), kv_cache","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:01.531585Z","iopub.execute_input":"2025-07-05T08:55:01.531842Z","iopub.status.idle":"2025-07-05T08:55:01.542141Z","shell.execute_reply.started":"2025-07-05T08:55:01.531819Z","shell.execute_reply":"2025-07-05T08:55:01.541381Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# mainly 2way to do one keep it into the model init like llama https://github.com/meta-llama/llama/blob/main/llama/model.py\n# every attention layer have its own kv-cache storage\n# or keep all attention layer kv-cache into single storage like Huggingface Transformer\n\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Generator, List, Optional, Tuple\nimport torch\n\n\nclass DynamicCache:\n    \"\"\"\n    A cache that grows dynamically as more tokens are generated.\n\n    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n    `[batch_size, num_heads, seq_len, head_dim]`.\n    \"\"\"\n\n    def __init__(self, config, is_gqa: bool = False) -> None:\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n        self._seen_tokens = False\n\n        self.layers = config.num_hidden_layers\n        for _ in range(self.layers):\n            self.key_cache.append([])\n            self.value_cache.append([])\n\n    def __len__(self) -> int:\n        \"\"\"\n        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n        to the number of layers in the model.\n        \"\"\"\n        if len(self.key_cache) == 0:\n            return 0\n        return self.key_cache[0].shape[-2]\n\n    def update(\n        self,\n        index: int,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        start_pos: int = 0,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n\n        # Update the cache first iteration'\n\n        if len(self.key_cache[index]) == 0:\n            self._seen_tokens = True\n            self.key_cache[index] = key_states.clone()\n            self.value_cache[index] = value_states.clone()\n        else:\n            self.key_cache[index] = torch.cat(\n                [self.key_cache[index], key_states], dim=-2\n            )\n            self.value_cache[index] = torch.cat(\n                [self.value_cache[index], value_states], dim=-2\n            )\n\n        return self.key_cache[index], self.value_cache[index]\n\n    def get(self, index: int) -> Tuple[torch.Tensor]:\n        if self._seen_tokens:\n            return self.key_cache[index], self.value_cache[index]\n        else:\n            raise ValueError(\"there is no token available in kv-cache\")\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        if self.key_cache is None:\n            return 0\n        return self.key_cache[layer_idx].shape[-2]\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n        return self.max_cache_len\n\n\nclass StaticCache:\n    \"\"\"\n    A cache that grows dynamically as more tokens are generated.\n\n    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n    `[batch_size, num_heads, seq_len, head_dim]`.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        max_cache_len: int = None,\n        dtype: torch.dtype = torch.float32,\n        batch_size: int = 1,\n        is_gqa: bool = False,\n    ) -> None:\n        self.head_size = int(config.hidden_size // config.num_attention_heads)\n        self.heads = None\n        self.batch_size = batch_size\n        # if is_gqa:\n        self.heads = getattr(config, \"num_key_value_heads\", None)\n        # if self.heads is None:\n        #     raise ValueError(\n        #         \"you are using is_gqa=True and config.num_key_value_heads is not available\"\n        #     )\n        if self.heads is None:\n\n            self.heads = config.num_attention_heads\n\n        self.max_cache_len = (\n            config.max_position_embeddings if max_cache_len is None else max_cache_len\n        )\n\n        self.dtype = dtype\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n\n        self.cache_shape = (\n            self.batch_size,\n            self.heads,\n            self.max_cache_len,\n            self.head_size,\n        )\n\n        self._seen_tokens = False\n        self.layers = config.num_hidden_layers\n        for _ in range(self.layers):\n            blank_key_cache = torch.zeros(\n                self.cache_shape, dtype=self.dtype, device=self.device\n            )\n            blank_value_cache = torch.zeros(\n                self.cache_shape, dtype=self.dtype, device=self.device\n            )\n            self.key_cache.append(blank_key_cache)\n            self.value_cache.append(blank_value_cache)\n\n    def __len__(self) -> int:\n        if self.key_cache is None:\n            return 0\n        \"\"\"\n        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n        to the number of layers in the model.\n        \"\"\"\n        return self.key_cache.shape[-2]\n\n    def update(\n        self,\n        index: int,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        start_pos: int = 0,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n\n        # Update the cache first iteration'\n\n        bsz, head, seqlen, _ = key_states.shape\n        if seqlen > self.key_cache[index].size()[2]:\n            raise ValueError(\n                f\"{k.shape} is more than init k_cache size {self.key_cache}\"\n            )\n\n        self.key_cache[index][:bsz, :, start_pos : start_pos + seqlen] = key_states\n        self.value_cache[index][:bsz, :, start_pos : start_pos + seqlen] = value_states\n\n        k = self.key_cache[index][:bsz, :, : start_pos + seqlen]\n        v = self.value_cache[index][:bsz, :, : start_pos + seqlen]\n\n        return k, v\n\n    def get(self, index: int) -> Tuple[torch.Tensor]:\n        if self._seen_tokens:\n            return self.key_cache[index], self.value_cache[index]\n        else:\n            raise ValueError(\"there is no token available in kv-cache\")\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        if self.key_cache is None:\n            return 0\n        return self.key_cache[layer_idx].shape[-2]\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n        return None\n","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:01.545075Z","iopub.execute_input":"2025-07-05T08:55:01.545323Z","iopub.status.idle":"2025-07-05T08:55:01.796964Z","shell.execute_reply.started":"2025-07-05T08:55:01.545306Z","shell.execute_reply":"2025-07-05T08:55:01.796101Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"@dataclass\nclass DecoderOutput(object):\n    logits: torch.Tensor\n    past_key_value: Optional[object]\n\n\n@dataclass\nclass CLMOutput(object):\n    hidden_state: torch.Tensor\n    logits: torch.Tensor\n    kv_cache: List[torch.FloatTensor] = None\n\n\nclass DecoderLayer(nn.Module):\n\n    def __init__(self, config, layer_idx: int, attention_type: str = None) -> None:\n        super().__init__()\n        self.attention = (DecoderAttention(config, layer_idx=layer_idx)\n        )\n        if attention_type == \"gqa\" and layer_idx == 0:  # avoid to print m times\n            print(\"Decoder Using GQA Attention\")\n        self.feed_forward = FFNGeluModule(config.hidden_size, 4*config.hidden_size, config.hidden_size)\n        self.layer_idx = layer_idx\n        self.layernorm = RMSNorm(\n            config.hidden_size, eps=getattr(config, \"layer_norm_eps\", 1e-6)\n        )\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = False,\n        kv_cache: List[torch.FloatTensor] = None,\n        start_pos: Optional[int] = 0,\n    ) -> torch.Tensor:\n        out,kv_cache = self.attention(\n            hidden_state=hidden_state,\n            attention_mask=attention_mask,\n            freqs=freqs,\n            use_cache=use_cache,\n            kv_cache=kv_cache,\n            start_pos=start_pos,\n        )\n        b,l,d = out.size()\n        out1 = out.view(-1,d).contiguous()\n        out1 = self.feed_forward(out1)\n        out1 = out1.view(b,l,d).contiguous() #out # \n        out = self.layernorm(out1 + out)\n        return out, kv_cache\n\n\nclass LMHead(nn.Module):\n    \"\"\"Head for masked language modelling\"\"\"\n\n    def __init__(self, config) -> None:\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.layerNorm = RMSNorm(\n            config.hidden_size, eps=getattr(config, \"layer_norm_eps\", 1e-6)\n        )\n\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n        self.decoder.bias = self.bias\n        self.gelu = GELU()\n\n    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n        x = self.dense(hidden_state)\n        x = self.gelu(x)\n        x = self.layerNorm(x)\n\n        # project back to size of vocabulary with bias\n        x = self.decoder(x)\n\n        return x\n\nclass LinearCrossEntropyIgnoreIndex(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, inputs, weight, bias, targets,chunk_size=32, ignore_index=-100):\n        logits = F.linear(inputs, weight, bias)\n        shape = logits.size()\n        logits_flat = logits.view(-1, shape[-1])\n        targets_flat = targets.view(-1)\n        valid_mask = targets_flat != ignore_index\n        valid_logits = logits_flat[valid_mask]\n        valid_targets = targets_flat[valid_mask]\n\n        if chunk_size is not None:\n            logit_chunks = valid_logits.split(chunk_size)\n            target_chunks = valid_targets.split(chunk_size)\n            loss_chunks = [\n                torch.nn.functional.cross_entropy(\n                    logit_chunk, target_chunk, ignore_index=ignore_index, reduction=\"none\"\n                )\n                for logit_chunk, target_chunk in zip(logit_chunks, target_chunks)]\n            \n            loss = torch.cat(loss_chunks).mean()\n\n\n        else:\n            softmax = F.softmax(valid_logits, dim=-1)\n            log_probs = torch.log(softmax + 1e-12)  # Numerical stability\n            target_log_probs = log_probs[torch.arange(valid_logits.size(0)), valid_targets]\n            loss = -target_log_probs.mean()\n\n        # softmax = F.softmax(valid_logits, dim=-1)\n        # log_probs = torch.log(softmax + 1e-12)  # Numerical stability\n        # target_log_probs = log_probs[torch.arange(valid_logits.size(0)), valid_targets]\n        # loss = -target_log_probs.mean()\n\n        ctx.save_for_backward(inputs, weight, valid_targets, valid_mask, logits_flat)\n        ctx.shape = shape\n        ctx.ignore_index = ignore_index\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_outputs):\n        inputs, weight, valid_targets, valid_mask, logits_flat = ctx.saved_tensors\n\n        grad_logits = torch.zeros_like(logits_flat,dtype = weight.dtype)\n        valid_logits = logits_flat[valid_mask]\n        valid_grad_logits = F.softmax(valid_logits, dim=-1)\n        valid_grad_logits[torch.arange(valid_grad_logits.size(0)), valid_targets] -= 1\n        valid_grad_logits /= valid_grad_logits.size(0)  # Normalize by batch size\n\n        grad_logits[valid_mask] = valid_grad_logits\n        grad_logits = grad_logits.view(*ctx.shape)\n        grad_input = grad_weight = grad_bias = None\n\n        grad_loss = grad_logits * grad_outputs\n\n        grad_input = grad_loss.matmul(weight)\n        grad_weight = grad_loss.transpose(-2, -1).matmul(inputs)\n        grad_bias = grad_loss.sum(dim=0)\n\n        return grad_input, grad_weight, grad_bias, None, None, None\n\n\n\nclass MyLinearCrossEntropy(torch.nn.Module):\n    def __init__(self, in_features=768, out_features=50265, ignore_index=-100,chunk_size=32):\n        super(MyLinearCrossEntropy, self).__init__()\n        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n        self.ignore_index = ignore_index\n        self.chunk_size = chunk_size\n        torch.nn.init.xavier_uniform_(self.weight)\n        torch.nn.init.zeros_(self.bias)\n\n    def forward(self, x, target=None):\n        if target is not None:\n            x = x[:,:-1,:].contiguous()\n            target = target[:,1:].contiguous()\n            return LinearCrossEntropyIgnoreIndex.apply(x, self.weight, self.bias, target,self.chunk_size, self.ignore_index)\n        else:\n            if x.dim() != 2:\n                B, l, d = x.shape\n                x = x.view(-1, d).contiguous()\n            output = x.mm(self.weight.t())\n            # if bias is not None:\n            output += self.bias.unsqueeze(0).expand_as(output)\n            \n            return output.view(B,l,50265)\nclass DecoderModel(nn.Module):\n\n    def __init__(\n        self,\n        config,\n        pos_embedding_type: Optional[str] = \"absolute\",\n        attention_type: str = None,\n    ) -> None:\n        super().__init__()\n        self.word_embeddings = nn.Embedding(\n            config.vocab_size,\n            config.hidden_size,\n            padding_idx=getattr(config, \"pad_token_id\", None),\n        )\n        self.emb_freq = RotaryEmbedding(config)(config.max_position_embeddings)\n        print(\n                \"Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\"\n            )\n        self.all_layer = nn.ModuleList(\n            [\n                DecoderLayer(config, layer_idx, attention_type)\n                for layer_idx in range(config.num_hidden_layers)\n            ]\n        )\n        self.lm_head = MyLinearCrossEntropy()\n        self.config = config\n\n    def _init_weights(self, module: nn.Module) -> None:\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(\n                module.weight, mean=0.0, std=0.02 / torch.sqrt(2 * len(self.all_layer))\n            )\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(\n                module.weight, mean=0.0, std=0.02 / torch.sqrt(2 * len(self.all_layer))\n            )\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = False,\n        target: Optional[torch.Tensor] = None,\n        kv_cache: List[torch.FloatTensor] = None,\n        start_pos: Optional[int] = 0,\n    ) -> torch.Tensor:\n        _bsz, seqlen = input_ids.shape\n        hidden_state = self.word_embeddings(input_ids)\n        freqs = self.emb_freq[:, start_pos : start_pos + seqlen].to(\n            input_ids.device\n        )\n        mask = None\n        if seqlen > 1:\n            mask = self.create_mask_for_decoder(\n                input_ids=input_ids, attention_mask=attention_mask, start_pos=start_pos\n            )\n            mask = (1.0 - mask) * torch.finfo(\n                hidden_state.dtype\n            ).min  # invert it to to add directly to attention score\n\n        for layer in self.all_layer:\n            hidden_state, kv_cache = layer(\n                hidden_state,\n                mask,\n                freqs=freqs,\n                use_cache=use_cache,\n                kv_cache=kv_cache,\n                start_pos=start_pos,\n            )\n        logits = self.lm_head(hidden_state,target)\n        if target is not None:\n            return logits\n        return CLMOutput(hidden_state=hidden_state, logits=logits, kv_cache=kv_cache)\n\n    def create_mask_for_decoder(\n        self,\n        input_ids,\n        attention_mask: Optional[torch.Tensor] = None,\n        start_pos: Optional[int] = 0,\n    ) -> torch.Tensor:\n        device = input_ids.device\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = (\n                torch.ones(seq_length + start_pos).repeat(batch_size, 1).to(device)\n            )\n        seq_ids = torch.arange(seq_length).to(device)\n        causal_mask = (\n            seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n            <= seq_ids[None, :, None]\n        )  # 1x1xl repeat bxlxl compare to 1xlx1\n\n        causal_mask = causal_mask.to(attention_mask.dtype)\n\n        if start_pos > 0:  # correct the attention mask  for kv-cache operation\n            causal_mask = torch.cat(\n                [\n                    torch.ones(\n                        (batch_size, seq_length, start_pos),\n                        device=device,\n                        dtype=causal_mask.dtype,\n                    ),\n                    causal_mask,\n                ],\n                axis=-1,\n            )\n\n        extended_attention_mask = (\n            causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n        )  # # this is mainly if batch contains <PAD> tokens. stop casual procees before <PAD>\n        return extended_attention_mask\n\n    @classmethod\n    def from_config(\n        cls,\n        config,\n        pos_embedding_type: Optional[str] = \"absolute\",\n        attention_type: Optional[str] = None,\n    ) -> nn.Module:\n        return cls(config, pos_embedding_type, attention_type)\n\n    def _setup_cache(self, config, cls: Optional[object] = StaticCache) -> None:\n        for layer in self.all_layer:\n            layer.attention.cache = cls(config)\n\n    def _clean_cache(self) -> None:\n        for layer in self.all_layer:\n            layer.attention.cache = None\n","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:01.797949Z","iopub.execute_input":"2025-07-05T08:55:01.798434Z","iopub.status.idle":"2025-07-05T08:55:01.833994Z","shell.execute_reply.started":"2025-07-05T08:55:01.798410Z","shell.execute_reply":"2025-07-05T08:55:01.833211Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from einops import rearrange","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:01.834849Z","iopub.execute_input":"2025-07-05T08:55:01.835131Z","iopub.status.idle":"2025-07-05T08:55:01.848736Z","shell.execute_reply.started":"2025-07-05T08:55:01.835107Z","shell.execute_reply":"2025-07-05T08:55:01.848053Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"m =AutoModelForCausalLM.from_pretrained(\"../input/transformer-distilation-gpt-2/gpt2_6L\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:01.849394Z","iopub.execute_input":"2025-07-05T08:55:01.849639Z","iopub.status.idle":"2025-07-05T08:55:18.750172Z","shell.execute_reply.started":"2025-07-05T08:55:01.849620Z","shell.execute_reply":"2025-07-05T08:55:18.749413Z"}},"outputs":[{"name":"stderr","text":"2025-07-05 08:55:04.065976: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751705704.256077      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751705704.315930      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:18.751083Z","iopub.execute_input":"2025-07-05T08:55:18.751705Z","iopub.status.idle":"2025-07-05T08:55:19.846378Z","shell.execute_reply.started":"2025-07-05T08:55:18.751678Z","shell.execute_reply":"2025-07-05T08:55:19.845662Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aabd8c0737d4a6a97ed25905ae57981"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cff5fa04443e4666a3bdf0dfbc6fce12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"952eacf0705846ab9c9815b585356803"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54bace7a5eeb413fa0223f446de75d41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c560b19c19c94a42b3ba847d1ec26e41"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"tokenizer.model_max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:19.847008Z","iopub.execute_input":"2025-07-05T08:55:19.847198Z","iopub.status.idle":"2025-07-05T08:55:19.852134Z","shell.execute_reply.started":"2025-07-05T08:55:19.847183Z","shell.execute_reply":"2025-07-05T08:55:19.851391Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"1024"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"if tokenizer.pad_token_id is None:\n  tokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Set reasonable default for models without max length\nif tokenizer.model_max_length > 512:\n  tokenizer.model_max_length = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:19.852915Z","iopub.execute_input":"2025-07-05T08:55:19.853254Z","iopub.status.idle":"2025-07-05T08:55:19.861885Z","shell.execute_reply.started":"2025-07-05T08:55:19.853214Z","shell.execute_reply":"2025-07-05T08:55:19.861276Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"state_dict = m.state_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:19.862580Z","iopub.execute_input":"2025-07-05T08:55:19.862814Z","iopub.status.idle":"2025-07-05T08:55:19.872690Z","shell.execute_reply.started":"2025-07-05T08:55:19.862791Z","shell.execute_reply":"2025-07-05T08:55:19.872124Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"string = open('/kaggle/input/mark-twain-books/Combine.txt',encoding='utf8',errors='ignore').read()\nnew_str = re.sub('�', '', string)\nopen('Train.txt', 'w').write(new_str)","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:19.873358Z","iopub.execute_input":"2025-07-05T08:55:19.873571Z","iopub.status.idle":"2025-07-05T08:55:19.931168Z","shell.execute_reply.started":"2025-07-05T08:55:19.873547Z","shell.execute_reply":"2025-07-05T08:55:19.930622Z"},"trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"6588596"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"model_ckpt = \"roberta-base\"\nconfig = AutoConfig.from_pretrained(model_ckpt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:19.931842Z","iopub.execute_input":"2025-07-05T08:55:19.932051Z","iopub.status.idle":"2025-07-05T08:55:20.042893Z","shell.execute_reply.started":"2025-07-05T08:55:19.932035Z","shell.execute_reply":"2025-07-05T08:55:20.042155Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eec268dc4e14dc09c2671e7ea0a2225"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:20.043723Z","iopub.execute_input":"2025-07-05T08:55:20.044156Z","iopub.status.idle":"2025-07-05T08:55:20.049782Z","shell.execute_reply.started":"2025-07-05T08:55:20.044132Z","shell.execute_reply":"2025-07-05T08:55:20.048958Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.51.3\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from types import SimpleNamespace\nfrom collections import namedtuple\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:20.050798Z","iopub.execute_input":"2025-07-05T08:55:20.051100Z","iopub.status.idle":"2025-07-05T08:55:20.059098Z","shell.execute_reply.started":"2025-07-05T08:55:20.051083Z","shell.execute_reply":"2025-07-05T08:55:20.058591Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"config = SimpleNamespace(**config.__dict__)\nconfig.vocab_size = len(tokenizer)\nconfig.num_hidden_layers = 6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:20.059915Z","iopub.execute_input":"2025-07-05T08:55:20.060160Z","iopub.status.idle":"2025-07-05T08:55:20.077567Z","shell.execute_reply.started":"2025-07-05T08:55:20.060141Z","shell.execute_reply":"2025-07-05T08:55:20.076962Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = DecoderModel.from_config(config,pos_embedding_type='rope')","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:20.078150Z","iopub.execute_input":"2025-07-05T08:55:20.078342Z","iopub.status.idle":"2025-07-05T08:55:21.077810Z","shell.execute_reply.started":"2025-07-05T08:55:20.078328Z","shell.execute_reply":"2025-07-05T08:55:21.077167Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"model.word_embeddings = nn.Embedding.from_pretrained(\n    state_dict[\"transformer.wte.weight\"], freeze=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T08:55:21.078600Z","iopub.execute_input":"2025-07-05T08:55:21.078873Z","iopub.status.idle":"2025-07-05T08:55:21.096494Z","shell.execute_reply.started":"2025-07-05T08:55:21.078850Z","shell.execute_reply":"2025-07-05T08:55:21.095983Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"**Data Source**\n\nhttps://www.kaggle.com/datasets/msinger007/mark-twain-books","metadata":{}},{"cell_type":"code","source":"train_path = 'Train.txt'","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:21.097076Z","iopub.execute_input":"2025-07-05T08:55:21.097268Z","iopub.status.idle":"2025-07-05T08:55:21.100536Z","shell.execute_reply.started":"2025-07-05T08:55:21.097247Z","shell.execute_reply":"2025-07-05T08:55:21.099675Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class TextDataset(Dataset):\n\n    def __init__(\n        self,\n        tokenizer,\n        file_path: str,\n        block_size: int):\n        if os.path.isfile(file_path) is False:\n            raise ValueError(f\"Input file path {file_path} not found\")\n\n        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n        saved = False\n        cache_dir = None\n        directory, filename = os.path.split(file_path)\n        cached_features_file = os.path.join(\n            cache_dir if cache_dir is not None else directory,\n            f\"cached_lm_{tokenizer.__class__.__name__}_{block_size}_{filename}\",\n        )\n\n     \n        if os.path.exists(cached_features_file) and saved :\n                start = time.time()\n                with open(cached_features_file, \"rb\") as handle:\n                    self.examples = pickle.load(handle)\n#                 logger.info(\n#                     f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n#                 )\n\n        else:\n#                 logger.info(f\"Creating features from dataset file at {directory}\")\n\n                self.examples = []\n                with open(file_path, encoding=\"utf-8\") as f:\n                    text = f.read()\n\n                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n\n                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n                    self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size]))\n                    # )\n                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n                # If your dataset is small, first you should look for a bigger one :-) and second you\n                # can change this behavior by adding (model specific) padding.\n\n                start = time.time()\n                with open(cached_features_file, \"wb\") as handle:\n                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n                    saved = True\n#                 logger.info(\n#                     f\"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]\"\n#                 )\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i) -> torch.Tensor:\n        return {\"input_ids\":torch.tensor(self.examples[i], dtype=torch.long)}","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:21.101141Z","iopub.execute_input":"2025-07-05T08:55:21.101364Z","iopub.status.idle":"2025-07-05T08:55:21.110748Z","shell.execute_reply.started":"2025-07-05T08:55:21.101349Z","shell.execute_reply":"2025-07-05T08:55:21.110060Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def collate(batch):\n    labels = batch[\"input_ids\"].clone()\n    if tokenizer.pad_token_id is not None:\n        labels[labels == tokenizer.pad_token_id] = -100\n    batch[\"labels\"] = labels\n    return batch","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:21.111609Z","iopub.execute_input":"2025-07-05T08:55:21.111830Z","iopub.status.idle":"2025-07-05T08:55:21.121879Z","shell.execute_reply.started":"2025-07-05T08:55:21.111804Z","shell.execute_reply":"2025-07-05T08:55:21.121302Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(TextDataset(tokenizer,train_path,128), batch_size=16, shuffle=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:21.124991Z","iopub.execute_input":"2025-07-05T08:55:21.125194Z","iopub.status.idle":"2025-07-05T08:55:29.041939Z","shell.execute_reply.started":"2025-07-05T08:55:21.125180Z","shell.execute_reply":"2025-07-05T08:55:29.041369Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1580900 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"\ndef train(model, train_loader, EPOCHS=3):\n    EPOCHS = 5\n    accumulation_steps = 2\n    lr=5e-5\n    # train it fully \n    no_decay = ['bias', 'layerNorm.weight','layerNorm.bias']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n    num_train_optimization_steps = int(EPOCHS * len(train_loader) / accumulation_steps)\n    # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.05 * num_train_optimization_steps,\n    #                                     num_training_steps=num_train_optimization_steps)\n    best_epoch_loss = np.inf\n    accelerator = Accelerator(log_with=\"tensorboard\",project_dir=\".\",gradient_accumulation_steps=2)\n    Config = {\n        \"num_epoch\": EPOCHS,\n        \"learning_rate\": lr,\n        \"loss_function\": str(torch.nn.CrossEntropyLoss),\n    }\n    epoch_check = len(train_loader)\n    total_step = epoch_check * EPOCHS\n    accelerator.init_trackers(\"CLM_project\", config=Config)\n    # train_bar =tqdm(total=total_step, dynamic_ncols=True, disable=not accelerator.is_main_process)\n    train_loader,model,optimizer =  accelerator.prepare(train_loader,model, optimizer)\n    model.train()\n    t_step = 0\n    # loss_fn = CustomLoss.apply\n    \n    for epoch in range(EPOCHS):\n        loss_list = []\n        for step, data in enumerate(train_loader):\n            # train_bar.update(1)\n            with accelerator.accumulate(model):\n                data =  collate(data)\n                x = data[\"input_ids\"] #.to(device)\n                y = data['labels'] #.to(device)\n                optimizer.zero_grad()\n                loss = model(input_ids = x,target=y)\n                # pred = pred[:, :-1, :].contiguous()\n                # y = y[:, 1:].contiguous()\n                # loss = loss_fn(pred,y)\n                accelerator.backward(loss)\n                optimizer.step()\n                # scheduler.step()\n                accelerator.log({\"training_loss_step\": loss}, step=t_step)\n                t_step+=1\n            \n            loss_list.append(loss.detach().cpu().item())\n        \n        avg_loss = np.round(np.mean(loss_list), 4)\n\n        accelerator.print(f'Epoch--{epoch+1} ### Train loss---{avg_loss}')\n        \n    PATH = f\"decoder__{epoch}.pth\"\n    model = accelerator.unwrap_model(model)\n    torch.save(model.state_dict(), PATH)\n    accelerator.end_training()\n    accelerator.free_memory(train_loader,model, optimizer)","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:29.155882Z","iopub.execute_input":"2025-07-05T08:55:29.156165Z","iopub.status.idle":"2025-07-05T08:55:29.167160Z","shell.execute_reply.started":"2025-07-05T08:55:29.156148Z","shell.execute_reply":"2025-07-05T08:55:29.166465Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"train(model,train_loader)","metadata":{"execution":{"iopub.status.busy":"2025-07-05T08:55:29.167859Z","iopub.execute_input":"2025-07-05T08:55:29.168018Z","iopub.status.idle":"2025-07-05T09:20:06.026799Z","shell.execute_reply.started":"2025-07-05T08:55:29.168005Z","shell.execute_reply":"2025-07-05T09:20:06.025946Z"},"trusted":true},"outputs":[{"name":"stderr","text":"W0705 08:55:39.077000 35 torch/_inductor/utils.py:1137] [1/0] Not enough SMs to use max_autotune_gemm mode\n","output_type":"stream"},{"name":"stdout","text":"Epoch--1 ### Train loss---6.8396\nEpoch--2 ### Train loss---5.5833\nEpoch--3 ### Train loss---5.2548\nEpoch--4 ### Train loss---5.0357\nEpoch--5 ### Train loss---4.8594\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# from accelerate import notebook_launcher\n# notebook_launcher(train, (model,train_loader), num_processes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:06.028214Z","iopub.execute_input":"2025-07-05T09:20:06.028525Z","iopub.status.idle":"2025-07-05T09:20:06.032053Z","shell.execute_reply.started":"2025-07-05T09:20:06.028496Z","shell.execute_reply":"2025-07-05T09:20:06.031392Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"model = DecoderModel.from_config(config,pos_embedding_type='rope')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:06.032839Z","iopub.execute_input":"2025-07-05T09:20:06.033419Z","iopub.status.idle":"2025-07-05T09:20:07.066969Z","shell.execute_reply.started":"2025-07-05T09:20:06.033401Z","shell.execute_reply":"2025-07-05T09:20:07.066162Z"}},"outputs":[{"name":"stdout","text":"Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/working/decoder__4.pth', weights_only=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:07.067717Z","iopub.execute_input":"2025-07-05T09:20:07.067977Z","iopub.status.idle":"2025-07-05T09:20:07.607816Z","shell.execute_reply.started":"2025-07-05T09:20:07.067950Z","shell.execute_reply":"2025-07-05T09:20:07.607056Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:07.608660Z","iopub.execute_input":"2025-07-05T09:20:07.609396Z","iopub.status.idle":"2025-07-05T09:20:07.612890Z","shell.execute_reply.started":"2025-07-05T09:20:07.609367Z","shell.execute_reply":"2025-07-05T09:20:07.612156Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"model.to(device)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:07.613724Z","iopub.execute_input":"2025-07-05T09:20:07.614509Z","iopub.status.idle":"2025-07-05T09:20:07.764142Z","shell.execute_reply.started":"2025-07-05T09:20:07.614483Z","shell.execute_reply":"2025-07-05T09:20:07.763528Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"DecoderModel(\n  (word_embeddings): Embedding(50257, 768, padding_idx=1)\n  (all_layer): ModuleList(\n    (0-5): 6 x DecoderLayer(\n      (attention): DecoderAttention(\n        (query): Linear(in_features=768, out_features=768, bias=True)\n        (key): Linear(in_features=768, out_features=768, bias=True)\n        (value): Linear(in_features=768, out_features=768, bias=True)\n        (out): LinearRMSFused()\n      )\n      (feed_forward): FFNGeluModule()\n      (layernorm): RMSNorm()\n    )\n  )\n  (lm_head): MyLinearCrossEntropy()\n)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"@torch.no_grad()\ndef generate(\n    model,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    max_len: int = 20,\n    temperature: float = 1.0,\n    use_cache: bool = True,\n    do_sample: bool = False,\n    use_static_cache: bool = False,\n) -> torch.Tensor:\n\n    device = input_ids.device\n\n    all_prompt_size = [t.size()[0] for t in input_ids]\n\n    min_prompt_len = min(all_prompt_size)\n    max_prompt_len = max(all_prompt_size)\n\n    max_len = (\n        max_len + max_prompt_len\n    )  # get  max len (prompt + to be generated token combined)\n\n    pad_id = getattr(model.config, \"pad_token_id\", 50256)\n    bsz, _ = input_ids.size()\n    tokens = torch.full((bsz, max_len), pad_id, dtype=torch.long, device=device)\n\n    kv_cache = None\n    if use_cache:\n        if use_static_cache:\n            kv_cache = StaticCache(model.config, max_cache_len=max_len, batch_size=bsz)\n        else:\n            kv_cache = DynamicCache(model.config)\n\n    for k, t in enumerate(input_ids):\n        tokens[k, : t.size()[0]] = t\n\n    prev_pos = torch.tensor(0, device=device)\n    eos_reached = torch.tensor([False] * bsz, device=device)\n    # to break generation if eos reached for all  prompt\n\n    input_text_mask = tokens != pad_id  # mask to fill generated values into batch\n\n    stop_tokens = torch.tensor(getattr(model.config, \"eos_token_id\", 50256), device=device)\n    for cur_pos in range(min_prompt_len, max_len):\n\n        # Get the model output\n        with torch.no_grad():\n            outputs = model(\n                input_ids=tokens[:, prev_pos:cur_pos],\n                attention_mask=attention_mask,\n                use_cache=use_cache,\n                kv_cache=kv_cache,\n                start_pos=prev_pos,\n            )\n        kv_cache = outputs.kv_cache\n        next_token_logits = outputs.logits[:, -1] / temperature\n\n        if do_sample:\n            next_token = torch.multinomial(next_token_logits, num_samples=1)\n        else:\n            _, next_token = torch.topk(next_token_logits, k=1, dim=-1)\n\n        next_token = next_token.reshape(-1)\n        # only replace token if prompt has already been generated\n        next_token = torch.where(\n            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n        )\n        tokens[:, cur_pos] = next_token\n        eos_reached |= (~input_text_mask[:, cur_pos]) & (\n            torch.isin(next_token, stop_tokens)\n        )\n\n        if use_cache:\n            prev_pos = cur_pos\n\n        attention_mask = torch.cat(\n            [attention_mask, torch.ones((bsz, 1), device=device)], dim=-1\n        )\n        if all(eos_reached):\n            break\n    return tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:07.764854Z","iopub.execute_input":"2025-07-05T09:20:07.765064Z","iopub.status.idle":"2025-07-05T09:20:07.774662Z","shell.execute_reply.started":"2025-07-05T09:20:07.765048Z","shell.execute_reply":"2025-07-05T09:20:07.774031Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"text = tokenizer([\"this is a test, blue\",\"Well, sir, you could\"], return_tensors=\"pt\", padding=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:07.775328Z","iopub.execute_input":"2025-07-05T09:20:07.775540Z","iopub.status.idle":"2025-07-05T09:20:07.789278Z","shell.execute_reply.started":"2025-07-05T09:20:07.775524Z","shell.execute_reply":"2025-07-05T09:20:07.788452Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:07.789979Z","iopub.execute_input":"2025-07-05T09:20:07.790179Z","iopub.status.idle":"2025-07-05T09:20:07.802618Z","shell.execute_reply.started":"2025-07-05T09:20:07.790164Z","shell.execute_reply":"2025-07-05T09:20:07.801922Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 5661,   318,   257,  1332,    11,  4171],\n        [ 5779,    11, 15967,    11,   345,   714]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"input_ids , attention_mask = text['input_ids'],text['attention_mask']\ninput_ids = input_ids.to(device)\nattention_mask = attention_mask.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:07.803368Z","iopub.execute_input":"2025-07-05T09:20:07.803617Z","iopub.status.idle":"2025-07-05T09:20:07.812163Z","shell.execute_reply.started":"2025-07-05T09:20:07.803593Z","shell.execute_reply":"2025-07-05T09:20:07.811501Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"out = generate(model,input_ids=input_ids , attention_mask=attention_mask,use_cache=False)\ntokenizer.batch_decode(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:07.813054Z","iopub.execute_input":"2025-07-05T09:20:07.813334Z","iopub.status.idle":"2025-07-05T09:20:16.743505Z","shell.execute_reply.started":"2025-07-05T09:20:07.813312Z","shell.execute_reply":"2025-07-05T09:20:16.742817Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"['this is a test, blue-room, and a  little, and a little, and a little, and a little,',\n 'Well, sir, you could see the  old man, and you are going to see you.  The man was a good']"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"out = generate(model,input_ids=input_ids , attention_mask=attention_mask,use_cache=True)\ntokenizer.batch_decode(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T09:20:16.744122Z","iopub.execute_input":"2025-07-05T09:20:16.744341Z","iopub.status.idle":"2025-07-05T09:20:21.113731Z","shell.execute_reply.started":"2025-07-05T09:20:16.744326Z","shell.execute_reply":"2025-07-05T09:20:21.113144Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"['this is a test, blue-room, and a  little, and a little, and a little, and a little,',\n 'Well, sir, you could see the  old man, and you are going to see you.  The man was a good']"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}