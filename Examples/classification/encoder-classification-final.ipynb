{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-29T08:31:31.654752Z","iopub.status.busy":"2024-04-29T08:31:31.654102Z","iopub.status.idle":"2024-04-29T08:31:37.173061Z","shell.execute_reply":"2024-04-29T08:31:37.172114Z","shell.execute_reply.started":"2024-04-29T08:31:31.654720Z"},"papermill":{"duration":7.433588,"end_time":"2022-08-17T12:43:27.012415","exception":false,"start_time":"2022-08-17T12:43:19.578827","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import gc\n","import os\n","import sys\n","import numpy as np\n","import random\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import math\n","import torch\n","import transformers\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.cuda.amp import GradScaler, autocast\n","from accelerate import Accelerator\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AutoModel,\n","    AutoTokenizer,\n","    AdamW,\n","    get_linear_schedule_with_warmup,\n",")\n","from transformers import AutoConfig\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","import warnings\n","\n","warnings.simplefilter(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:37.175754Z","iopub.status.busy":"2024-04-29T08:31:37.175238Z","iopub.status.idle":"2024-04-29T08:31:37.182555Z","shell.execute_reply":"2024-04-29T08:31:37.181544Z","shell.execute_reply.started":"2024-04-29T08:31:37.175725Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'2.1.2'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.__version__"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:37.184017Z","iopub.status.busy":"2024-04-29T08:31:37.183673Z","iopub.status.idle":"2024-04-29T08:31:37.228437Z","shell.execute_reply":"2024-04-29T08:31:37.227519Z","shell.execute_reply.started":"2024-04-29T08:31:37.183980Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:37.230915Z","iopub.status.busy":"2024-04-29T08:31:37.230609Z","iopub.status.idle":"2024-04-29T08:31:37.249116Z","shell.execute_reply":"2024-04-29T08:31:37.248140Z","shell.execute_reply.started":"2024-04-29T08:31:37.230889Z"},"papermill":{"duration":0.013001,"end_time":"2022-08-17T12:43:27.028923","exception":false,"start_time":"2022-08-17T12:43:27.015922","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:37.250655Z","iopub.status.busy":"2024-04-29T08:31:37.250358Z","iopub.status.idle":"2024-04-29T08:31:39.581976Z","shell.execute_reply":"2024-04-29T08:31:39.580986Z","shell.execute_reply.started":"2024-04-29T08:31:37.250630Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61e56fdeac644fa28b0c9f6b331ad6e1","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8552ddfd39f4d098cd655024624bfe1","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38ab749937ed424086dcf7d40e5b60bb","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa1b9bc1c9cc45df93f74e1385790d7d","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9110061ed404fe69d56307bc253e1ac","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_ckpt = \"roberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","config = AutoConfig.from_pretrained(model_ckpt)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:39.583842Z","iopub.status.busy":"2024-04-29T08:31:39.583481Z","iopub.status.idle":"2024-04-29T08:31:39.591649Z","shell.execute_reply":"2024-04-29T08:31:39.590877Z","shell.execute_reply.started":"2024-04-29T08:31:39.583811Z"},"papermill":{"duration":0.012823,"end_time":"2022-08-17T12:43:27.068846","exception":false,"start_time":"2022-08-17T12:43:27.056023","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["EPOCHS = 5\n","lr = 1e-3\n","SEED = 42\n","MAX_LEN = 128\n","BATCH_SIZE = 64\n","accumulation_steps = 2\n","seed_everything(SEED)"]},{"cell_type":"markdown","metadata":{},"source":["**Data Source**\n","\n","from datasets import load_dataset\n","\n","\n","clinc = load_dataset(\"clinc_oos\", \"plus\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:39.594245Z","iopub.status.busy":"2024-04-29T08:31:39.593912Z","iopub.status.idle":"2024-04-29T08:31:39.689461Z","shell.execute_reply":"2024-04-29T08:31:39.688464Z","shell.execute_reply.started":"2024-04-29T08:31:39.594221Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","      <th>Target</th>\n","      <th>intent</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>what expression would i use to say i love you ...</td>\n","      <td>61</td>\n","      <td>translate</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>can you tell me how to say 'i do not speak muc...</td>\n","      <td>61</td>\n","      <td>translate</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Text  Target     intent\n","0  what expression would i use to say i love you ...      61  translate\n","1  can you tell me how to say 'i do not speak muc...      61  translate"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["data_path = \"../input/data-for-distilation\"\n","train = pd.read_csv(\"../input/data-for-distilation/Clinc_Train.csv\")\n","valid = pd.read_csv(\"../input/data-for-distilation/Clinc_valid.csv\")\n","n_classes = np.unique(train.Target).shape[0]\n","train.head(2)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:39.690697Z","iopub.status.busy":"2024-04-29T08:31:39.690436Z","iopub.status.idle":"2024-04-29T08:31:39.700272Z","shell.execute_reply":"2024-04-29T08:31:39.699305Z","shell.execute_reply.started":"2024-04-29T08:31:39.690674Z"},"trusted":true},"outputs":[{"data":{"text/plain":["151"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train.Target.nunique()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:39.701748Z","iopub.status.busy":"2024-04-29T08:31:39.701468Z","iopub.status.idle":"2024-04-29T08:31:53.327322Z","shell.execute_reply":"2024-04-29T08:31:53.326333Z","shell.execute_reply.started":"2024-04-29T08:31:39.701723Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n","Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n"]}],"source":["!pip install einops"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:53.329201Z","iopub.status.busy":"2024-04-29T08:31:53.328887Z","iopub.status.idle":"2024-04-29T08:31:53.364230Z","shell.execute_reply":"2024-04-29T08:31:53.363441Z","shell.execute_reply.started":"2024-04-29T08:31:53.329173Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from einops import rearrange, reduce, repeat\n","from typing import Optional, Tuple\n","\n","\n","def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n","    \"\"\"\n","    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n","    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n","    \"\"\"\n","    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n","    if n_rep == 1:\n","        return hidden_states\n","    hidden_states = hidden_states[:, :, None, :, :].expand(\n","        batch, num_key_value_heads, n_rep, slen, head_dim\n","    )\n","    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n","\n","\n","def repeat_kv_einops(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n","    \"\"\"\n","    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n","    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n","    \"\"\"\n","    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n","    if n_rep == 1:\n","        return hidden_states\n","    hidden_states = repeat(\n","        hidden_states,\n","        \"batch num_key_value_heads slen head_dim -> batch num_key_value_heads n_rep slen head_dim\",\n","        n_rep=n_rep,\n","    )  # hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n","    # return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n","    return rearrange(\n","        hidden_states,\n","        \"batch num_key_value_heads n_rep slen head_dim -> batch (num_key_value_heads n_rep) slen head_dim\",\n","    )\n","\n","\n","class EncoderAttention(nn.Module):\n","    def __init__(self, config, layer_idx: int) -> None:\n","        super().__init__()\n","        if config.hidden_size % config.num_attention_heads != 0:\n","            raise ValueError(\n","                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n","                f\"heads ({config.num_attention_heads})\"\n","            )\n","        self.head_size = int(config.hidden_size // config.num_attention_heads)\n","        self.attention_bias = getattr(config, \"attention_bias\", True)\n","        self.layer_idx = layer_idx\n","        # self.qkv = nn.Linear(config.hidden_size,3*config.hidden_size)\n","        self.q = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.k = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.v = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.out = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.num_attention_heads = config.num_attention_heads\n","\n","    def forward(\n","        self,\n","        hidden_state: torch.Tensor,\n","        attention_mask: torch.Tensor,\n","        freqs: Optional[torch.Tensor] = None,\n","    ) -> torch.Tensor:\n","        q = self.q(hidden_state)\n","        k = self.k(hidden_state)\n","        v = self.v(hidden_state)\n","        # q,k,v = self.qkv(hidden_state).chunk(3, dim = -1) #b X l X d dim =-1 or 2\n","        # place holder for RoPe operation\n","        q = rearrange(q, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n","        k = rearrange(k, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n","        v = rearrange(v, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n","        if freqs is not None:\n","            q, k = apply_rotary_pos_emb(q, k, freqs)\n","\n","        out = torch.nn.functional.scaled_dot_product_attention(\n","            query=q, key=k, value=v, attn_mask=attention_mask, is_causal=False\n","        )\n","        out = rearrange(out, \"b h l d -> b l (h d)\")\n","        out = self.out(out)\n","        return out\n","\n","\n","class EncoderAttentionGqa(nn.Module):\n","    def __init__(self, config, layer_idx: int) -> None:\n","        super().__init__()\n","        if config.hidden_size % config.num_attention_heads != 0:\n","            raise ValueError(\n","                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n","                f\"heads ({config.num_attention_heads})\"\n","            )\n","        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n","        if not self.flash and self.layer_idx == 0:  # avoid to print m times\n","            print(\"WARNING: Flash Attention requires PyTorch >= 2.0\")\n","        self.layer_idx = layer_idx\n","        self.head_dim = int(config.hidden_size // config.num_attention_heads)\n","        self.num_attention_heads = config.num_attention_heads\n","        self.num_key_value_heads = getattr(config, \"num_key_value_heads\", 4)\n","        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads\n","        if (\n","            self.num_attention_heads % self.num_key_value_heads != 0\n","            or self.num_attention_heads < self.num_key_value_heads\n","        ):\n","            raise ValueError(\n","                f\"num_key_value_heads {self.num_key_value_heads }  should be less than equal num_attention_heads {config.num_attention_heads} and  multiple of num_attention_heads {config.num_attention_heads} \"\n","            )\n","        self.attention_bias = getattr(config, \"attention_bias\", True)\n","        self.out = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.q = nn.Linear(\n","            config.hidden_size, config.hidden_size, bias=self.attention_bias\n","        )\n","        self.k = nn.Linear(\n","            config.hidden_size,\n","            self.num_key_value_heads * self.head_dim,\n","            bias=self.attention_bias,\n","        )\n","        self.v = nn.Linear(\n","            config.hidden_size,\n","            self.num_key_value_heads * self.head_dim,\n","            bias=self.attention_bias,\n","        )\n","\n","    def forward(\n","        self,\n","        hidden_state: torch.Tensor,\n","        attention_mask: torch.Tensor,\n","        freqs: Optional[torch.Tensor] = None,\n","    ) -> torch.Tensor:\n","        q = self.q(hidden_state)\n","        k = self.k(hidden_state)\n","        v = self.v(hidden_state)\n","        q = rearrange(q, \"b l (h d) -> b h l d\", d=self.head_dim)\n","        k = rearrange(k, \"b l (h d) -> b h l d\", d=self.head_dim)\n","        v = rearrange(v, \"b l (h d) -> b h l d\", d=self.head_dim)\n","\n","        if freqs is not None:\n","            q, k = apply_rotary_pos_emb(q, k, freqs)\n","\n","        k = repeat_kv(k, n_rep=self.num_key_value_groups)\n","        v = repeat_kv(v, n_rep=self.num_key_value_groups)\n","        out = torch.nn.functional.scaled_dot_product_attention(\n","            query=q, key=k, value=v, attn_mask=attention_mask, is_causal=False\n","        )"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:53.366209Z","iopub.status.busy":"2024-04-29T08:31:53.365620Z","iopub.status.idle":"2024-04-29T08:31:53.389503Z","shell.execute_reply":"2024-04-29T08:31:53.388740Z","shell.execute_reply.started":"2024-04-29T08:31:53.366178Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from einops import rearrange, reduce\n","from typing import Optional, Tuple\n","\n","\n","class AbsoluteEncoding(nn.Module):\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        self.pos_embeddings = nn.Embedding(\n","            config.max_position_embeddings, config.hidden_size\n","        )\n","        self.register_buffer(\n","            \"position_ids\",\n","            torch.arange(config.max_position_embeddings).expand((1, -1)),\n","            persistent=False,\n","        )\n","        self.max_size = config.max_position_embeddings\n","\n","    def forward(self, size: int) -> torch.Tensor:\n","        if self.max_size < size:\n","            raise ValueError(\n","                f\"The hidden size ({size }) is more than the config max_position_embeddings {self.max_size}\"\n","            )\n","        return self.pos_embeddings(self.position_ids[:, :size])\n","\n","\n","class SinusoidalEncoding(nn.Module):\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        if config.hidden_size % 2 != 0:\n","            raise ValueError(\n","                f\"Cannot use SinusoidalEncoding with \"\n","                \"odd hidden dim got dim {config.hidden_size}\"\n","            )\n","        self.positional_encoding = torch.zeros(\n","            1, config.max_position_embeddings, config.hidden_size\n","        )\n","        self.position = torch.arange(0, config.max_position_embeddings).unsqueeze(1)\n","        self.div_term = torch.exp(\n","            (\n","                torch.arange(0, config.hidden_size, 2, dtype=torch.float)\n","                * -(torch.log(torch.tensor(10000.0)) / config.hidden_size)\n","            )\n","        )\n","\n","        self.positional_encoding[:, :, 0::2] = torch.sin(\n","            self.position.float() * self.div_term\n","        )\n","        self.positional_encoding[:, :, 1::2] = torch.cos(\n","            self.position.float() * self.div_term\n","        )\n","\n","    def forward(self, seq_len: int) -> torch.Tensor:\n","\n","        return self.positional_encoding[:, :seq_len]\n","\n","\n","# copied from transformer/models/gemma\n","class RotaryEmbedding(nn.Module):\n","    def __init__(self, config, base=10000, device=None):\n","        super().__init__()\n","\n","        self.dim = int(config.hidden_size // config.num_attention_heads)\n","        self.max_position_embeddings = config.max_position_embeddings\n","        self.base = base\n","        self.register_buffer(\n","            \"inv_freq\",\n","            1.0\n","            / (\n","                self.base\n","                ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim)\n","            ),\n","            persistent=False,\n","        )\n","        self.register_buffer(\n","            \"position_ids\",\n","            torch.arange(config.max_position_embeddings).expand((1, -1)),\n","            persistent=False,\n","        )\n","\n","    @torch.no_grad()\n","    def forward(self, seq_len: int = None):\n","        # x: [bs, num_attention_heads, seq_len, head_size]\n","        # size = x.size()[2]\n","        position_ids = torch.arange(seq_len).unsqueeze(0)\n","        # position_ids = self.position_ids[:, :size].float()\n","\n","        inv_freq_expanded = (\n","            self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n","        )\n","        position_ids_expanded = position_ids[:, None, :].float()\n","\n","        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(\n","            1, 2\n","        )\n","        return freqs\n","\n","\n","# Copied from transformers.models.llama.modeling_llama.rotate_half\n","def rotate_half(x):\n","    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n","    x1 = x[..., : x.shape[-1] // 2]\n","    x2 = x[..., x.shape[-1] // 2 :]\n","    return torch.cat((-x2, x1), dim=-1)\n","\n","\n","# def rotate_half(x):\n","#     x1, x2 = x.chunk(2, dim=-1)\n","#     return torch.cat((-x2, x1), dim=-1)\n","\n","\n","# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n","def apply_rotary_pos_emb(\n","    q, k, freqs, only_q: bool = False, unsqueeze_dim=1\n",") -> Tuple[torch.Tensor]:\n","    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n","\n","    Args:\n","        q (`torch.Tensor`): The query tensor.\n","        k (`torch.Tensor`): The key tensor.\n","        freqs: precalculated frqs for sin cos\n","        only_q: bool = False for encoder decoder\n","        unsqueeze_dim (`int`, *optional*, defaults to 1):\n","            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n","            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n","            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n","            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n","            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n","            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n","    Returns:\n","        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n","    \"\"\"\n","    emb = torch.cat((freqs, freqs), dim=-1)\n","    cos = emb.cos()\n","    sin = emb.sin()\n","    cos = cos.unsqueeze(unsqueeze_dim)\n","    sin = sin.unsqueeze(unsqueeze_dim)\n","    #     print(cos.size(),sin.size(),q.size(),k.size())\n","    if only_q:\n","        q_embed = (q * cos) + (rotate_half(q) * sin)\n","    else:\n","\n","        q_embed = (q * cos) + (rotate_half(q) * sin)\n","        k_embed = (k * cos) + (rotate_half(k) * sin)\n","        return q_embed, k_embed\n","\n","\n","# To do :  Alibi"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:53.392093Z","iopub.status.busy":"2024-04-29T08:31:53.391765Z","iopub.status.idle":"2024-04-29T08:31:53.403112Z","shell.execute_reply":"2024-04-29T08:31:53.402364Z","shell.execute_reply.started":"2024-04-29T08:31:53.392070Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from einops import rearrange, reduce\n","from typing import Optional, Tuple, Union\n","\n","_ACT_ = {\n","    \"gelu\": nn.GELU(),\n","    \"leaky_relu\": nn.LeakyReLU(),\n","    \"relu6\": nn.ReLU6(),\n","    \"sigmoid\": nn.Sigmoid(),\n","    \"silu\": nn.SiLU(),\n","    \"swish\": nn.SiLU(),\n","    \"tanh\": nn.Tanh(),\n","}\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, config, multiplier: Union[int, float] = 4) -> None:\n","        super().__init__()\n","        self.intermediate = nn.Linear(\n","            config.hidden_size, int(multiplier) * config.hidden_size\n","        )\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.layerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        if _ACT_.get(getattr(config, \"hidden_act\", None), None):\n","            self.act_fn = _ACT_[config.hidden_act]\n","        else:\n","            self.act_fn = nn.GELU()\n","        self.out = nn.Linear(int(multiplier) * config.hidden_size, config.hidden_size)\n","\n","    def forward(\n","        self, hidden_state: torch.Tensor, input_tensor: torch.Tensor\n","    ) -> torch.Tensor:\n","        output = self.intermediate(hidden_state)\n","        output = self.act_fn(output)\n","        output = self.out(output)\n","        output = self.dropout(output)\n","        output = self.layerNorm(output + input_tensor)\n","        return output"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:53.404513Z","iopub.status.busy":"2024-04-29T08:31:53.404247Z","iopub.status.idle":"2024-04-29T08:31:53.431687Z","shell.execute_reply":"2024-04-29T08:31:53.430742Z","shell.execute_reply.started":"2024-04-29T08:31:53.404493Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from typing import Optional, Tuple\n","\n","from dataclasses import dataclass\n","\n","_position_embeddings = {\n","    \"absolute\": AbsoluteEncoding,\n","    \"sinusoidal\": SinusoidalEncoding,\n","}  #'relative':RelativePositionalEncoding\n","\n","\n","@dataclass\n","class EncoderOutput(object):\n","    logits: torch.Tensor\n","\n","\n","@dataclass\n","class MLMOutput(object):\n","    hidden_state: torch.Tensor\n","    logits: torch.Tensor\n","\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, config, layer_idx: int, attention_type: str = None) -> None:\n","        super().__init__()\n","        self.attention = (\n","            EncoderAttentionGqa(config, layer_idx=layer_idx)\n","            if attention_type == \"gqa\"\n","            else EncoderAttention(config, layer_idx=layer_idx)\n","        )\n","        if attention_type == \"gqa\" and layer_idx == 0:  # avoid to print m times\n","            print(\"Encoder Using GQA Attention\")\n","        self.feed_forward = FeedForward(config)\n","        self.layer_idx = layer_idx\n","\n","    def forward(\n","        self,\n","        hidden_state: torch.Tensor,\n","        attention_mask: torch.Tensor,\n","        freqs: torch.Tensor = None,\n","    ) -> torch.Tensor:\n","        out = self.attention(\n","            hidden_state=hidden_state, attention_mask=attention_mask, freqs=freqs\n","        )\n","        out = self.feed_forward(out, hidden_state)\n","        return out\n","\n","\n","class LMHead(nn.Module):\n","    \"\"\"Head for masked language modelling\"\"\"\n","\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","\n","        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n","        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n","        self.decoder.bias = self.bias\n","\n","    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n","        x = self.dense(hidden_state)\n","        x = nn.GELU()(x)\n","        x = self.layer_norm(x)\n","\n","        # project back to size of vocabulary with bias\n","        x = self.decoder(x)\n","\n","        return x\n","\n","\n","class EncoderModel(nn.Module):\n","\n","    def __init__(\n","        self,\n","        config,\n","        pos_embedding_type: Optional[str] = \"absolute\",\n","        attention_type: str = None,\n","    ) -> None:\n","        super().__init__()\n","        self.word_embeddings = nn.Embedding(\n","            config.vocab_size,\n","            config.hidden_size,\n","            padding_idx=getattr(config, \"pad_token_id\", None),\n","        )\n","        if _position_embeddings.get(pos_embedding_type, None) is not None:\n","            self.position_embeddings = _position_embeddings.get(pos_embedding_type)(\n","                config\n","            )\n","        else:\n","            self.position_embeddings = None\n","        if pos_embedding_type == \"rope\":\n","            self.emb_freq = RotaryEmbedding(config)(config.max_position_embeddings)\n","            print(\n","                \"Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\"\n","            )\n","        self.all_layer = nn.ModuleList(\n","            [\n","                EncoderLayer(config, layer_idx, attention_type)\n","                for layer_idx in range(config.num_hidden_layers)\n","            ]\n","        )\n","\n","    def forward(\n","        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n","    ) -> torch.Tensor:\n","        bsz, seqlen = input_ids.shape\n","        hidden_state = self.word_embeddings(input_ids)\n","        freqs = None\n","        if self.position_embeddings is not None:\n","            pos_info = self.position_embeddings(seqlen)[:, :seqlen, :].to(\n","                input_ids.device\n","            )\n","            hidden_state = hidden_state + pos_info\n","        else:\n","            freqs = self.emb_freq[:, :seqlen].to(input_ids.device)\n","\n","        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2).type_as(hidden_state)\n","        attention_mask = (1.0 - attention_mask) * torch.finfo(hidden_state.dtype).min\n","\n","        for layer in self.all_layer:\n","            hidden_state = layer(hidden_state, attention_mask, freqs)\n","        return EncoderOutput(hidden_state)\n","\n","    @classmethod\n","    def from_config(\n","        cls,\n","        config,\n","        pos_embedding_type: Optional[str] = \"absolute\",\n","        attention_type: str = None,\n","    ) -> nn.Module:\n","        return cls(config, pos_embedding_type, attention_type)\n","\n","\n","class EncoderForMaskedLM(nn.Module):\n","\n","    def __init__(\n","        self,\n","        config,\n","        pos_embedding_type: Optional[str] = \"absolute\",\n","        attention_type: str = None,\n","    ) -> None:\n","        super().__init__()\n","        self.encoder = EncoderModel(\n","            config, pos_embedding_type=pos_embedding_type, attention_type=attention_type\n","        )\n","        self.lm_head = LMHead(config=config)\n","\n","    def forward(\n","        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n","    ) -> torch.Tensor:\n","        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = self.lm_head(out.logits)\n","        return MLMOutput(hidden_state=out.logits, logits=logits)\n","\n","    @classmethod\n","    def from_config(\n","        cls,\n","        config,\n","        pos_embedding_type: Optional[str] = \"absolute\",\n","        attention_type: str = None,\n","    ) -> nn.Module:\n","        return cls(config, pos_embedding_type, attention_type)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:53.432965Z","iopub.status.busy":"2024-04-29T08:31:53.432713Z","iopub.status.idle":"2024-04-29T08:31:53.447684Z","shell.execute_reply":"2024-04-29T08:31:53.446891Z","shell.execute_reply.started":"2024-04-29T08:31:53.432943Z"},"trusted":true},"outputs":[{"data":{"text/plain":["RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.3\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["config"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:53.448925Z","iopub.status.busy":"2024-04-29T08:31:53.448675Z","iopub.status.idle":"2024-04-29T08:31:54.315321Z","shell.execute_reply":"2024-04-29T08:31:54.314521Z","shell.execute_reply.started":"2024-04-29T08:31:53.448903Z"},"trusted":true},"outputs":[],"source":["config.num_hidden_layers = 6\n","model = EncoderModel(config)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:31:54.316774Z","iopub.status.busy":"2024-04-29T08:31:54.316515Z","iopub.status.idle":"2024-04-29T08:31:54.323536Z","shell.execute_reply":"2024-04-29T08:31:54.322690Z","shell.execute_reply.started":"2024-04-29T08:31:54.316752Z"},"trusted":true},"outputs":[{"data":{"text/plain":["EncoderModel(\n","  (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","  (position_embeddings): AbsoluteEncoding(\n","    (pos_embeddings): Embedding(514, 768)\n","  )\n","  (all_layer): ModuleList(\n","    (0-5): 6 x EncoderLayer(\n","      (attention): EncoderAttention(\n","        (q): Linear(in_features=768, out_features=768, bias=True)\n","        (k): Linear(in_features=768, out_features=768, bias=True)\n","        (v): Linear(in_features=768, out_features=768, bias=True)\n","        (out): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (feed_forward): FeedForward(\n","        (intermediate): Linear(in_features=768, out_features=3072, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (layerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (act_fn): GELU(approximate='none')\n","        (out): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","    )\n","  )\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:32:02.090330Z","iopub.status.busy":"2024-04-29T08:32:02.089712Z","iopub.status.idle":"2024-04-29T08:32:02.096425Z","shell.execute_reply":"2024-04-29T08:32:02.095448Z","shell.execute_reply.started":"2024-04-29T08:32:02.090297Z"},"papermill":{"duration":0.023052,"end_time":"2022-08-17T12:43:27.095211","exception":false,"start_time":"2022-08-17T12:43:27.072159","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class ClinicModel(nn.Module):\n","    def __init__(self, config, model):\n","        super(ClinicModel, self).__init__()\n","        self.model = model\n","        self.output = nn.Linear(768, n_classes)\n","\n","    def forward(self, ids, mask):\n","        sequence_output = self.model(ids, mask).logits[:, 0, :]\n","        #         sequence_output = sequence_output[:, 0, :]\n","        logits = self.output(sequence_output)\n","        return logits"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:32:08.120004Z","iopub.status.busy":"2024-04-29T08:32:08.119176Z","iopub.status.idle":"2024-04-29T08:32:09.362230Z","shell.execute_reply":"2024-04-29T08:32:09.361232Z","shell.execute_reply.started":"2024-04-29T08:32:08.119974Z"},"trusted":true},"outputs":[],"source":["train_texts = train[\"Text\"].values.tolist()\n","val_texts = valid[\"Text\"].values.tolist()\n","train_labels = train[\"Target\"].values.tolist()\n","val_labels = valid[\"Target\"].values.tolist()\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n","\n","\n","class ClinicDatasetV2(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return {\n","            \"ids\": item.get(\"input_ids\"),\n","            \"mask\": item.get(\"attention_mask\"),\n","            \"labels\": item.get(\"labels\"),\n","        }\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","\n","train_loader = torch.utils.data.DataLoader(\n","    ClinicDatasetV2(train_encodings, train_labels),\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=2,\n",")\n","val_loader = torch.utils.data.DataLoader(\n","    ClinicDatasetV2(val_encodings, val_labels),\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=2,\n",")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:32:15.401127Z","iopub.status.busy":"2024-04-29T08:32:15.400767Z","iopub.status.idle":"2024-04-29T08:32:15.410536Z","shell.execute_reply":"2024-04-29T08:32:15.409502Z","shell.execute_reply.started":"2024-04-29T08:32:15.401099Z"},"trusted":true},"outputs":[],"source":["def valid_func(model, val_loader, val_bar):\n","    model.eval()\n","    #     bar = tqdm(valid_loader,file=sys.stdout)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    PROB = []\n","    TARGETS = []\n","    losses = []\n","    PREDS = []\n","\n","    for batch_idx, data in enumerate(val_loader):\n","        val_bar.update(1)\n","        input_ids = data[\"ids\"].cuda()\n","        input_masks = data[\"mask\"].cuda()\n","        targets = data[\"labels\"].long().view(-1).cuda()\n","        with torch.no_grad():\n","            logits = model(input_ids, input_masks)\n","\n","        PREDS += [torch.argmax(logits, 1).detach().cpu()]\n","        TARGETS += [targets.detach().cpu()]\n","\n","        loss = loss_fn(logits, targets)\n","        losses.append(loss.item())\n","        val_bar.set_description(f'step: {batch_idx+1} loss: {\"%.4f\" % loss}')\n","\n","    PREDS = torch.cat(PREDS).cpu().numpy()\n","    TARGETS = torch.cat(TARGETS).cpu().numpy()\n","    accuracy = (PREDS == TARGETS).mean()\n","\n","    loss_valid = np.mean(losses)\n","    return loss_valid, accuracy"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:32:37.997205Z","iopub.status.busy":"2024-04-29T08:32:37.996465Z","iopub.status.idle":"2024-04-29T08:32:38.013026Z","shell.execute_reply":"2024-04-29T08:32:38.012042Z","shell.execute_reply.started":"2024-04-29T08:32:37.997176Z"},"trusted":true},"outputs":[],"source":["def single_gpu(name):\n","    use_amp = True\n","    debug = False\n","    gc.collect()\n","    best_epoch_loss = np.inf\n","\n","    net = ClinicModel(config, model)\n","    net.cuda()\n","    log_df = pd.DataFrame(\n","        columns=[\"Epoch\", \"Train_Loss\", \"Valid_Loss\", \"Valid_Accuracy\"]\n","    )\n","    accelerator = Accelerator(\n","        log_with=\"tensorboard\", project_dir=\".\", mixed_precision=\"fp16\"\n","    )\n","    #     accelerator = Accelerator(mixed_precision='bf16')\n","    Config = {\n","        \"num_epoch\": EPOCHS,\n","        \"learning_rate\": lr,\n","        \"loss_function\": str(torch.nn.CrossEntropyLoss),\n","    }\n","\n","    accelerator.init_trackers(f\"{name}_project\", config=Config)\n","\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    optimizer = AdamW(net.parameters(), lr=lr)\n","    num_train_optimization_steps = int(EPOCHS * len(train_loader) / accumulation_steps)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=0.05 * num_train_optimization_steps,\n","        num_training_steps=num_train_optimization_steps,\n","    )  # PyTorch scheduler\n","    if use_amp:\n","        scaler = torch.cuda.amp.GradScaler()\n","    epoch_check = len(train_loader)\n","    total_step = epoch_check * EPOCHS\n","    train_bar = tqdm(total=total_step, dynamic_ncols=True)\n","    val_bar = tqdm(total=len(val_loader), leave=True, dynamic_ncols=True)\n","    t_step = 1\n","    for epoch in range(5):\n","        avg_loss = 0.0\n","        net.train()\n","        loss_list = []\n","        for step, data in enumerate(train_loader):\n","            train_bar.update(1)\n","            #         train_bar.set_description(f\"step: {i+1} epoch: {epoch+1}\")\n","            input_ids = data[\"ids\"].cuda()\n","            input_masks = data[\"mask\"].cuda()\n","            targets = data[\"labels\"].long().view(-1).cuda()\n","            with torch.cuda.amp.autocast():\n","                pred = net(input_ids, input_masks)\n","            loss = loss_fn(pred, targets)\n","            scaler.scale(loss).backward()\n","            if step % accumulation_steps == 0:\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","            accelerator.log({\"training_loss_step\": loss}, step=t_step)\n","            t_step += 1\n","\n","            loss_list.append(loss.detach().cpu().item())\n","        scheduler.step()\n","        avg_loss = np.round(np.mean(loss_list), 4)\n","        accelerator.log({\"train_epoch\": avg_loss}, step=epoch)\n","        vloss, vaccuracy = valid_func(net, val_loader, val_bar)\n","        accelerator.log({\"vloss_epoch\": loss}, step=epoch)\n","        accelerator.log({\"vaccuracy_epoch\": vaccuracy}, step=epoch)\n","        print(f'Epoc: {epoch} loss: {\"%.4f\" % vloss},accuracy: {\"%.4f\" % vaccuracy}')\n","        val_bar.reset()\n","    accelerator.end_training()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:33:02.740482Z","iopub.status.busy":"2024-04-29T08:33:02.740146Z","iopub.status.idle":"2024-04-29T08:35:30.405823Z","shell.execute_reply":"2024-04-29T08:35:30.404699Z","shell.execute_reply.started":"2024-04-29T08:33:02.740459Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-29 08:33:04.992409: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-29 08:33:04.992533: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-29 08:33:05.113146: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb864fd5a6764cf3bf4a4109ca49778f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1195 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d45a80ab34648a6b80e15940b619c8e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/49 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoc: 0 loss: 5.1766,accuracy: 0.0061\n","Epoc: 1 loss: 2.0515,accuracy: 0.5268\n","Epoc: 2 loss: 0.7607,accuracy: 0.8184\n","Epoc: 3 loss: 0.6425,accuracy: 0.8494\n","Epoc: 4 loss: 0.6074,accuracy: 0.8581\n"]}],"source":[" single_gpu(name = 'absolute_embedding')"]},{"cell_type":"markdown","metadata":{},"source":["![Absolute](<Screenshot 2024-04-29 133812-1.png>)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:35:35.944887Z","iopub.status.busy":"2024-04-29T08:35:35.944493Z","iopub.status.idle":"2024-04-29T08:35:36.838004Z","shell.execute_reply":"2024-04-29T08:35:36.837100Z","shell.execute_reply.started":"2024-04-29T08:35:35.944852Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\n"]}],"source":["model = EncoderModel(config, pos_embedding_type=\"rope\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:35:40.030175Z","iopub.status.busy":"2024-04-29T08:35:40.029392Z","iopub.status.idle":"2024-04-29T08:38:08.295275Z","shell.execute_reply":"2024-04-29T08:38:08.293795Z","shell.execute_reply.started":"2024-04-29T08:35:40.030144Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a35b527db4df4cf1a6b194c345974faa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1195 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ad694af488749638ecb63b5d5a51ea4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/49 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoc: 0 loss: 5.1949,accuracy: 0.0081\n","Epoc: 1 loss: 1.3297,accuracy: 0.6874\n","Epoc: 2 loss: 0.6135,accuracy: 0.8471\n","Epoc: 3 loss: 0.5565,accuracy: 0.8619\n","Epoc: 4 loss: 0.5512,accuracy: 0.8671\n"]}],"source":["single_gpu(name=\"RoPE\")"]},{"cell_type":"markdown","metadata":{},"source":["![RoPE](Absolute-2.png)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2663421,"sourceId":4620664,"sourceType":"datasetVersion"},{"sourceId":117289270,"sourceType":"kernelVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
