{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":8264861,"sourceType":"datasetVersion","datasetId":4903552},{"sourceId":117289270,"sourceType":"kernelVersion"}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport os\nimport sys\nimport numpy as np\nimport random\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport math\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom accelerate import Accelerator\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    AdamW,\n    get_linear_schedule_with_warmup,\n)\nfrom transformers import AutoConfig\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport warnings\n\nwarnings.simplefilter(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-02-18T08:10:41.208465Z","iopub.execute_input":"2025-02-18T08:10:41.208761Z","iopub.status.idle":"2025-02-18T08:10:49.315650Z","shell.execute_reply.started":"2025-02-18T08:10:41.208738Z","shell.execute_reply":"2025-02-18T08:10:49.315031Z"},"papermill":{"duration":7.433588,"end_time":"2022-08-17T12:43:27.012415","exception":false,"start_time":"2022-08-17T12:43:19.578827","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"torch.__version__","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:49.316683Z","iopub.execute_input":"2025-02-18T08:10:49.317183Z","iopub.status.idle":"2025-02-18T08:10:49.322867Z","shell.execute_reply.started":"2025-02-18T08:10:49.317151Z","shell.execute_reply":"2025-02-18T08:10:49.321985Z"},"trusted":true},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'2.5.1+cu121'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"! nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:49.325073Z","iopub.execute_input":"2025-02-18T08:10:49.325438Z","iopub.status.idle":"2025-02-18T08:10:49.578658Z","shell.execute_reply.started":"2025-02-18T08:10:49.325417Z","shell.execute_reply":"2025-02-18T08:10:49.577899Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Tue Feb 18 08:10:49 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   47C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   50C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:49.580037Z","iopub.execute_input":"2025-02-18T08:10:49.580309Z","iopub.status.idle":"2025-02-18T08:10:49.584919Z","shell.execute_reply.started":"2025-02-18T08:10:49.580284Z","shell.execute_reply":"2025-02-18T08:10:49.583867Z"},"papermill":{"duration":0.013001,"end_time":"2022-08-17T12:43:27.028923","exception":false,"start_time":"2022-08-17T12:43:27.015922","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model_ckpt = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nconfig = AutoConfig.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:49.585879Z","iopub.execute_input":"2025-02-18T08:10:49.586132Z","iopub.status.idle":"2025-02-18T08:10:50.581742Z","shell.execute_reply.started":"2025-02-18T08:10:49.586113Z","shell.execute_reply":"2025-02-18T08:10:50.580869Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af8ce486da5d4ff49bf7a42211c4f8b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1502aec2b79d4d6d9df9111f918f840b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d13da28d9d7486ca09c77bff860089d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66222d3ed3ee4579af8e326c2ba50a60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c06bf2652416432b83bb466fee3c7146"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"EPOCHS = 5\nlr = 1e-3\nSEED = 42\nMAX_LEN = 128\nBATCH_SIZE = 128\naccumulation_steps = 2\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:50.582822Z","iopub.execute_input":"2025-02-18T08:10:50.583124Z","iopub.status.idle":"2025-02-18T08:10:50.594132Z","shell.execute_reply.started":"2025-02-18T08:10:50.583096Z","shell.execute_reply":"2025-02-18T08:10:50.593247Z"},"papermill":{"duration":0.012823,"end_time":"2022-08-17T12:43:27.068846","exception":false,"start_time":"2022-08-17T12:43:27.056023","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"**Data Source**\n\nfrom datasets import load_dataset\n\n\nclinc = load_dataset(\"clinc_oos\", \"plus\")","metadata":{}},{"cell_type":"code","source":"data_path = \"../input/data-for-distilation\"\ntrain = pd.read_csv(\"../input/data-for-distilation/Clinc_Train.csv\")\nvalid = pd.read_csv(\"../input/data-for-distilation/Clinc_valid.csv\")\nn_classes = np.unique(train.Target).shape[0]\ntrain.head(2)","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:50.595072Z","iopub.execute_input":"2025-02-18T08:10:50.595324Z","iopub.status.idle":"2025-02-18T08:10:50.667296Z","shell.execute_reply.started":"2025-02-18T08:10:50.595305Z","shell.execute_reply":"2025-02-18T08:10:50.666711Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                Text  Target     intent\n0  what expression would i use to say i love you ...      61  translate\n1  can you tell me how to say 'i do not speak muc...      61  translate","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Target</th>\n      <th>intent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>what expression would i use to say i love you ...</td>\n      <td>61</td>\n      <td>translate</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>can you tell me how to say 'i do not speak muc...</td>\n      <td>61</td>\n      <td>translate</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train.Target.nunique()","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:50.669349Z","iopub.execute_input":"2025-02-18T08:10:50.669586Z","iopub.status.idle":"2025-02-18T08:10:50.679217Z","shell.execute_reply.started":"2025-02-18T08:10:50.669566Z","shell.execute_reply":"2025-02-18T08:10:50.678555Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"151"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:50.681024Z","iopub.execute_input":"2025-02-18T08:10:50.681286Z","iopub.status.idle":"2025-02-18T08:10:54.834670Z","shell.execute_reply.started":"2025-02-18T08:10:50.681263Z","shell.execute_reply":"2025-02-18T08:10:54.833697Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom einops import rearrange, reduce, repeat\nfrom typing import Optional, Tuple\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(\n        batch, num_key_value_heads, n_rep, slen, head_dim\n    )\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\ndef repeat_kv_einops(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = repeat(\n        hidden_states,\n        \"batch num_key_value_heads slen head_dim -> batch num_key_value_heads n_rep slen head_dim\",\n        n_rep=n_rep,\n    )  # hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    # return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n    return rearrange(\n        hidden_states,\n        \"batch num_key_value_heads n_rep slen head_dim -> batch (num_key_value_heads n_rep) slen head_dim\",\n    )\n\n\nclass EncoderAttention(nn.Module):\n    def __init__(self, config, layer_idx: int) -> None:\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.head_size = int(config.hidden_size // config.num_attention_heads)\n        self.attention_bias = getattr(config, \"attention_bias\", True)\n        self.layer_idx = layer_idx\n        # self.qkv = nn.Linear(config.hidden_size,3*config.hidden_size)\n        self.q = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.k = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.v = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.out = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.num_attention_heads = config.num_attention_heads\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        q = self.q(hidden_state)\n        k = self.k(hidden_state)\n        v = self.v(hidden_state)\n        # q,k,v = self.qkv(hidden_state).chunk(3, dim = -1) #b X l X d dim =-1 or 2\n        # place holder for RoPe operation\n        q = rearrange(q, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        k = rearrange(k, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        v = rearrange(v, \"b l (h d) -> b h l d\", h=self.num_attention_heads)\n        if freqs is not None:\n            q, k = apply_rotary_pos_emb(q, k, freqs)\n\n        out = torch.nn.functional.scaled_dot_product_attention(\n            query=q, key=k, value=v, attn_mask=attention_mask, is_causal=False\n        )\n        out = rearrange(out, \"b h l d -> b l (h d)\")\n        out = self.out(out)\n        return out\n\n\nclass EncoderAttentionGqa(nn.Module):\n    def __init__(self, config, layer_idx: int) -> None:\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n        if not self.flash and self.layer_idx == 0:  # avoid to print m times\n            print(\"WARNING: Flash Attention requires PyTorch >= 2.0\")\n        self.layer_idx = layer_idx\n        self.head_dim = int(config.hidden_size // config.num_attention_heads)\n        self.num_attention_heads = config.num_attention_heads\n        self.num_key_value_heads = getattr(config, \"num_key_value_heads\", 4)\n        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads\n        if (\n            self.num_attention_heads % self.num_key_value_heads != 0\n            or self.num_attention_heads < self.num_key_value_heads\n        ):\n            raise ValueError(\n                f\"num_key_value_heads {self.num_key_value_heads }  should be less than equal num_attention_heads {config.num_attention_heads} and  multiple of num_attention_heads {config.num_attention_heads} \"\n            )\n        self.attention_bias = getattr(config, \"attention_bias\", True)\n        self.out = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.q = nn.Linear(\n            config.hidden_size, config.hidden_size, bias=self.attention_bias\n        )\n        self.k = nn.Linear(\n            config.hidden_size,\n            self.num_key_value_heads * self.head_dim,\n            bias=self.attention_bias,\n        )\n        self.v = nn.Linear(\n            config.hidden_size,\n            self.num_key_value_heads * self.head_dim,\n            bias=self.attention_bias,\n        )\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        q = self.q(hidden_state)\n        k = self.k(hidden_state)\n        v = self.v(hidden_state)\n        q = rearrange(q, \"b l (h d) -> b h l d\", d=self.head_dim)\n        k = rearrange(k, \"b l (h d) -> b h l d\", d=self.head_dim)\n        v = rearrange(v, \"b l (h d) -> b h l d\", d=self.head_dim)\n\n        if freqs is not None:\n            q, k = apply_rotary_pos_emb(q, k, freqs)\n\n        k = repeat_kv(k, n_rep=self.num_key_value_groups)\n        v = repeat_kv(v, n_rep=self.num_key_value_groups)\n        out = torch.nn.functional.scaled_dot_product_attention(\n            query=q, key=k, value=v, attn_mask=attention_mask, is_causal=False\n        )","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:54.835901Z","iopub.execute_input":"2025-02-18T08:10:54.836197Z","iopub.status.idle":"2025-02-18T08:10:54.874420Z","shell.execute_reply.started":"2025-02-18T08:10:54.836162Z","shell.execute_reply":"2025-02-18T08:10:54.873522Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom einops import rearrange, reduce\nfrom typing import Optional, Tuple\n\n\nclass AbsoluteEncoding(nn.Module):\n    def __init__(self, config) -> None:\n        super().__init__()\n        self.pos_embeddings = nn.Embedding(\n            config.max_position_embeddings, config.hidden_size\n        )\n        self.register_buffer(\n            \"position_ids\",\n            torch.arange(config.max_position_embeddings).expand((1, -1)),\n            persistent=False,\n        )\n        self.max_size = config.max_position_embeddings\n\n    def forward(self, size: int) -> torch.Tensor:\n        if self.max_size < size:\n            raise ValueError(\n                f\"The hidden size ({size }) is more than the config max_position_embeddings {self.max_size}\"\n            )\n        return self.pos_embeddings(self.position_ids[:, :size])\n\n\nclass SinusoidalEncoding(nn.Module):\n    def __init__(self, config) -> None:\n        super().__init__()\n        if config.hidden_size % 2 != 0:\n            raise ValueError(\n                f\"Cannot use SinusoidalEncoding with \"\n                \"odd hidden dim got dim {config.hidden_size}\"\n            )\n        self.positional_encoding = torch.zeros(\n            1, config.max_position_embeddings, config.hidden_size\n        )\n        self.position = torch.arange(0, config.max_position_embeddings).unsqueeze(1)\n        self.div_term = torch.exp(\n            (\n                torch.arange(0, config.hidden_size, 2, dtype=torch.float)\n                * -(torch.log(torch.tensor(10000.0)) / config.hidden_size)\n            )\n        )\n\n        self.positional_encoding[:, :, 0::2] = torch.sin(\n            self.position.float() * self.div_term\n        )\n        self.positional_encoding[:, :, 1::2] = torch.cos(\n            self.position.float() * self.div_term\n        )\n\n    def forward(self, seq_len: int) -> torch.Tensor:\n\n        return self.positional_encoding[:, :seq_len]\n\n\nclass RotaryEmbedding(nn.Module):\n    \"\"\"\n    RotaryEmbedding is a PyTorch module that implements rotary positional embeddings for attention mechanisms.\n    Args:\n        config (object): Configuration object containing the following attributes:\n            hidden_size (int): The hidden size of the model.\n            num_attention_heads (int): The number of attention heads.\n    Attributes:\n        inv_freq (torch.Tensor): A tensor containing the inverse frequencies for the rotary embeddings.\n    Methods:\n        forward(seq_len):\n            Computes the rotary positional embeddings for a given sequence length.\n            Args:\n                seq_len (int): The length of the input sequence.\n            Returns:\n                torch.Tensor: A tensor containing the rotary positional embeddings with shape (1, seq_len, dim).\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        dim = int(config.hidden_size // config.num_attention_heads)\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, seq_len):\n        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq)\n        freqs = torch.einsum(\"i, j -> i j\", t, self.inv_freq)\n\n        return freqs[None, :, :]\n\n\ndef rotate_half(x):\n    \"\"\"\n    Rotates half the hidden dimensions of the input tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor to be rotated.\n\n    Returns:\n        torch.Tensor: The tensor with half of its hidden dimensions rotated.\n    \"\"\"\n    x1, x2 = x.chunk(2, dim=-1)\n    return torch.cat((-x2, x1), dim=-1)\n\n\n# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\ndef apply_rotary_pos_emb(\n    q, k, freqs, only_q: bool = False, unsqueeze_dim=1\n) -> Tuple[torch.Tensor]:\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        freqs: precalculated frqs for sin cos\n        only_q: bool = False for encoder decoder\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    emb = torch.cat((freqs, freqs), dim=-1)\n    cos = emb.cos()\n    sin = emb.sin()\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    #     print(cos.size(),sin.size(),q.size(),k.size())\n    if only_q:\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n    else:\n\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n        k_embed = (k * cos) + (rotate_half(k) * sin)\n        return q_embed, k_embed\n\n\n# To do :  Alibi","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:54.875452Z","iopub.execute_input":"2025-02-18T08:10:54.875751Z","iopub.status.idle":"2025-02-18T08:10:54.888072Z","shell.execute_reply.started":"2025-02-18T08:10:54.875722Z","shell.execute_reply":"2025-02-18T08:10:54.887363Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom einops import rearrange, reduce\nfrom typing import Optional, Tuple, Union\n\n_ACT_ = {\n    \"gelu\": nn.GELU(),\n    \"leaky_relu\": nn.LeakyReLU(),\n    \"relu6\": nn.ReLU6(),\n    \"sigmoid\": nn.Sigmoid(),\n    \"silu\": nn.SiLU(),\n    \"swish\": nn.SiLU(),\n    \"tanh\": nn.Tanh(),\n}\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, config, multiplier: Union[int, float] = 4) -> None:\n        super().__init__()\n        self.intermediate = nn.Linear(\n            config.hidden_size, int(multiplier) * config.hidden_size\n        )\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.layerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        if _ACT_.get(getattr(config, \"hidden_act\", None), None):\n            self.act_fn = _ACT_[config.hidden_act]\n        else:\n            self.act_fn = nn.GELU()\n        self.out = nn.Linear(int(multiplier) * config.hidden_size, config.hidden_size)\n\n    def forward(\n        self, hidden_state: torch.Tensor, input_tensor: torch.Tensor\n    ) -> torch.Tensor:\n        output = self.intermediate(hidden_state)\n        output = self.act_fn(output)\n        output = self.out(output)\n        output = self.dropout(output)\n        output = self.layerNorm(output + input_tensor)\n        return output","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:54.888945Z","iopub.execute_input":"2025-02-18T08:10:54.889224Z","iopub.status.idle":"2025-02-18T08:10:54.902967Z","shell.execute_reply.started":"2025-02-18T08:10:54.889196Z","shell.execute_reply":"2025-02-18T08:10:54.902273Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple\n\nfrom dataclasses import dataclass\n\n_position_embeddings = {\n    \"absolute\": AbsoluteEncoding,\n    \"sinusoidal\": SinusoidalEncoding,\n}  #'relative':RelativePositionalEncoding\n\n\n@dataclass\nclass EncoderOutput(object):\n    logits: torch.Tensor\n\n\n@dataclass\nclass MLMOutput(object):\n    hidden_state: torch.Tensor\n    logits: torch.Tensor\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, config, layer_idx: int, attention_type: str = None) -> None:\n        super().__init__()\n        self.attention = (\n            EncoderAttentionGqa(config, layer_idx=layer_idx)\n            if attention_type == \"gqa\"\n            else EncoderAttention(config, layer_idx=layer_idx)\n        )\n        if attention_type == \"gqa\" and layer_idx == 0:  # avoid to print m times\n            print(\"Encoder Using GQA Attention\")\n        self.feed_forward = FeedForward(config)\n        self.layer_idx = layer_idx\n\n    def forward(\n        self,\n        hidden_state: torch.Tensor,\n        attention_mask: torch.Tensor,\n        freqs: torch.Tensor = None,\n    ) -> torch.Tensor:\n        out = self.attention(\n            hidden_state=hidden_state, attention_mask=attention_mask, freqs=freqs\n        )\n        out = self.feed_forward(out, hidden_state)\n        return out\n\n\nclass LMHead(nn.Module):\n    \"\"\"Head for masked language modelling\"\"\"\n\n    def __init__(self, config) -> None:\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n        x = self.dense(hidden_state)\n        x = nn.GELU()(x)\n        x = self.layer_norm(x)\n\n        # project back to size of vocabulary with bias\n        x = self.decoder(x)\n\n        return x\n\n\nclass EncoderModel(nn.Module):\n\n    def __init__(\n        self,\n        config,\n        pos_embedding_type: Optional[str] = \"absolute\",\n        attention_type: str = None,\n    ) -> None:\n        super().__init__()\n        self.word_embeddings = nn.Embedding(\n            config.vocab_size,\n            config.hidden_size,\n            padding_idx=getattr(config, \"pad_token_id\", None),\n        )\n        if _position_embeddings.get(pos_embedding_type, None) is not None:\n            self.position_embeddings = _position_embeddings.get(pos_embedding_type)(\n                config\n            )\n        else:\n            self.position_embeddings = None\n        if pos_embedding_type == \"rope\":\n            self.emb_freq = RotaryEmbedding(config)(config.max_position_embeddings)\n            print(\n                \"Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\"\n            )\n        self.all_layer = nn.ModuleList(\n            [\n                EncoderLayer(config, layer_idx, attention_type)\n                for layer_idx in range(config.num_hidden_layers)\n            ]\n        )\n\n    def forward(\n        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n    ) -> torch.Tensor:\n        bsz, seqlen = input_ids.shape\n        hidden_state = self.word_embeddings(input_ids)\n        freqs = None\n        if self.position_embeddings is not None:\n            pos_info = self.position_embeddings(seqlen)[:, :seqlen, :].to(\n                input_ids.device\n            )\n            hidden_state = hidden_state + pos_info\n        else:\n            freqs = self.emb_freq[:, :seqlen].to(input_ids.device)\n\n        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2).type_as(hidden_state)\n        attention_mask = (1.0 - attention_mask) * torch.finfo(hidden_state.dtype).min\n\n        for layer in self.all_layer:\n            hidden_state = layer(hidden_state, attention_mask, freqs)\n        return EncoderOutput(hidden_state)\n\n    @classmethod\n    def from_config(\n        cls,\n        config,\n        pos_embedding_type: Optional[str] = \"absolute\",\n        attention_type: str = None,\n    ) -> nn.Module:\n        return cls(config, pos_embedding_type, attention_type)","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:54.903663Z","iopub.execute_input":"2025-02-18T08:10:54.903886Z","iopub.status.idle":"2025-02-18T08:10:54.924177Z","shell.execute_reply.started":"2025-02-18T08:10:54.903866Z","shell.execute_reply":"2025-02-18T08:10:54.923304Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"config","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:54.925323Z","iopub.execute_input":"2025-02-18T08:10:54.925609Z","iopub.status.idle":"2025-02-18T08:10:54.941193Z","shell.execute_reply.started":"2025-02-18T08:10:54.925582Z","shell.execute_reply":"2025-02-18T08:10:54.940585Z"},"trusted":true},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"RobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.47.0\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"config.num_hidden_layers = 6\nmodel = EncoderModel(config,pos_embedding_type='rope')","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:54.942031Z","iopub.execute_input":"2025-02-18T08:10:54.942312Z","iopub.status.idle":"2025-02-18T08:10:55.756305Z","shell.execute_reply.started":"2025-02-18T08:10:54.942279Z","shell.execute_reply":"2025-02-18T08:10:55.755635Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Encoder Ignoring sinusoidal or absolute position embeddings because rope,is enable\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:55.757026Z","iopub.execute_input":"2025-02-18T08:10:55.757305Z","iopub.status.idle":"2025-02-18T08:10:55.762987Z","shell.execute_reply.started":"2025-02-18T08:10:55.757276Z","shell.execute_reply":"2025-02-18T08:10:55.762242Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"EncoderModel(\n  (word_embeddings): Embedding(50265, 768, padding_idx=1)\n  (all_layer): ModuleList(\n    (0-5): 6 x EncoderLayer(\n      (attention): EncoderAttention(\n        (q): Linear(in_features=768, out_features=768, bias=True)\n        (k): Linear(in_features=768, out_features=768, bias=True)\n        (v): Linear(in_features=768, out_features=768, bias=True)\n        (out): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (feed_forward): FeedForward(\n        (intermediate): Linear(in_features=768, out_features=3072, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (layerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (act_fn): GELU(approximate='none')\n        (out): Linear(in_features=3072, out_features=768, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"class ClinicModel(nn.Module):\n    def __init__(self,model,n_classes=n_classes):\n        super(ClinicModel, self).__init__()\n        self.model = model\n        self.output = nn.Linear(768, n_classes)\n\n    def forward(self, ids, mask):\n        sequence_output = self.model(ids, mask).logits[:, 0, :]\n        #         sequence_output = sequence_output[:, 0, :]\n        logits = self.output(sequence_output)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:55.763847Z","iopub.execute_input":"2025-02-18T08:10:55.764125Z","iopub.status.idle":"2025-02-18T08:10:55.776542Z","shell.execute_reply.started":"2025-02-18T08:10:55.764104Z","shell.execute_reply":"2025-02-18T08:10:55.775772Z"},"papermill":{"duration":0.023052,"end_time":"2022-08-17T12:43:27.095211","exception":false,"start_time":"2022-08-17T12:43:27.072159","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"train_texts = train[\"Text\"].values.tolist()\nval_texts = valid[\"Text\"].values.tolist()\ntrain_labels = train[\"Target\"].values.tolist()\nval_labels = valid[\"Target\"].values.tolist()\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n\n\nclass ClinicDatasetV2(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return {\n            \"ids\": item.get(\"input_ids\"),\n            \"mask\": item.get(\"attention_mask\"),\n            \"labels\": item.get(\"labels\"),\n        }\n\n    def __len__(self):\n        return len(self.labels)\n\n\ntrain_loader = torch.utils.data.DataLoader(\n    ClinicDatasetV2(train_encodings, train_labels),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n)\nval_loader = torch.utils.data.DataLoader(\n    ClinicDatasetV2(val_encodings, val_labels),\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:55.777264Z","iopub.execute_input":"2025-02-18T08:10:55.777501Z","iopub.status.idle":"2025-02-18T08:10:56.658893Z","shell.execute_reply.started":"2025-02-18T08:10:55.777482Z","shell.execute_reply":"2025-02-18T08:10:56.658187Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def valid_func(model, val_loader, accelerator):\n    model.eval()\n    loss_fn = torch.nn.CrossEntropyLoss()\n    PROB = []\n    TARGETS = []\n    losses = []\n    PREDS = []\n\n    for batch_idx, data in enumerate(val_loader):\n        input_ids = data[\"ids\"]\n        input_masks = data[\"mask\"]\n        targets = data[\"labels\"].long().view(-1)\n        with torch.no_grad():\n            logits = model(input_ids, input_masks)\n            \n        # logits = logits.argmax(logits, 1)\n            \n        logits, targets = accelerator.gather_for_metrics((logits, targets))\n\n        PREDS += [torch.argmax(logits, 1).detach().cpu()]\n        TARGETS += [targets.detach().cpu()]\n\n        loss = loss_fn(logits, targets)\n        losses.append(loss.item())\n\n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n    accuracy = (PREDS == TARGETS).mean()\n\n    loss_valid = np.mean(losses)\n    return loss_valid, accuracy","metadata":{"execution":{"iopub.status.busy":"2025-02-18T08:10:56.659611Z","iopub.execute_input":"2025-02-18T08:10:56.659877Z","iopub.status.idle":"2025-02-18T08:10:56.665683Z","shell.execute_reply.started":"2025-02-18T08:10:56.659856Z","shell.execute_reply":"2025-02-18T08:10:56.664667Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"model = ClinicModel(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:10:56.666506Z","iopub.execute_input":"2025-02-18T08:10:56.666766Z","iopub.status.idle":"2025-02-18T08:10:56.705019Z","shell.execute_reply.started":"2025-02-18T08:10:56.666728Z","shell.execute_reply":"2025-02-18T08:10:56.704006Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:10:56.706251Z","iopub.execute_input":"2025-02-18T08:10:56.706549Z","iopub.status.idle":"2025-02-18T08:10:56.787460Z","shell.execute_reply.started":"2025-02-18T08:10:56.706515Z","shell.execute_reply":"2025-02-18T08:10:56.786751Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"ClinicModel(\n  (model): EncoderModel(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (all_layer): ModuleList(\n      (0-5): 6 x EncoderLayer(\n        (attention): EncoderAttention(\n          (q): Linear(in_features=768, out_features=768, bias=True)\n          (k): Linear(in_features=768, out_features=768, bias=True)\n          (v): Linear(in_features=768, out_features=768, bias=True)\n          (out): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (feed_forward): FeedForward(\n          (intermediate): Linear(in_features=768, out_features=3072, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (act_fn): GELU(approximate='none')\n          (out): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n    )\n  )\n  (output): Linear(in_features=768, out_features=151, bias=True)\n)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"def main(model,train_loader,val_loader,lr=1e-3,num_epochs= 3,name='Rope_classification'):\n    \n    accelerator = Accelerator(\n        log_with=\"tensorboard\", project_dir=\"./\", mixed_precision=\"bf16\",gradient_accumulation_steps=1\n    )\n    #     accelerator = Accelerator(mixed_precision='bf16')\n    Config = {\n        \"num_epoch\": EPOCHS,\n        \"learning_rate\": lr,\n        \"loss_function\": str(torch.nn.CrossEntropyLoss)}\n\n    accelerator.init_trackers(f\"{name}_project\", config=Config)\n    \n\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = AdamW(model.parameters(), lr=lr)\n    num_train_optimization_steps = int(EPOCHS * len(train_loader) / accumulation_steps)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0.05 * num_train_optimization_steps,\n        num_training_steps=num_train_optimization_steps,\n    ) \n    train_loader,val_loader,model,optimizer,scheduler =  accelerator.prepare(train_loader,val_loader, model, optimizer,scheduler)\n    all_step=0\n    for epoch in range(num_epochs):\n        avg_loss = 0.0\n        model.train()\n        loss_list = []\n        for step, data in enumerate(train_loader):\n            with accelerator.accumulate(model):\n                input_ids = data[\"ids\"]\n                attention_masks = data[\"mask\"]\n                targets = data[\"labels\"].long().view(-1)\n                outputs = model(input_ids,attention_masks)\n                loss = loss_fn(outputs, targets)\n                accelerator.backward(loss)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                accelerator.log({\"step_loss\":loss},step= all_step)\n                all_step+=1\n                loss_list.append(loss.detach().cpu().item())\n                \n        avg_loss = np.round(np.mean(loss_list), 4)\n        accelerator.log({\"train_epoch\": avg_loss}, step=epoch)    \n                \n        vloss, vaccuracy = valid_func(model, val_loader,accelerator)\n        accelerator.print(f\"Epoch {epoch+1} : loss = {avg_loss}, accuracy =  {vaccuracy}\")\n        unwrapped_model = accelerator.unwrap_model(model)\n        torch.save(unwrapped_model.state_dict(),'rope_classification_model.pt')\n    accelerator.end_training()\n    accelerator.free_memory(train_loader,val_loader, model, optimizer,scheduler)\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:10:56.788443Z","iopub.execute_input":"2025-02-18T08:10:56.788729Z","iopub.status.idle":"2025-02-18T08:10:56.803037Z","shell.execute_reply.started":"2025-02-18T08:10:56.788699Z","shell.execute_reply":"2025-02-18T08:10:56.802214Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from accelerate import notebook_launcher","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:10:56.806303Z","iopub.execute_input":"2025-02-18T08:10:56.806553Z","iopub.status.idle":"2025-02-18T08:10:56.818371Z","shell.execute_reply.started":"2025-02-18T08:10:56.806533Z","shell.execute_reply":"2025-02-18T08:10:56.817559Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"notebook_launcher(main, (model,train_loader,val_loader), num_processes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:10:56.819278Z","iopub.execute_input":"2025-02-18T08:10:56.819561Z","iopub.status.idle":"2025-02-18T08:13:09.158994Z","shell.execute_reply.started":"2025-02-18T08:10:56.819531Z","shell.execute_reply":"2025-02-18T08:13:09.157953Z"}},"outputs":[{"name":"stdout","text":"Launching training on 2 GPUs.\nEpoch 1 : loss = 1.5886, accuracy =  0.8216129032258065\nEpoch 2 : loss = 0.1724, accuracy =  0.8732258064516129\nEpoch 3 : loss = 0.0311, accuracy =  0.88\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}