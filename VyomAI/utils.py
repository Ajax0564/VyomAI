#for model making utils
import torch.nn as nn
from typing import Dict

def model_size(model: nn.Module) -> float:
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()

    size_all_mb = (param_size + buffer_size) / 1024**2
    return size_all_mb

def init_weights(module: nn.Module) -> None:
    """Initialize the weights"""
    if isinstance(module, nn.Linear):
        #copied from Transformer
        module.weight.data.normal_(mean=0.0, std=0.02) #initializer_range": 0.02
        if module.bias is not None:
            module.bias.data.zero_()
    elif isinstance(module, nn.Embedding):
        module.weight.data.normal_(mean=0.0, std=0.02)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()
    elif isinstance(module, nn.LayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)

def model_parameters(model: nn.Module) -> Dict[str,int]:
    total_params = sum([p.numel() for p in model.parameters()])
    trainable_params = sum([p.numel() for p in model.parameters() if p.requires_grad])
    return {'total_params':total_params,'trainable_params':trainable_params}