# VyomAI
**Transfomer models implementation from scratch using pytorch to make it more accessible for research purposes.** 

# Examples
Models implemented from scratch using Pytorch 2.0
| Task | dataset link | Pyotch 2.0 | description
|---|---|:---:|:---:|
|[**`text classification`**](https://) | [WikiText-2](https://) |✅|encoder model for text classification|
|[**`masked language modeling`**](https://) | [WikiText-2](https://) |✅| encoder model pretraining with mlm style| 
|[**`electra language modeling`**](https://) | [WikiText-2](https://) |✅|encoder model pretraining with electra style| 
|[**`casual language-modeling`**](https://) | [WikiText-2](https://) |✅|decoder model pretraining with gpt style and kv-cache for fast inference | 
|[**`knowledge distilation`**](https://) | [WikiText-2](https://) |✅|initilization of a model from pretrained model|
|[**`seq2seq modeling`**](https://) | [WikiText-2](https://) |✅|seq2seq model training for image caption with kv-cache|
|[**`adapters`**](https://) | [WikiText-2](https://) |✅|Lora and Dora for parameter efficient tunning|
|[**`vit`**](https://) | [WikiText-2](https://) |✅|visual image transformer for image classification|
|[**`clip`**](https://) | [WikiText-2](https://) |✅|scratch implementation of contrastive language-image pre-training|

****Many more to come**